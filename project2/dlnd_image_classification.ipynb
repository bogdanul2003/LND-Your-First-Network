{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:46, 3.69MB/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 0:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+4ZgtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f2474dc18>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 0\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    tmp=np.ndarray(x.shape,dtype=float)\n",
    "    tmp[:] = x\n",
    "    tmp[...] /= 255.0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    ret = np.zeros((len(x),10))\n",
    "    for idx,lbl in enumerate(x):\n",
    "        ret[idx,lbl] = 1\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " [[[ 0.10980392  0.09803922  0.03921569]\n",
      "   [ 0.14509804  0.13333333  0.0745098 ]\n",
      "   [ 0.14901961  0.1372549   0.07843137]\n",
      "   ..., \n",
      "   [ 0.29803922  0.2627451   0.15294118]\n",
      "   [ 0.31764706  0.28235294  0.16862745]\n",
      "   [ 0.33333333  0.29803922  0.18431373]]\n",
      "\n",
      "  [[ 0.12941176  0.10980392  0.05098039]\n",
      "   [ 0.13333333  0.11764706  0.05490196]\n",
      "   [ 0.1254902   0.10588235  0.04705882]\n",
      "   ..., \n",
      "   [ 0.37254902  0.32156863  0.21568627]\n",
      "   [ 0.37647059  0.32156863  0.21960784]\n",
      "   [ 0.33333333  0.28235294  0.17647059]]\n",
      "\n",
      "  [[ 0.15294118  0.1254902   0.05882353]\n",
      "   [ 0.15686275  0.12941176  0.06666667]\n",
      "   [ 0.22352941  0.19607843  0.12941176]\n",
      "   ..., \n",
      "   [ 0.36470588  0.29803922  0.20392157]\n",
      "   [ 0.41960784  0.34901961  0.25882353]\n",
      "   [ 0.37254902  0.30196078  0.21176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.3254902   0.28627451  0.20392157]\n",
      "   [ 0.34117647  0.30196078  0.21960784]\n",
      "   [ 0.32941176  0.29019608  0.20392157]\n",
      "   ..., \n",
      "   [ 0.38823529  0.36470588  0.2745098 ]\n",
      "   [ 0.35294118  0.32941176  0.23921569]\n",
      "   [ 0.31764706  0.29411765  0.20392157]]\n",
      "\n",
      "  [[ 0.34509804  0.28235294  0.2       ]\n",
      "   [ 0.35294118  0.29019608  0.20392157]\n",
      "   [ 0.36470588  0.30196078  0.21960784]\n",
      "   ..., \n",
      "   [ 0.31372549  0.29019608  0.20784314]\n",
      "   [ 0.29803922  0.2745098   0.19215686]\n",
      "   [ 0.32156863  0.29803922  0.21568627]]\n",
      "\n",
      "  [[ 0.38039216  0.30588235  0.21960784]\n",
      "   [ 0.36862745  0.29411765  0.20784314]\n",
      "   [ 0.36470588  0.29411765  0.20784314]\n",
      "   ..., \n",
      "   [ 0.21176471  0.18431373  0.10980392]\n",
      "   [ 0.24705882  0.21960784  0.14509804]\n",
      "   [ 0.28235294  0.25490196  0.18039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.66666667  0.70588235  0.77647059]\n",
      "   [ 0.65882353  0.69803922  0.76862745]\n",
      "   [ 0.69411765  0.7254902   0.79607843]\n",
      "   ..., \n",
      "   [ 0.63529412  0.70196078  0.84313725]\n",
      "   [ 0.61960784  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]]\n",
      "\n",
      "  [[ 0.65882353  0.70980392  0.77647059]\n",
      "   [ 0.6745098   0.7254902   0.78823529]\n",
      "   [ 0.67058824  0.71764706  0.78431373]\n",
      "   ..., \n",
      "   [ 0.62352941  0.69411765  0.83137255]\n",
      "   [ 0.61176471  0.69019608  0.82745098]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  [[ 0.60392157  0.66666667  0.72941176]\n",
      "   [ 0.58431373  0.64705882  0.70980392]\n",
      "   [ 0.50588235  0.56470588  0.63529412]\n",
      "   ..., \n",
      "   [ 0.63137255  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.32941176  0.31372549]\n",
      "   [ 0.29803922  0.33333333  0.31764706]\n",
      "   [ 0.30588235  0.33333333  0.32156863]\n",
      "   ..., \n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.28235294  0.29411765]\n",
      "   [ 0.23921569  0.25490196  0.26666667]]\n",
      "\n",
      "  [[ 0.26666667  0.29803922  0.30196078]\n",
      "   [ 0.27058824  0.30196078  0.30588235]\n",
      "   [ 0.28235294  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.29803922  0.31372549  0.3254902 ]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.27843137  0.29411765  0.30588235]]\n",
      "\n",
      "  [[ 0.2627451   0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.29803922  0.30980392]\n",
      "   [ 0.27058824  0.29411765  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30980392  0.32156863]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.28627451  0.30196078  0.31372549]]]]\n"
     ]
    }
   ],
   "source": [
    "filename = 'preprocess_batch_' + str(1) + '.p'\n",
    "features, labels = pickle.load(open(filename, mode='rb'))\n",
    "print(features[0:5,...])\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    (width, height, chann) = image_shape\n",
    "    x = tf.placeholder(tf.float32, [None, width, height, chann], name='x')\n",
    "    print(x.get_shape())\n",
    "    # TODO: Implement Function\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y= tf.placeholder(tf.int8, [None,n_classes], name='y')\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tmp = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(type(x_tensor.get_shape().as_list()[3] ))\n",
    "    print(conv_ksize)\n",
    "    (convk1, convk2) = conv_ksize\n",
    "    (convs1, convs2) = conv_strides\n",
    "    (poolk1, poolk2) = pool_ksize\n",
    "    (pools1, pools2) = pool_strides\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([convk1, convk2,x_tensor.get_shape().as_list()[3],conv_num_outputs ], stddev=0.1, mean=0.0,dtype=tf.float32, seed=None, name=None ))\n",
    "    biases= tf.Variable(tf.constant(0,dtype=tf.float32, shape=[conv_num_outputs]))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=x_tensor, filter= weights, strides = [1,convs1, convs2,1],padding='SAME')\n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer += biases\n",
    "    #print(layer.get_shape().as_list())\n",
    "    \n",
    "    #print(layer.get_shape().as_list())\n",
    "    \n",
    "    layer = tf.nn.max_pool(value=layer,ksize=[1,poolk1,poolk2,1], \n",
    "                           strides = [1,pools1,pools2,1],padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    #print(layer.get_shape().as_list())\n",
    "    #print(conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 10 30 6\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    [batch_size, h,w,c] = x_tensor.get_shape().as_list()\n",
    "    features= h*w*c\n",
    "    \n",
    "    layer = tf.reshape(x_tensor, [-1, features])\n",
    "    print(batch_size,h,w,c)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    layer = tf.nn.relu(layer)\n",
    "    #layer = tf.sigmoid(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convl = conv2d_maxpool(x,20, (5,5), (1,1), (2,2), (2,2))\n",
    "    convl = conv2d_maxpool(convl,40, (5,5), (1,1), (2,2), (2,2))\n",
    "    #convl = conv2d_maxpool(convl,64, (5,5), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    layer = flatten(convl)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    print(layer.get_shape().as_list())\n",
    "    layer = fully_conn(layer,250)\n",
    "    layer = tf.nn.dropout(layer, keep_prob)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,250)\n",
    "#     layer = tf.layers.dropout(layer, 0.5)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    layer= output(layer,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01,epsilon=0.00001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(type(valid_features), type(valid_labels), type(feature_batch), type(label_batch))\n",
    "    #print(\"print_stats valid_features:{} valid_labels:{}\".format(valid_features.shape, valid_labels.shape))\n",
    "    \n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    valid = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features[0:100,...],\n",
    "        y: valid_labels[0:100,...],\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    print('loss:{} acc:{}'.format( loss,valid))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.164783477783203 acc:0.26999998092651367\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.05415678024292 acc:0.3799999952316284\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.9705619812011719 acc:0.38999998569488525\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.8800562620162964 acc:0.4099999666213989\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.8100510835647583 acc:0.3999999761581421\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.72808837890625 acc:0.4099999666213989\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.6664652824401855 acc:0.3999999761581421\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.6265156269073486 acc:0.4299999475479126\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.578696370124817 acc:0.429999977350235\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.5329322814941406 acc:0.429999977350235\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.4884045124053955 acc:0.44999998807907104\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.4457906484603882 acc:0.46000000834465027\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.4059876203536987 acc:0.4599999785423279\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.4077249765396118 acc:0.4599999785423279\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.343083381652832 acc:0.44999998807907104\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.304502248764038 acc:0.46000000834465027\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.2920490503311157 acc:0.44999998807907104\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.263545274734497 acc:0.4699999988079071\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.2414072751998901 acc:0.4399999678134918\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.207359790802002 acc:0.47999998927116394\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.1833378076553345 acc:0.47999998927116394\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.150385856628418 acc:0.47999995946884155\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.1266754865646362 acc:0.47999995946884155\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.1085293292999268 acc:0.5099999904632568\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.0748119354248047 acc:0.48000001907348633\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.0296876430511475 acc:0.49000000953674316\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.0187650918960571 acc:0.5099999904632568\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.004189372062683 acc:0.5199999809265137\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:0.9565466642379761 acc:0.5099999904632568\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:0.9390946626663208 acc:0.5399999618530273\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:0.894492506980896 acc:0.5199999809265137\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:0.8750162124633789 acc:0.5199999809265137\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:0.8748573660850525 acc:0.5299999713897705\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:0.8338221311569214 acc:0.5399999618530273\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:0.8461304903030396 acc:0.5199999809265137\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:0.8262377381324768 acc:0.5199999809265137\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:0.8121901154518127 acc:0.559999942779541\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:0.7921522855758667 acc:0.559999942779541\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:0.7486798167228699 acc:0.5399999618530273\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:0.7366176843643188 acc:0.5399999618530273\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:0.7302287817001343 acc:0.5499999523162842\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:0.6930360794067383 acc:0.5699999332427979\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:0.6880885362625122 acc:0.559999942779541\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:0.6918078064918518 acc:0.5399999618530273\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:0.672795832157135 acc:0.5499999523162842\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:0.6443469524383545 acc:0.5699999332427979\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:0.6342713236808777 acc:0.5699999332427979\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:0.6303519606590271 acc:0.5799999237060547\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:0.6167824268341064 acc:0.5499999523162842\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:0.5975375175476074 acc:0.5799999833106995\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:0.5829063653945923 acc:0.5499999523162842\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:0.564935028553009 acc:0.5299999713897705\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:0.5302915573120117 acc:0.5899999737739563\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:0.5496526956558228 acc:0.559999942779541\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:0.5281123518943787 acc:0.559999942779541\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:0.5088496208190918 acc:0.5799999833106995\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:0.49111852049827576 acc:0.5899999141693115\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:0.5010803937911987 acc:0.5699999332427979\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:0.47226959466934204 acc:0.5699999928474426\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:0.4864444136619568 acc:0.5899999737739563\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:0.45208418369293213 acc:0.5799999833106995\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:0.4391862154006958 acc:0.5899999737739563\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:0.4453544020652771 acc:0.5799999237060547\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:0.40228962898254395 acc:0.5699999332427979\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:0.4144664704799652 acc:0.5899999737739563\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:0.4016917943954468 acc:0.5799999237060547\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.390305757522583 acc:0.559999942779541\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.38478246331214905 acc:0.5799999833106995\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.38800251483917236 acc:0.5699999332427979\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.3621591329574585 acc:0.5899999141693115\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.352291464805603 acc:0.5399999618530273\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.34989285469055176 acc:0.5499999523162842\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.3509916067123413 acc:0.5499999523162842\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.3083513379096985 acc:0.5499999523162842\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.29988884925842285 acc:0.5799999833106995\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.30245548486709595 acc:0.5399999618530273\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.29801303148269653 acc:0.5499999523162842\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.28507256507873535 acc:0.5499999523162842\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.2771596312522888 acc:0.5699999928474426\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.2631343603134155 acc:0.5499999523162842\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.25558072328567505 acc:0.5499999523162842\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.2671261429786682 acc:0.559999942779541\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.24779418110847473 acc:0.5399999618530273\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.24184660613536835 acc:0.559999942779541\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.2171112447977066 acc:0.5499999523162842\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.22485938668251038 acc:0.5399999618530273\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.24161040782928467 acc:0.5499999523162842\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.19732683897018433 acc:0.559999942779541\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.19601836800575256 acc:0.5499999523162842\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.20086687803268433 acc:0.559999942779541\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.21096664667129517 acc:0.5799999237060547\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.17764580249786377 acc:0.5999999046325684\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.16792304813861847 acc:0.5499999523162842\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.17313526570796967 acc:0.559999942779541\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.18207038938999176 acc:0.5399999618530273\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.15934860706329346 acc:0.5399999618530273\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.14577248692512512 acc:0.559999942779541\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.1432374119758606 acc:0.5399999618530273\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.16190844774246216 acc:0.559999942779541\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.1509585976600647 acc:0.559999942779541\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.1431109756231308 acc:0.5799999237060547\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.1330314427614212 acc:0.559999942779541\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.1343686580657959 acc:0.559999942779541\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.13102823495864868 acc:0.5499999523162842\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.1375310719013214 acc:0.5699999332427979\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.11674560606479645 acc:0.5399999618530273\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.12342257797718048 acc:0.559999942779541\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.11492776870727539 acc:0.5799999237060547\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.12252304702997208 acc:0.5399999618530273\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.10641086846590042 acc:0.5899999737739563\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.10783793032169342 acc:0.5399999618530273\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.10544300079345703 acc:0.559999942779541\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.12257300317287445 acc:0.5499999523162842\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.08898453414440155 acc:0.559999942779541\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.08395634591579437 acc:0.559999942779541\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.10381713509559631 acc:0.559999942779541\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.08688945323228836 acc:0.559999942779541\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.08482150733470917 acc:0.5499999523162842\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.09416119009256363 acc:0.5399999618530273\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.07108653336763382 acc:0.5499999523162842\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.08693474531173706 acc:0.5499999523162842\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.08538079261779785 acc:0.5299999713897705\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.09487829357385635 acc:0.5499999523162842\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.08418291062116623 acc:0.5399999618530273\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.07570771872997284 acc:0.5499999523162842\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.06192300468683243 acc:0.559999942779541\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.0745224803686142 acc:0.5499999523162842\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.07151097804307938 acc:0.5499999523162842\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.05335366353392601 acc:0.559999942779541\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.07275904715061188 acc:0.5499999523162842\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.06093335151672363 acc:0.5499999523162842\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.07064877450466156 acc:0.559999942779541\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.06497880816459656 acc:0.5799999237060547\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.04648483172059059 acc:0.5799999237060547\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.052229031920433044 acc:0.5199999809265137\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.05902900546789169 acc:0.5399999618530273\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.053324513137340546 acc:0.5499999523162842\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.05906309187412262 acc:0.5399999618530273\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.06012614443898201 acc:0.559999942779541\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.04717505723237991 acc:0.5699999332427979\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.04191233590245247 acc:0.559999942779541\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.0498838797211647 acc:0.559999942779541\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.041919462382793427 acc:0.5299999713897705\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.04054073616862297 acc:0.5499999523162842\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.037384383380413055 acc:0.559999942779541\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.03965861350297928 acc:0.5399999618530273\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.04659336805343628 acc:0.559999942779541\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.03401456028223038 acc:0.559999942779541\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.03873157501220703 acc:0.5799999237060547\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.035871393978595734 acc:0.5699999332427979\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.03415434807538986 acc:0.5699999332427979\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.03836546838283539 acc:0.559999942779541\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.03737586736679077 acc:0.5399999618530273\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.02850598283112049 acc:0.559999942779541\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.03048885613679886 acc:0.5699999332427979\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.027177201583981514 acc:0.5399999618530273\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.025355633348226547 acc:0.559999942779541\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.030565373599529266 acc:0.5399999618530273\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.03742212429642677 acc:0.559999942779541\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.027309097349643707 acc:0.559999942779541\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.024443671107292175 acc:0.559999942779541\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.020628485828638077 acc:0.5699999332427979\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.02702811360359192 acc:0.5699999332427979\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.020587686449289322 acc:0.5399999618530273\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.020736772567033768 acc:0.5699999928474426\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.021027684211730957 acc:0.5499999523162842\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.02398410066962242 acc:0.5399999618530273\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.026544420048594475 acc:0.559999942779541\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.02144719660282135 acc:0.5600000023841858\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.017859328538179398 acc:0.5099999904632568\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.02203686349093914 acc:0.559999942779541\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.02342047169804573 acc:0.559999942779541\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.017604690045118332 acc:0.5699999928474426\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.015217384323477745 acc:0.5699999928474426\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.014924122020602226 acc:0.559999942779541\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.015809467062354088 acc:0.5899999737739563\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.016535824164748192 acc:0.559999942779541\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.019676469266414642 acc:0.5799999237060547\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.018905673176050186 acc:0.559999942779541\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.014775826595723629 acc:0.5499999523162842\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.015511812642216682 acc:0.5799999833106995\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.015242921188473701 acc:0.5499999523162842\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.011322167702019215 acc:0.559999942779541\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.013583049178123474 acc:0.5699999332427979\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.013568448834121227 acc:0.5799999833106995\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.013606775552034378 acc:0.5499999523162842\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.012411249801516533 acc:0.5499999523162842\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.013373180292546749 acc:0.5699999928474426\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.013688613660633564 acc:0.5699999928474426\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.014206399209797382 acc:0.5600000023841858\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.01106006558984518 acc:0.5499999523162842\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.014721940271556377 acc:0.5499999523162842\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.01107429713010788 acc:0.559999942779541\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.013039730489253998 acc:0.5699999332427979\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.01193639263510704 acc:0.5399999618530273\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.014050578698515892 acc:0.5299999713897705\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.0134152602404356 acc:0.5499999523162842\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.010433511808514595 acc:0.5499999523162842\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.010633809491991997 acc:0.5699999928474426\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.009322118945419788 acc:0.5299999713897705\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.1809823513031006 acc:0.20999999344348907\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:1.9891483783721924 acc:0.28999999165534973\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:1.6891634464263916 acc:0.3999999761581421\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:1.765767216682434 acc:0.35999998450279236\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:1.6742416620254517 acc:0.3699999749660492\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:1.8691996335983276 acc:0.3699999749660492\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:1.6229660511016846 acc:0.3799999952316284\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:1.4146628379821777 acc:0.38999998569488525\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:1.5294101238250732 acc:0.429999977350235\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:1.4804229736328125 acc:0.4399999678134918\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.6414817571640015 acc:0.3999999761581421\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:1.5030972957611084 acc:0.4099999666213989\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:1.2796196937561035 acc:0.4399999976158142\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:1.4036614894866943 acc:0.47999998927116394\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:1.393147349357605 acc:0.4999999701976776\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.537057638168335 acc:0.3999999761581421\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:1.3475724458694458 acc:0.4599999785423279\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:1.172949194908142 acc:0.5199999809265137\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:1.3415188789367676 acc:0.4899999797344208\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:1.300269603729248 acc:0.4699999690055847\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.484673023223877 acc:0.4599999785423279\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:1.2967463731765747 acc:0.4899999797344208\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:1.0661323070526123 acc:0.47999998927116394\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:1.2711973190307617 acc:0.4999999701976776\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:1.2482688426971436 acc:0.47999995946884155\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.4296531677246094 acc:0.5\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:1.217826247215271 acc:0.5399999618530273\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:1.0218298435211182 acc:0.4599999785423279\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:1.2010772228240967 acc:0.4999999701976776\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:1.1920509338378906 acc:0.4599999785423279\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.396916389465332 acc:0.5099999904632568\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:1.126653790473938 acc:0.5299999713897705\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:0.952713131904602 acc:0.4899999797344208\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:1.149802565574646 acc:0.5299999713897705\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:1.1248598098754883 acc:0.5\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.3312660455703735 acc:0.5199999809265137\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:1.0799965858459473 acc:0.5399999618530273\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:0.8768244981765747 acc:0.4999999701976776\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:1.1025490760803223 acc:0.5\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:1.1125152111053467 acc:0.5\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.28114652633667 acc:0.5299999713897705\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:1.034218430519104 acc:0.5299999713897705\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:0.8832001686096191 acc:0.47999995946884155\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:1.0740928649902344 acc:0.5299999713897705\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:1.0523262023925781 acc:0.5399999618530273\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.2478563785552979 acc:0.5799999237060547\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:0.9762618541717529 acc:0.5699999332427979\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:0.8490558862686157 acc:0.5\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:1.001440167427063 acc:0.5399999618530273\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:1.0137981176376343 acc:0.5299999713897705\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.2499406337738037 acc:0.5499999523162842\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:0.9685935974121094 acc:0.5499999523162842\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:0.8217824697494507 acc:0.5099999904632568\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:0.9758747816085815 acc:0.559999942779541\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:0.9630981683731079 acc:0.559999942779541\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.203068733215332 acc:0.5899999141693115\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:0.9263514280319214 acc:0.5699999332427979\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:0.783130407333374 acc:0.5399999618530273\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:0.9448443651199341 acc:0.5899999737739563\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:0.9502339363098145 acc:0.5699999332427979\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.1468242406845093 acc:0.559999942779541\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:0.8757486939430237 acc:0.5499999523162842\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:0.7578541040420532 acc:0.559999942779541\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:0.9183487296104431 acc:0.559999942779541\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:0.8879695534706116 acc:0.5399999618530273\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.1576893329620361 acc:0.5399999618530273\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:0.8487576842308044 acc:0.5499999523162842\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:0.7363306879997253 acc:0.5299999713897705\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:0.8928576707839966 acc:0.5999999642372131\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:0.862220048904419 acc:0.5799999237060547\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.120557427406311 acc:0.559999942779541\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:0.808061957359314 acc:0.5899999737739563\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:0.7184533476829529 acc:0.5399999618530273\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:0.8620434999465942 acc:0.6100000143051147\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:0.8285173177719116 acc:0.5799999833106995\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.0847705602645874 acc:0.5699999332427979\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:0.7816727757453918 acc:0.559999942779541\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:0.7180505990982056 acc:0.5799999237060547\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:0.8459885716438293 acc:0.6299999952316284\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:0.8490371108055115 acc:0.5699999928474426\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.0256232023239136 acc:0.5499999523162842\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:0.7682340145111084 acc:0.5699999928474426\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:0.6789092421531677 acc:0.5899999737739563\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:0.8092369437217712 acc:0.6399999856948853\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:0.7868021726608276 acc:0.5899999737739563\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.044100046157837 acc:0.559999942779541\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:0.7666663527488708 acc:0.5999999642372131\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:0.6727417707443237 acc:0.60999995470047\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:0.7920482158660889 acc:0.6200000047683716\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:0.7555157542228699 acc:0.5899999737739563\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.0023176670074463 acc:0.559999942779541\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:0.7317110896110535 acc:0.5799999833106995\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:0.6610774993896484 acc:0.5799999833106995\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:0.791090190410614 acc:0.6299999952316284\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:0.7690407037734985 acc:0.6499999761581421\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:0.9853127002716064 acc:0.6000000238418579\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:0.6939120888710022 acc:0.6000000238418579\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:0.6323384046554565 acc:0.6299999356269836\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:0.7614187002182007 acc:0.6200000047683716\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:0.7445541620254517 acc:0.60999995470047\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:0.9219574332237244 acc:0.6000000238418579\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:0.6845427751541138 acc:0.5899999737739563\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:0.6101455688476562 acc:0.6199999451637268\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:0.7470916509628296 acc:0.6499999761581421\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:0.7050353288650513 acc:0.6399999856948853\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:0.8940688371658325 acc:0.6199999451637268\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:0.6837474703788757 acc:0.6699999570846558\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:0.5937303900718689 acc:0.6399999856948853\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:0.7367962598800659 acc:0.6499999761581421\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:0.6680096387863159 acc:0.60999995470047\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:0.8737329244613647 acc:0.6299999952316284\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:0.6696354746818542 acc:0.6200000047683716\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:0.5628865957260132 acc:0.6699999570846558\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:0.6984658241271973 acc:0.6899999380111694\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:0.6437196731567383 acc:0.6200000047683716\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:0.8165392875671387 acc:0.6299999952316284\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:0.6425630450248718 acc:0.6499999761581421\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:0.5673239827156067 acc:0.6499999761581421\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:0.6543105840682983 acc:0.6899999380111694\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:0.6364009976387024 acc:0.6799999475479126\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:0.8145627975463867 acc:0.6199999451637268\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:0.6101874113082886 acc:0.6399999856948853\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:0.5568864941596985 acc:0.6399999856948853\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:0.66495680809021 acc:0.699999988079071\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:0.6037642955780029 acc:0.6699999570846558\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:0.8163526058197021 acc:0.6599999666213989\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:0.5952352285385132 acc:0.6499999761581421\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:0.5382382869720459 acc:0.6499999761581421\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:0.6380556225776672 acc:0.6899999380111694\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:0.5818809270858765 acc:0.6499999761581421\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:0.7813147306442261 acc:0.6499999761581421\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:0.5789254903793335 acc:0.6599999666213989\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:0.5115639567375183 acc:0.6999999284744263\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:0.6210756301879883 acc:0.7199999690055847\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:0.5854262709617615 acc:0.6499999761581421\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:0.7377083897590637 acc:0.6499999761581421\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:0.5853304266929626 acc:0.6499999761581421\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:0.4962948262691498 acc:0.6299999356269836\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:0.5978991389274597 acc:0.699999988079071\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:0.5801125168800354 acc:0.6899999380111694\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:0.7025763392448425 acc:0.6599999666213989\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:0.5819454193115234 acc:0.6699999570846558\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:0.4888548254966736 acc:0.6200000047683716\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:0.5973426103591919 acc:0.6899999380111694\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:0.5482959747314453 acc:0.6599999666213989\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:0.6971474885940552 acc:0.6699999570846558\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:0.5549979209899902 acc:0.6699999570846558\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:0.4680896997451782 acc:0.6899999380111694\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:0.5907143950462341 acc:0.699999988079071\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:0.5176267027854919 acc:0.6899999976158142\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:0.6923217177391052 acc:0.6599999666213989\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:0.5562328100204468 acc:0.7099999785423279\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:0.4531770944595337 acc:0.6499999761581421\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:0.5722468495368958 acc:0.7099999785423279\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:0.5183463096618652 acc:0.6699999570846558\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:0.6583354473114014 acc:0.6699999570846558\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:0.503572940826416 acc:0.6499999761581421\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:0.4579780399799347 acc:0.6699999570846558\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:0.5453363656997681 acc:0.6999999284744263\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:0.4984273910522461 acc:0.699999988079071\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:0.6580930948257446 acc:0.6699999570846558\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:0.5067728757858276 acc:0.6699999570846558\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:0.44992122054100037 acc:0.6699999570846558\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:0.5563921928405762 acc:0.6999999284744263\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:0.490052193403244 acc:0.699999988079071\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:0.6037248969078064 acc:0.6599999666213989\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:0.49709248542785645 acc:0.6899999380111694\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:0.4286520183086395 acc:0.6699999570846558\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:0.5014163255691528 acc:0.7299999594688416\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:0.44217750430107117 acc:0.6599999666213989\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:0.6013658046722412 acc:0.7099999785423279\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:0.48886993527412415 acc:0.6799999475479126\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:0.41263025999069214 acc:0.6499999761581421\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:0.4940580427646637 acc:0.6899999976158142\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:0.4601966142654419 acc:0.699999988079071\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:0.6050260066986084 acc:0.6899999380111694\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:0.48508548736572266 acc:0.6699999570846558\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:0.41504764556884766 acc:0.6899999380111694\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:0.5039920806884766 acc:0.6999999284744263\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:0.42592936754226685 acc:0.6799999475479126\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:0.5558120012283325 acc:0.699999988079071\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:0.4682311713695526 acc:0.7099999189376831\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:0.39666837453842163 acc:0.7099999189376831\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:0.4512510895729065 acc:0.7199999690055847\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:0.4229087829589844 acc:0.7099999785423279\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:0.5546996593475342 acc:0.6899999976158142\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:0.4482487440109253 acc:0.6899999380111694\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:0.386271595954895 acc:0.6899999976158142\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:0.4650003910064697 acc:0.6999999284744263\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:0.42415857315063477 acc:0.7300000190734863\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:0.5297282934188843 acc:0.6999999284744263\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:0.41639381647109985 acc:0.6899999380111694\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:0.3761630654335022 acc:0.699999988079071\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:0.44751864671707153 acc:0.7299999594688416\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:0.4206573963165283 acc:0.7200000286102295\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:0.5029085874557495 acc:0.6999999284744263\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:0.4359036684036255 acc:0.6799999475479126\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:0.37768882513046265 acc:0.6599999666213989\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:0.41869133710861206 acc:0.7599999904632568\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:0.4081932306289673 acc:0.6999999284744263\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:0.5012706518173218 acc:0.6999999284744263\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:0.39278507232666016 acc:0.7199999690055847\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:0.3464750051498413 acc:0.6899999380111694\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:0.41839560866355896 acc:0.7400000095367432\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:0.37897124886512756 acc:0.7099999785423279\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:0.502333402633667 acc:0.699999988079071\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:0.408740758895874 acc:0.6899999380111694\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:0.34187501668930054 acc:0.6999999284744263\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:0.4079316258430481 acc:0.7299999594688416\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:0.3891475796699524 acc:0.7399999499320984\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:0.4921015799045563 acc:0.7099999785423279\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:0.37633901834487915 acc:0.699999988079071\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:0.3443865180015564 acc:0.7299999594688416\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:0.3822866380214691 acc:0.7299999594688416\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:0.3691796660423279 acc:0.699999988079071\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:0.47511225938796997 acc:0.6899999380111694\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:0.3566173315048218 acc:0.7199999690055847\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:0.31980544328689575 acc:0.6799999475479126\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:0.3847551941871643 acc:0.7400000095367432\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:0.33949506282806396 acc:0.7199999690055847\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:0.44869717955589294 acc:0.699999988079071\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:0.3561287820339203 acc:0.7199999690055847\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:0.33080774545669556 acc:0.6699999570846558\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:0.37095916271209717 acc:0.6999999284744263\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:0.3589969575405121 acc:0.6899999976158142\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:0.4308210015296936 acc:0.7099999785423279\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:0.3465662896633148 acc:0.7299999594688416\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:0.3086036741733551 acc:0.6599999666213989\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:0.36485373973846436 acc:0.7299999594688416\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:0.3270290493965149 acc:0.7099999785423279\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:0.4186301529407501 acc:0.7099999785423279\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:0.3560171127319336 acc:0.699999988079071\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:0.2961485683917999 acc:0.6999999284744263\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:0.3621850609779358 acc:0.7199999690055847\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:0.3340577483177185 acc:0.7099999189376831\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:0.4189114570617676 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:0.3426373600959778 acc:0.699999988079071\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:0.2919080853462219 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:0.3498295843601227 acc:0.7099999785423279\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:0.2951948046684265 acc:0.7299999594688416\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:0.4093530774116516 acc:0.699999988079071\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:0.31837332248687744 acc:0.6999999284744263\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:0.3084915280342102 acc:0.6899999976158142\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:0.3186316192150116 acc:0.7299999594688416\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:0.287870317697525 acc:0.7199999690055847\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:0.40609538555145264 acc:0.7199999690055847\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:0.31073713302612305 acc:0.7399999499320984\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:0.2847580909729004 acc:0.6899999380111694\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:0.32131773233413696 acc:0.7199999690055847\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:0.3036881983280182 acc:0.7400000095367432\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:0.3908642530441284 acc:0.7099999785423279\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:0.29953616857528687 acc:0.7299999594688416\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:0.291108638048172 acc:0.6699999570846558\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:0.3115493953227997 acc:0.7199999690055847\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:0.2675520181655884 acc:0.7199999690055847\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:0.3633840084075928 acc:0.699999988079071\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:0.29689982533454895 acc:0.7199999690055847\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:0.2731305658817291 acc:0.6599999666213989\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:0.30494993925094604 acc:0.7299999594688416\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:0.27143749594688416 acc:0.7399999499320984\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:0.3844451904296875 acc:0.6899999976158142\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:0.2958980202674866 acc:0.7099999785423279\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:0.27651578187942505 acc:0.7099999189376831\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:0.2851235270500183 acc:0.7400000095367432\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:0.2638190984725952 acc:0.7199999690055847\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:0.34943002462387085 acc:0.7099999785423279\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:0.2923756539821625 acc:0.7099999785423279\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:0.26862072944641113 acc:0.6999999284744263\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:0.2904289662837982 acc:0.7199999690055847\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:0.25701433420181274 acc:0.7300000190734863\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:0.3318261206150055 acc:0.7099999785423279\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:0.28105178475379944 acc:0.7199999690055847\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:0.24971041083335876 acc:0.6899999380111694\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:0.28115949034690857 acc:0.7399999499320984\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:0.2592957615852356 acc:0.7099999785423279\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:0.34423694014549255 acc:0.699999988079071\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:0.27203062176704407 acc:0.7399999499320984\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:0.25461018085479736 acc:0.7299999594688416\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:0.3018420934677124 acc:0.7099999785423279\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:0.24967169761657715 acc:0.7099999189376831\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:0.34533989429473877 acc:0.699999988079071\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:0.26269346475601196 acc:0.699999988079071\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:0.2517174780368805 acc:0.6899999380111694\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:0.2789524793624878 acc:0.7199999690055847\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:0.25075680017471313 acc:0.7099999785423279\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:0.3278471827507019 acc:0.7099999785423279\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:0.2480568140745163 acc:0.699999988079071\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:0.2534933090209961 acc:0.7099999189376831\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:0.24020367860794067 acc:0.7099999785423279\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:0.24295684695243835 acc:0.7199999690055847\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:0.29905349016189575 acc:0.699999988079071\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:0.23628397285938263 acc:0.7299999594688416\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:0.23162560164928436 acc:0.7199999690055847\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:0.27280575037002563 acc:0.7099999785423279\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:0.2223929762840271 acc:0.7300000190734863\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:0.32868996262550354 acc:0.6899999976158142\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:0.239934504032135 acc:0.7299999594688416\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:0.22384628653526306 acc:0.7400000095367432\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:0.26645591855049133 acc:0.7400000095367432\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:0.2279353141784668 acc:0.7299999594688416\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:0.3106926679611206 acc:0.7099999785423279\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:0.22976264357566833 acc:0.7300000190734863\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:0.22808974981307983 acc:0.7199999690055847\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:0.23850922286510468 acc:0.7399999499320984\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:0.21783886849880219 acc:0.7199999690055847\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:0.2875531315803528 acc:0.7400000095367432\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:0.2300437092781067 acc:0.7099999785423279\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:0.24511194229125977 acc:0.6899999380111694\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:0.24292173981666565 acc:0.7199999690055847\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:0.21017658710479736 acc:0.699999988079071\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:0.28805363178253174 acc:0.699999988079071\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:0.2303680032491684 acc:0.7199999690055847\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:0.22660255432128906 acc:0.7099999785423279\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:0.22616054117679596 acc:0.7400000095367432\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:0.21505902707576752 acc:0.7400000095367432\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:0.27170825004577637 acc:0.7200000286102295\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:0.19558608531951904 acc:0.7399999499320984\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:0.20719312131404877 acc:0.7400000095367432\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:0.21927307546138763 acc:0.75\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:0.1957184374332428 acc:0.7199999690055847\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:0.272975355386734 acc:0.699999988079071\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:0.20248320698738098 acc:0.7099999785423279\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:0.2035350203514099 acc:0.7399999499320984\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:0.2265971153974533 acc:0.7599999904632568\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:0.21091732382774353 acc:0.7199999690055847\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:0.26026207208633423 acc:0.6800000071525574\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:0.202956423163414 acc:0.7199999690055847\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:0.20108355581760406 acc:0.7199999690055847\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:0.20013007521629333 acc:0.7199999690055847\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:0.21023458242416382 acc:0.7199999690055847\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.24041257798671722 acc:0.7199999690055847\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:0.18206581473350525 acc:0.7199999690055847\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:0.18928316235542297 acc:0.7200000286102295\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:0.200966477394104 acc:0.7300000190734863\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:0.18404638767242432 acc:0.7199999690055847\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.23593921959400177 acc:0.7099999785423279\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:0.17440587282180786 acc:0.7099999785423279\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:0.18204237520694733 acc:0.7299999594688416\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:0.2102590799331665 acc:0.7899999618530273\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:0.19928690791130066 acc:0.7299999594688416\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.23031410574913025 acc:0.699999988079071\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:0.18608911335468292 acc:0.7299999594688416\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:0.18538832664489746 acc:0.7400000095367432\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:0.18498165905475616 acc:0.7599999904632568\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:0.17476321756839752 acc:0.7199999690055847\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.23730018734931946 acc:0.6899999380111694\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:0.17951373755931854 acc:0.7099999785423279\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:0.18228773772716522 acc:0.6699999570846558\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:0.1909337043762207 acc:0.7499999403953552\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:0.1780109703540802 acc:0.7199999690055847\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.20575827360153198 acc:0.699999988079071\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:0.17028096318244934 acc:0.7199999690055847\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:0.16442763805389404 acc:0.7599999904632568\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:0.19257956743240356 acc:0.7400000095367432\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:0.17308363318443298 acc:0.75\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.204662024974823 acc:0.7099999785423279\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:0.17605066299438477 acc:0.7199999690055847\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:0.17558197677135468 acc:0.7299999594688416\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:0.17121313512325287 acc:0.7199999690055847\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:0.16241253912448883 acc:0.7299999594688416\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.21818476915359497 acc:0.699999988079071\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:0.16336849331855774 acc:0.75\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:0.16527268290519714 acc:0.7599999308586121\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:0.15743348002433777 acc:0.7299999594688416\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:0.17030824720859528 acc:0.75\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.22806145250797272 acc:0.7099999785423279\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:0.16613569855690002 acc:0.7199999690055847\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:0.1460743099451065 acc:0.75\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:0.16189175844192505 acc:0.7599999904632568\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:0.15363630652427673 acc:0.7599999904632568\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.1885073184967041 acc:0.699999988079071\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:0.17108970880508423 acc:0.7299999594688416\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:0.14233937859535217 acc:0.7299999594688416\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:0.16846615076065063 acc:0.75\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:0.17539574205875397 acc:0.7299999594688416\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.18075880408287048 acc:0.7400000095367432\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:0.13244587182998657 acc:0.7199999690055847\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:0.15980851650238037 acc:0.7199999690055847\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:0.15831972658634186 acc:0.7599999904632568\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:0.14866536855697632 acc:0.7400000095367432\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.19012048840522766 acc:0.7099999785423279\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:0.13648338615894318 acc:0.7299999594688416\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:0.14948341250419617 acc:0.7399999499320984\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:0.15727469325065613 acc:0.7199999690055847\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:0.14830948412418365 acc:0.7599999904632568\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.1756754070520401 acc:0.7099999785423279\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:0.14452531933784485 acc:0.7099999785423279\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:0.15116210281848907 acc:0.7299999594688416\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:0.15952621400356293 acc:0.7699999809265137\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:0.14497272670269012 acc:0.7299999594688416\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.19549047946929932 acc:0.7199999690055847\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:0.13025690615177155 acc:0.7099999785423279\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:0.14964446425437927 acc:0.7699999809265137\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:0.13613685965538025 acc:0.7599999308586121\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:0.13799355924129486 acc:0.7599999904632568\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.16221705079078674 acc:0.7300000190734863\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:0.1312805712223053 acc:0.7299999594688416\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:0.1377469301223755 acc:0.7399999499320984\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:0.1520002782344818 acc:0.75\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:0.13916096091270447 acc:0.7400000095367432\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.17209216952323914 acc:0.7299999594688416\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:0.12646088004112244 acc:0.7300000190734863\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:0.12619078159332275 acc:0.7399999499320984\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:0.1441645324230194 acc:0.75\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:0.14066985249519348 acc:0.7299999594688416\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.16432364284992218 acc:0.7299999594688416\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:0.12300200015306473 acc:0.7199999690055847\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:0.12877289950847626 acc:0.6999999284744263\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:0.1467795968055725 acc:0.7699999809265137\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:0.14952439069747925 acc:0.7399999499320984\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.15401393175125122 acc:0.7099999785423279\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:0.12141548097133636 acc:0.7099999189376831\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:0.11369955539703369 acc:0.7400000095367432\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:0.12918198108673096 acc:0.7499999403953552\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:0.12706615030765533 acc:0.7199999690055847\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.1515251100063324 acc:0.7299999594688416\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:0.1186310350894928 acc:0.75\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:0.11774444580078125 acc:0.7099999785423279\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:0.12422385066747665 acc:0.7599999904632568\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:0.12499365955591202 acc:0.7300000190734863\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.15482395887374878 acc:0.7199999690055847\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:0.11093498766422272 acc:0.7400000095367432\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:0.11756005883216858 acc:0.7400000095367432\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:0.1178881898522377 acc:0.7599999904632568\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:0.11652703583240509 acc:0.7399999499320984\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.16012230515480042 acc:0.699999988079071\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:0.1049148440361023 acc:0.7400000095367432\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:0.1241781935095787 acc:0.7299999594688416\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:0.1147473156452179 acc:0.7599999904632568\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:0.11828687787055969 acc:0.7400000095367432\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.14660906791687012 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:0.11982826143503189 acc:0.7400000095367432\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:0.11115308851003647 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:0.10129828006029129 acc:0.7599999904632568\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:0.12252918630838394 acc:0.7299999594688416\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.13188008964061737 acc:0.7099999785423279\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:0.09750702977180481 acc:0.7199999690055847\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:0.12314159423112869 acc:0.7099999785423279\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:0.13000242412090302 acc:0.7400000095367432\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:0.11257952451705933 acc:0.7099999785423279\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.15151792764663696 acc:0.7299999594688416\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:0.10463845729827881 acc:0.7300000190734863\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:0.10368674248456955 acc:0.7199999690055847\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:0.0929207056760788 acc:0.7499999403953552\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:0.11100541055202484 acc:0.7199999690055847\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.13760672509670258 acc:0.7400000095367432\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:0.10104496031999588 acc:0.7299999594688416\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:0.10504092276096344 acc:0.7799999713897705\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:0.1147153452038765 acc:0.7699999809265137\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:0.10749364644289017 acc:0.7599999904632568\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.12855874001979828 acc:0.7399999499320984\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:0.10602103173732758 acc:0.7099999785423279\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:0.1113019734621048 acc:0.7299999594688416\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:0.10475428402423859 acc:0.75\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:0.1103271096944809 acc:0.7199999690055847\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.1280803084373474 acc:0.7199999690055847\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:0.08830475062131882 acc:0.7099999785423279\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:0.1071728840470314 acc:0.7199999690055847\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:0.09324205666780472 acc:0.7299999594688416\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:0.09996476769447327 acc:0.75\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.11609974503517151 acc:0.7299999594688416\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:0.08797051757574081 acc:0.7299999594688416\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:0.0895792692899704 acc:0.7399999499320984\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:0.08867768198251724 acc:0.75\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:0.09588771313428879 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.11021929979324341 acc:0.7099999785423279\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:0.08540289103984833 acc:0.7499999403953552\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:0.08452104777097702 acc:0.7599999904632568\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:0.09562208503484726 acc:0.7699999809265137\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:0.08955162018537521 acc:0.7599999904632568\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.10863655805587769 acc:0.7399999499320984\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:0.08035693317651749 acc:0.75\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:0.09471962600946426 acc:0.7299999594688416\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:0.08952610939741135 acc:0.75\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:0.10945621132850647 acc:0.7400000095367432\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.12427958846092224 acc:0.7199999690055847\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:0.09583353996276855 acc:0.75\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:0.08556932210922241 acc:0.7399999499320984\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:0.07849663496017456 acc:0.7399999499320984\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:0.09296762943267822 acc:0.7300000190734863\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.10060665756464005 acc:0.7599999904632568\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:0.08288294076919556 acc:0.75\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:0.0798986554145813 acc:0.75\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:0.08715689182281494 acc:0.7599999308586121\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:0.0783119723200798 acc:0.699999988079071\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.09783685207366943 acc:0.7199999690055847\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:0.07620618492364883 acc:0.7299999594688416\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:0.0951966792345047 acc:0.6999999284744263\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:0.06634818017482758 acc:0.7299999594688416\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:0.08623167127370834 acc:0.7200000286102295\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.10108047723770142 acc:0.7199999690055847\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:0.0741310566663742 acc:0.7400000095367432\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:0.08301807194948196 acc:0.6899999380111694\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:0.0704929307103157 acc:0.7199999690055847\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:0.08179140090942383 acc:0.7400000095367432\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.0850866436958313 acc:0.75\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:0.06836757063865662 acc:0.75\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:0.0776057168841362 acc:0.7299998998641968\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:0.07449072599411011 acc:0.7299999594688416\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:0.08092784881591797 acc:0.7199999690055847\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.08728701621294022 acc:0.7299999594688416\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:0.07965849339962006 acc:0.75\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:0.08405663818120956 acc:0.7399999499320984\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:0.0761115625500679 acc:0.7599999904632568\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:0.08335071057081223 acc:0.7299999594688416\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.08836343884468079 acc:0.7699999809265137\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:0.08657810091972351 acc:0.75\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:0.07029730826616287 acc:0.7599999904632568\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:0.06501132994890213 acc:0.7599999904632568\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:0.06823034584522247 acc:0.7400000095367432\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.08907013386487961 acc:0.7099999785423279\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:0.0842604711651802 acc:0.7599999904632568\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:0.06530360877513885 acc:0.7299999594688416\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:0.059615522623062134 acc:0.7499999403953552\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:0.05774392932653427 acc:0.7400000095367432\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.08620061725378036 acc:0.7399999499320984\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:0.06463640928268433 acc:0.7099999785423279\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:0.08381322026252747 acc:0.7399999499320984\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:0.05760429799556732 acc:0.7599999904632568\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:0.06777657568454742 acc:0.7299999594688416\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.08707522600889206 acc:0.7599999308586121\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:0.05985548719763756 acc:0.7400000095367432\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:0.06845880299806595 acc:0.7399999499320984\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:0.08569909632205963 acc:0.7299999594688416\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:0.06685565412044525 acc:0.7199999690055847\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.0751156285405159 acc:0.7099999785423279\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:0.06491459906101227 acc:0.7099999785423279\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:0.0765291303396225 acc:0.7199999690055847\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:0.061306897550821304 acc:0.7699999809265137\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:0.05821283161640167 acc:0.75\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.08268076181411743 acc:0.7400000095367432\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:0.06396524608135223 acc:0.7400000095367432\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:0.05435698479413986 acc:0.75\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:0.06345468759536743 acc:0.75\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:0.06013386696577072 acc:0.7199999690055847\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.06591788679361343 acc:0.7400000095367432\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:0.058519355952739716 acc:0.75\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:0.055027350783348083 acc:0.7099999785423279\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:0.05260276049375534 acc:0.7400000095367432\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:0.06265605986118317 acc:0.699999988079071\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.07106030732393265 acc:0.7299999594688416\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:0.05792316421866417 acc:0.7399999499320984\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:0.06080987676978111 acc:0.75\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:0.05075027793645859 acc:0.7400000095367432\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:0.06169857457280159 acc:0.7599999904632568\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.06605417281389236 acc:0.7099999785423279\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:0.05703829228878021 acc:0.7599999904632568\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:0.04779587686061859 acc:0.7399999499320984\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:0.05029190331697464 acc:0.7599999904632568\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:0.06059320271015167 acc:0.7400000095367432\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.0774502158164978 acc:0.7699999809265137\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss:0.04776296392083168 acc:0.7599999904632568\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss:0.04142874851822853 acc:0.699999988079071\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss:0.050000738352537155 acc:0.75\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss:0.053125083446502686 acc:0.7300000190734863\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.05739327520132065 acc:0.7299999594688416\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss:0.051776476204395294 acc:0.7699999809265137\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss:0.044120218604803085 acc:0.7099999785423279\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss:0.05233926326036453 acc:0.7699999213218689\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss:0.059187501668930054 acc:0.7199999690055847\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.05591309070587158 acc:0.7499999403953552\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss:0.041138678789138794 acc:0.7199999690055847\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss:0.03833287954330444 acc:0.7699999809265137\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss:0.04927626997232437 acc:0.7499999403953552\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss:0.05229409039020538 acc:0.7300000190734863\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.06189940124750137 acc:0.7599999904632568\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss:0.05162665247917175 acc:0.7099999785423279\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss:0.049577511847019196 acc:0.7399999499320984\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss:0.05207648128271103 acc:0.7599999904632568\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss:0.05980198457837105 acc:0.7299999594688416\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.058967895805835724 acc:0.7400000095367432\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss:0.04831643030047417 acc:0.75\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss:0.040356218814849854 acc:0.7700000405311584\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss:0.03848706930875778 acc:0.7599999904632568\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss:0.05959977209568024 acc:0.75\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.05108585208654404 acc:0.7599999904632568\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss:0.044035568833351135 acc:0.7300000190734863\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss:0.0464043915271759 acc:0.75\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss:0.05802455544471741 acc:0.7599999904632568\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss:0.046782806515693665 acc:0.75\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.06078660488128662 acc:0.7199999690055847\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss:0.0443410649895668 acc:0.7599999904632568\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss:0.038468003273010254 acc:0.7199999690055847\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss:0.03804224729537964 acc:0.7799999713897705\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss:0.06313508749008179 acc:0.7499999403953552\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.04452973231673241 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss:0.04185373708605766 acc:0.7400000095367432\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss:0.03814130648970604 acc:0.7099999785423279\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss:0.04971526190638542 acc:0.7599999308586121\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss:0.053644150495529175 acc:0.75\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.05154773220419884 acc:0.75\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss:0.04101075604557991 acc:0.7599999904632568\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss:0.04056054726243019 acc:0.7799999713897705\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss:0.03368215262889862 acc:0.75\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss:0.03967513516545296 acc:0.7400000095367432\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.04767684265971184 acc:0.7400000095367432\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss:0.03872682899236679 acc:0.7299999594688416\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss:0.0304217841476202 acc:0.7599999904632568\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss:0.048877861350774765 acc:0.7399999499320984\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss:0.03422245755791664 acc:0.7399999499320984\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.042803213000297546 acc:0.7299999594688416\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss:0.03909505158662796 acc:0.7299999594688416\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss:0.04122375696897507 acc:0.7199999094009399\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss:0.04303948953747749 acc:0.7699999809265137\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss:0.037796299904584885 acc:0.7599999904632568\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.05316556990146637 acc:0.7599999904632568\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss:0.034411296248435974 acc:0.7400000095367432\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss:0.03066396340727806 acc:0.7599999904632568\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss:0.04083153232932091 acc:0.7299999594688416\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss:0.041199445724487305 acc:0.7699999809265137\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.04172889143228531 acc:0.75\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss:0.03906106576323509 acc:0.75\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss:0.03251632675528526 acc:0.7299999594688416\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss:0.038766033947467804 acc:0.7899999618530273\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss:0.04515736922621727 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.05727287381887436 acc:0.7599999904632568\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss:0.03140654414892197 acc:0.7099999785423279\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss:0.03327193111181259 acc:0.75\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss:0.03176233172416687 acc:0.7399999499320984\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss:0.03519044071435928 acc:0.75\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.042678602039813995 acc:0.7300000190734863\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss:0.034211501479148865 acc:0.7200000286102295\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss:0.029265208169817924 acc:0.75\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss:0.037069227546453476 acc:0.7699999809265137\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss:0.037370432168245316 acc:0.75\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.040034219622612 acc:0.7299999594688416\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss:0.03322795033454895 acc:0.7300000190734863\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss:0.02690971828997135 acc:0.7599999904632568\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss:0.029565131291747093 acc:0.7599999308586121\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss:0.03659491986036301 acc:0.7500000596046448\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.05763321742415428 acc:0.7400000095367432\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss:0.02998529188334942 acc:0.7199999690055847\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss:0.024962101131677628 acc:0.6999999284744263\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss:0.028858795762062073 acc:0.7599999904632568\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss:0.027074351906776428 acc:0.75\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.05031595006585121 acc:0.699999988079071\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss:0.03221854194998741 acc:0.7099999785423279\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss:0.02682400681078434 acc:0.7199999690055847\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss:0.027391482144594193 acc:0.7400000095367432\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss:0.03452223166823387 acc:0.7399999499320984\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.04307812452316284 acc:0.7499999403953552\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss:0.03506261855363846 acc:0.7699999809265137\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss:0.028907757252454758 acc:0.7099999189376831\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss:0.02594522200524807 acc:0.7599999308586121\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss:0.043261803686618805 acc:0.75\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.032018642872571945 acc:0.7399999499320984\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss:0.03398996591567993 acc:0.7199999690055847\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss:0.025508500635623932 acc:0.75\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss:0.028627032414078712 acc:0.7499999403953552\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss:0.023904792964458466 acc:0.7300000190734863\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.032575786113739014 acc:0.7599999904632568\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss:0.029837800189852715 acc:0.7199999690055847\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss:0.026981201022863388 acc:0.7699999809265137\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss:0.02857571840286255 acc:0.7399999499320984\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss:0.032645050436258316 acc:0.7400000095367432\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.02917744219303131 acc:0.7300000190734863\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss:0.032059717923402786 acc:0.7299999594688416\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss:0.023130077868700027 acc:0.7299999594688416\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss:0.030488647520542145 acc:0.75\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss:0.02540167048573494 acc:0.7099999785423279\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.037142928689718246 acc:0.75\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss:0.023395735770463943 acc:0.7199999690055847\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss:0.018894817680120468 acc:0.7199999690055847\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss:0.02399635873734951 acc:0.7499999403953552\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss:0.025250257924199104 acc:0.7399999499320984\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.03210340812802315 acc:0.75\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss:0.02878415398299694 acc:0.6899999380111694\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss:0.018547270447015762 acc:0.7599999904632568\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss:0.02647889032959938 acc:0.7499999403953552\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss:0.03238167613744736 acc:0.7199999690055847\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.028736209496855736 acc:0.7899999618530273\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss:0.030393892899155617 acc:0.7400000095367432\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss:0.018313497304916382 acc:0.7299999594688416\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss:0.030876604840159416 acc:0.7599999904632568\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss:0.029213551431894302 acc:0.7599999904632568\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.0315384566783905 acc:0.7699999809265137\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss:0.027210542932152748 acc:0.7300000190734863\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss:0.019786285236477852 acc:0.7399999499320984\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss:0.019934145733714104 acc:0.7599999308586121\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss:0.030685026198625565 acc:0.7400000095367432\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.033596817404031754 acc:0.75\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss:0.021376991644501686 acc:0.7399999499320984\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss:0.018485359847545624 acc:0.7299999594688416\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss:0.023456919938325882 acc:0.7799999713897705\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss:0.028700092807412148 acc:0.7399999499320984\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.035396821796894073 acc:0.75\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss:0.02682323381304741 acc:0.75\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss:0.018569348379969597 acc:0.7299999594688416\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss:0.032596953213214874 acc:0.7699999809265137\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss:0.018237676471471786 acc:0.7099999785423279\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.023231711238622665 acc:0.7699999809265137\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss:0.026566538959741592 acc:0.7599999904632568\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss:0.01880308985710144 acc:0.7400000095367432\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss:0.022747140377759933 acc:0.7199999094009399\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss:0.02131853997707367 acc:0.75\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.02243650145828724 acc:0.7599999904632568\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss:0.02561930939555168 acc:0.7399999499320984\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss:0.02017572522163391 acc:0.7299999594688416\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss:0.019433842971920967 acc:0.75\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss:0.029746025800704956 acc:0.7500000596046448\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.023560339584946632 acc:0.7599999904632568\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss:0.02679617516696453 acc:0.6899999380111694\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss:0.022833725437521935 acc:0.75\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss:0.01957097463309765 acc:0.7699999809265137\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss:0.019740041345357895 acc:0.7599999904632568\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.02739161252975464 acc:0.7699999809265137\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss:0.020626364275813103 acc:0.7199999690055847\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss:0.017343668267130852 acc:0.7399999499320984\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss:0.020656786859035492 acc:0.7299999594688416\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss:0.015356944873929024 acc:0.7600000500679016\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.023676514625549316 acc:0.75\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss:0.023085523396730423 acc:0.7399999499320984\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss:0.017266564071178436 acc:0.7699999809265137\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss:0.02976229041814804 acc:0.7699999809265137\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss:0.0162670835852623 acc:0.75\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.020494144409894943 acc:0.7599999904632568\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss:0.01749572530388832 acc:0.7099999785423279\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss:0.014254790730774403 acc:0.7399999499320984\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss:0.023880738765001297 acc:0.7699999809265137\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss:0.014337679371237755 acc:0.75\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.027905508875846863 acc:0.7799999713897705\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss:0.01728559285402298 acc:0.7400000095367432\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss:0.0194485392421484 acc:0.7499999403953552\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss:0.0249475184828043 acc:0.7599999904632568\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss:0.014284908771514893 acc:0.7300000190734863\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.024805039167404175 acc:0.7599999904632568\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss:0.018612509593367577 acc:0.7300000190734863\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss:0.014085729606449604 acc:0.7499999403953552\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss:0.0257854200899601 acc:0.7599999904632568\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss:0.01513612736016512 acc:0.75\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.022618364542722702 acc:0.7299999594688416\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss:0.017942722886800766 acc:0.75\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss:0.013600531965494156 acc:0.7399999499320984\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss:0.0134848328307271 acc:0.7599999308586121\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss:0.01180603913962841 acc:0.7300000190734863\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.023628734052181244 acc:0.75\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss:0.017443867400288582 acc:0.7300000190734863\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss:0.011040344834327698 acc:0.7700000405311584\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss:0.020930595695972443 acc:0.7299999594688416\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss:0.010246193036437035 acc:0.75\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.01719900593161583 acc:0.7400000095367432\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss:0.01549532637000084 acc:0.7400000095367432\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss:0.011068616062402725 acc:0.7099999785423279\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss:0.019264711067080498 acc:0.7499999403953552\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss:0.012092015706002712 acc:0.7699999809265137\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.020911235362291336 acc:0.7599999904632568\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss:0.0157014150172472 acc:0.7400000095367432\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss:0.013212960213422775 acc:0.7299999594688416\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss:0.021845007315278053 acc:0.7599999308586121\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss:0.012796787545084953 acc:0.7400000095367432\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.02486647292971611 acc:0.7600000500679016\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss:0.015502550639212132 acc:0.7099999785423279\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss:0.011882074177265167 acc:0.7700000405311584\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss:0.01518167182803154 acc:0.7400000095367432\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss:0.00837062019854784 acc:0.75\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.019090646877884865 acc:0.75\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss:0.018252426758408546 acc:0.7299999594688416\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss:0.00957854837179184 acc:0.7299999594688416\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss:0.011125704273581505 acc:0.7699999809265137\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss:0.010131826624274254 acc:0.7199999690055847\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.017022758722305298 acc:0.7400000095367432\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss:0.014781703241169453 acc:0.7399999499320984\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss:0.01156756840646267 acc:0.75\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss:0.02112438529729843 acc:0.7699999809265137\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss:0.015378294512629509 acc:0.7600000500679016\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.02342776022851467 acc:0.75\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss:0.017592182382941246 acc:0.7299999594688416\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss:0.009023010730743408 acc:0.7499999403953552\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss:0.019657166674733162 acc:0.7299999594688416\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss:0.009572446346282959 acc:0.7499999403953552\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.01923934929072857 acc:0.75\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss:0.018601354211568832 acc:0.7400000095367432\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss:0.009484119713306427 acc:0.7199999690055847\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss:0.013989345170557499 acc:0.7499999403953552\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss:0.011796783655881882 acc:0.7700000405311584\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.02258121781051159 acc:0.7399999499320984\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss:0.01763872429728508 acc:0.7299999594688416\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss:0.00888805091381073 acc:0.7400000095367432\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss:0.012655501253902912 acc:0.7599999904632568\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss:0.011944474652409554 acc:0.7499999403953552\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.020635923370718956 acc:0.7399999499320984\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss:0.016267506405711174 acc:0.7400000095367432\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss:0.010019472800195217 acc:0.7599999904632568\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss:0.02106454223394394 acc:0.7699999809265137\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss:0.0106995590031147 acc:0.75\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.02075663022696972 acc:0.7899999618530273\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss:0.014653368853032589 acc:0.7599999904632568\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss:0.00916941836476326 acc:0.75\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss:0.013943537138402462 acc:0.7399999499320984\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss:0.009823284111917019 acc:0.7399999499320984\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.019124988466501236 acc:0.7599999904632568\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss:0.017038244754076004 acc:0.7099999785423279\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss:0.010902000591158867 acc:0.7299999594688416\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss:0.011179016903042793 acc:0.7799999713897705\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss:0.005936003290116787 acc:0.7299999594688416\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.020944198593497276 acc:0.7599999904632568\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss:0.013641110621392727 acc:0.75\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss:0.008443550206720829 acc:0.7499999403953552\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss:0.011666398495435715 acc:0.7599999904632568\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss:0.012058902531862259 acc:0.7600000500679016\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.01674165576696396 acc:0.75\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss:0.012354160659015179 acc:0.7199999690055847\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss:0.009471279568970203 acc:0.7599999904632568\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss:0.01352263055741787 acc:0.7399999499320984\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss:0.009662964381277561 acc:0.7299999594688416\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.011188020929694176 acc:0.7599999904632568\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss:0.019886601716279984 acc:0.7199999690055847\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss:0.011607089079916477 acc:0.7499999403953552\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss:0.017793167382478714 acc:0.7599999904632568\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss:0.008051184937357903 acc:0.7399999499320984\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.01800372079014778 acc:0.7599999904632568\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss:0.015162033960223198 acc:0.7099999785423279\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss:0.012125627137720585 acc:0.7399999499320984\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss:0.01146143302321434 acc:0.7399999499320984\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss:0.007759741507470608 acc:0.75\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.01615282893180847 acc:0.7599999904632568\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss:0.0136587955057621 acc:0.7399999499320984\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss:0.01118822768330574 acc:0.7299999594688416\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss:0.009336115792393684 acc:0.75\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss:0.009717551060020924 acc:0.7400000095367432\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.013411262072622776 acc:0.7799999713897705\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss:0.012430532835423946 acc:0.7299999594688416\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss:0.007381469011306763 acc:0.7699999809265137\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss:0.01037149503827095 acc:0.7699999809265137\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss:0.008149772882461548 acc:0.7699999809265137\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.013454105705022812 acc:0.7699999809265137\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss:0.011229202151298523 acc:0.7199999690055847\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss:0.008680309168994427 acc:0.75\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss:0.01917979121208191 acc:0.7599999904632568\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss:0.0121842622756958 acc:0.75\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.012534822337329388 acc:0.75\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss:0.010429223999381065 acc:0.7299999594688416\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss:0.0065564559772610664 acc:0.7599999904632568\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss:0.010995378717780113 acc:0.7599999904632568\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss:0.010586144402623177 acc:0.7699999809265137\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.011667460203170776 acc:0.7799999713897705\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss:0.008360035717487335 acc:0.75\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss:0.005837355740368366 acc:0.7599999904632568\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss:0.015313920564949512 acc:0.7399999499320984\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss:0.005359030328691006 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.014518753625452518 acc:0.7599999904632568\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss:0.010713783092796803 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss:0.006364665925502777 acc:0.7599999308586121\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss:0.013084091246128082 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss:0.012147877365350723 acc:0.7599999904632568\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.008822746574878693 acc:0.75\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss:0.009312944486737251 acc:0.7299999594688416\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss:0.009064693003892899 acc:0.7399999499320984\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss:0.012292142026126385 acc:0.75\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss:0.008071724325418472 acc:0.7499999403953552\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.01613670028746128 acc:0.7499999403953552\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss:0.008710707537829876 acc:0.7399999499320984\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss:0.009022573009133339 acc:0.7499999403953552\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss:0.01657193712890148 acc:0.7499999403953552\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss:0.008541273884475231 acc:0.7199999690055847\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.01781594753265381 acc:0.7599999904632568\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss:0.010854148305952549 acc:0.699999988079071\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss:0.007126668468117714 acc:0.7399999499320984\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss:0.011751019395887852 acc:0.7599999904632568\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss:0.01000288687646389 acc:0.7499999403953552\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.010886264964938164 acc:0.7499999403953552\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss:0.008341411128640175 acc:0.7299999594688416\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss:0.0053701987490057945 acc:0.75\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss:0.009137471206486225 acc:0.7599999308586121\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss:0.0058186231181025505 acc:0.7400000095367432\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.010634936392307281 acc:0.7499999403953552\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss:0.00987282581627369 acc:0.7399999499320984\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss:0.005895579233765602 acc:0.7699999809265137\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss:0.008307036012411118 acc:0.7599999308586121\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss:0.006144740153104067 acc:0.75\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.00825407262891531 acc:0.7399999499320984\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss:0.009617096744477749 acc:0.7299999594688416\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss:0.00453685037791729 acc:0.7299999594688416\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss:0.007257903926074505 acc:0.7399999499320984\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss:0.005420120432972908 acc:0.7099999785423279\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.00925618875771761 acc:0.7599999308586121\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss:0.011397920548915863 acc:0.6899999380111694\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss:0.007107367739081383 acc:0.7599999904632568\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss:0.012936673127114773 acc:0.7399999499320984\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss:0.007841438986361027 acc:0.7299999594688416\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.009233616292476654 acc:0.7899999618530273\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss:0.00884857214987278 acc:0.7599999904632568\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss:0.006618772633373737 acc:0.7099999785423279\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss:0.010000059381127357 acc:0.7199999690055847\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss:0.006431443616747856 acc:0.7399999499320984\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.010747677646577358 acc:0.75\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss:0.007778013125061989 acc:0.7299999594688416\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss:0.006981152109801769 acc:0.75\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss:0.009734625928103924 acc:0.75\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss:0.004957790020853281 acc:0.7299999594688416\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.010597295127809048 acc:0.7299999594688416\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss:0.009080198593437672 acc:0.7199999690055847\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss:0.005915606860071421 acc:0.7599999904632568\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss:0.009064747020602226 acc:0.7399999499320984\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss:0.00772381154820323 acc:0.7599999904632568\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.019259227439761162 acc:0.7299999594688416\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss:0.006629372946918011 acc:0.7299999594688416\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss:0.00846417248249054 acc:0.7399999499320984\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss:0.011447323486208916 acc:0.7199999690055847\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss:0.006220054812729359 acc:0.75\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.004505084827542305 acc:0.7099999785423279\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss:0.012336255051195621 acc:0.7099999785423279\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss:0.00507739931344986 acc:0.7299999594688416\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss:0.00965879112482071 acc:0.7199999690055847\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss:0.004434570670127869 acc:0.7399999499320984\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.005420516710728407 acc:0.7299999594688416\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss:0.007450777105987072 acc:0.7299999594688416\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss:0.005590531975030899 acc:0.7099999785423279\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss:0.01007106713950634 acc:0.7299999594688416\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss:0.008414698764681816 acc:0.7399999499320984\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.005856722127646208 acc:0.7299999594688416\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss:0.008696145378053188 acc:0.699999988079071\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss:0.004927229136228561 acc:0.7399999499320984\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss:0.0076630557887256145 acc:0.7099999189376831\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss:0.003394457045942545 acc:0.7799999713897705\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.009904666803777218 acc:0.7399999499320984\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss:0.006151015870273113 acc:0.7200000286102295\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss:0.00578266941010952 acc:0.7200000286102295\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss:0.009610111825168133 acc:0.7299999594688416\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss:0.00533065851777792 acc:0.7399999499320984\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.00806556735187769 acc:0.7299999594688416\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss:0.006528802216053009 acc:0.7399999499320984\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss:0.005288043059408665 acc:0.7599999904632568\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss:0.00832456722855568 acc:0.7299999594688416\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss:0.00469193747267127 acc:0.7099999785423279\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.005567085463553667 acc:0.7299999594688416\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss:0.008184127509593964 acc:0.7099999785423279\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss:0.003973717801272869 acc:0.7099999785423279\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss:0.007364622317254543 acc:0.7399999499320984\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss:0.002836270024999976 acc:0.7299999594688416\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.012775684706866741 acc:0.7599999904632568\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss:0.00521106319501996 acc:0.7400000095367432\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss:0.0040899706073105335 acc:0.7499999403953552\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss:0.0058299764059484005 acc:0.7299999594688416\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss:0.003161489497870207 acc:0.7600000500679016\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.007178412284702063 acc:0.7199999690055847\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss:0.010192520916461945 acc:0.7299999594688416\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss:0.003568796208128333 acc:0.7499999403953552\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss:0.006824484560638666 acc:0.7299999594688416\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss:0.002951567992568016 acc:0.75\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.009835955686867237 acc:0.7399999499320984\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss:0.007105404045432806 acc:0.7599999904632568\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss:0.004763780161738396 acc:0.7399999499320984\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss:0.00744108110666275 acc:0.7199999690055847\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss:0.0022383318282663822 acc:0.7399999499320984\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.005481048487126827 acc:0.7399999499320984\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss:0.006561652757227421 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss:0.006307422183454037 acc:0.7299999594688416\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss:0.012300140224397182 acc:0.7299999594688416\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss:0.003836969844996929 acc:0.7400000095367432\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.004695566836744547 acc:0.7399999499320984\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss:0.006670244503766298 acc:0.7099999785423279\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss:0.004056117031723261 acc:0.7599999904632568\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss:0.009342307224869728 acc:0.6999999284744263\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss:0.003514639101922512 acc:0.7199999690055847\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.003453704295679927 acc:0.7599999904632568\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss:0.00553099112585187 acc:0.699999988079071\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss:0.004097867291420698 acc:0.7400000095367432\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss:0.006513850763440132 acc:0.7299999594688416\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss:0.0026878598146140575 acc:0.7399999499320984\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.010007422417402267 acc:0.7199999690055847\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss:0.005646590143442154 acc:0.7300000190734863\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss:0.008851529099047184 acc:0.75\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss:0.0053870887495577335 acc:0.7599999904632568\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss:0.0025474480353295803 acc:0.7299999594688416\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.005218998529016972 acc:0.7599999904632568\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss:0.008737035095691681 acc:0.7099999785423279\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss:0.004725063685327768 acc:0.7199999094009399\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss:0.006125192623585463 acc:0.75\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss:0.004586794413626194 acc:0.7199999690055847\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.00556257413700223 acc:0.7799999713897705\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss:0.006743254140019417 acc:0.7300000190734863\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss:0.0033059383276849985 acc:0.7499999403953552\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss:0.007972655817866325 acc:0.7299999594688416\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss:0.003977540414780378 acc:0.7199999690055847\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.007788326125591993 acc:0.7299999594688416\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss:0.007632359862327576 acc:0.7499999403953552\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss:0.002637797500938177 acc:0.7400000095367432\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss:0.007122080773115158 acc:0.7399999499320984\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss:0.0014854386681690812 acc:0.7299999594688416\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.004458639770746231 acc:0.7400000095367432\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss:0.006427236367017031 acc:0.699999988079071\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss:0.004289643839001656 acc:0.7299999594688416\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss:0.0055551305413246155 acc:0.699999988079071\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss:0.0025168920401483774 acc:0.7299999594688416\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.011222438886761665 acc:0.75\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss:0.0072688451036810875 acc:0.7099999785423279\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss:0.005342032294720411 acc:0.75\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss:0.005688963457942009 acc:0.75\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss:0.0023996371310204268 acc:0.7199999690055847\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.012828603386878967 acc:0.7199999690055847\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss:0.006098292768001556 acc:0.7400000095367432\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss:0.004100136458873749 acc:0.75\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss:0.006426317151635885 acc:0.7099999189376831\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss:0.002733459696173668 acc:0.7299999594688416\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.014764437451958656 acc:0.7199999690055847\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss:0.008800644427537918 acc:0.7399999499320984\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss:0.0032518585212528706 acc:0.7599999904632568\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss:0.005376655608415604 acc:0.6999999284744263\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss:0.0029494003392755985 acc:0.7400000095367432\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7028264331210191\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4f42a2e80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
