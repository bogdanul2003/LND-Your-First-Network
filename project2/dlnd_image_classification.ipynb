{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:46, 3.69MB/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 0:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+4ZgtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f2474dc18>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 0\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    tmp=np.ndarray(x.shape,dtype=float)\n",
    "    tmp[:] = x\n",
    "    tmp[...] /= 255.0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    ret = np.zeros((len(x),10))\n",
    "    for idx,lbl in enumerate(x):\n",
    "        ret[idx,lbl] = 1\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " [[[ 0.10980392  0.09803922  0.03921569]\n",
      "   [ 0.14509804  0.13333333  0.0745098 ]\n",
      "   [ 0.14901961  0.1372549   0.07843137]\n",
      "   ..., \n",
      "   [ 0.29803922  0.2627451   0.15294118]\n",
      "   [ 0.31764706  0.28235294  0.16862745]\n",
      "   [ 0.33333333  0.29803922  0.18431373]]\n",
      "\n",
      "  [[ 0.12941176  0.10980392  0.05098039]\n",
      "   [ 0.13333333  0.11764706  0.05490196]\n",
      "   [ 0.1254902   0.10588235  0.04705882]\n",
      "   ..., \n",
      "   [ 0.37254902  0.32156863  0.21568627]\n",
      "   [ 0.37647059  0.32156863  0.21960784]\n",
      "   [ 0.33333333  0.28235294  0.17647059]]\n",
      "\n",
      "  [[ 0.15294118  0.1254902   0.05882353]\n",
      "   [ 0.15686275  0.12941176  0.06666667]\n",
      "   [ 0.22352941  0.19607843  0.12941176]\n",
      "   ..., \n",
      "   [ 0.36470588  0.29803922  0.20392157]\n",
      "   [ 0.41960784  0.34901961  0.25882353]\n",
      "   [ 0.37254902  0.30196078  0.21176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.3254902   0.28627451  0.20392157]\n",
      "   [ 0.34117647  0.30196078  0.21960784]\n",
      "   [ 0.32941176  0.29019608  0.20392157]\n",
      "   ..., \n",
      "   [ 0.38823529  0.36470588  0.2745098 ]\n",
      "   [ 0.35294118  0.32941176  0.23921569]\n",
      "   [ 0.31764706  0.29411765  0.20392157]]\n",
      "\n",
      "  [[ 0.34509804  0.28235294  0.2       ]\n",
      "   [ 0.35294118  0.29019608  0.20392157]\n",
      "   [ 0.36470588  0.30196078  0.21960784]\n",
      "   ..., \n",
      "   [ 0.31372549  0.29019608  0.20784314]\n",
      "   [ 0.29803922  0.2745098   0.19215686]\n",
      "   [ 0.32156863  0.29803922  0.21568627]]\n",
      "\n",
      "  [[ 0.38039216  0.30588235  0.21960784]\n",
      "   [ 0.36862745  0.29411765  0.20784314]\n",
      "   [ 0.36470588  0.29411765  0.20784314]\n",
      "   ..., \n",
      "   [ 0.21176471  0.18431373  0.10980392]\n",
      "   [ 0.24705882  0.21960784  0.14509804]\n",
      "   [ 0.28235294  0.25490196  0.18039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.66666667  0.70588235  0.77647059]\n",
      "   [ 0.65882353  0.69803922  0.76862745]\n",
      "   [ 0.69411765  0.7254902   0.79607843]\n",
      "   ..., \n",
      "   [ 0.63529412  0.70196078  0.84313725]\n",
      "   [ 0.61960784  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]]\n",
      "\n",
      "  [[ 0.65882353  0.70980392  0.77647059]\n",
      "   [ 0.6745098   0.7254902   0.78823529]\n",
      "   [ 0.67058824  0.71764706  0.78431373]\n",
      "   ..., \n",
      "   [ 0.62352941  0.69411765  0.83137255]\n",
      "   [ 0.61176471  0.69019608  0.82745098]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  [[ 0.60392157  0.66666667  0.72941176]\n",
      "   [ 0.58431373  0.64705882  0.70980392]\n",
      "   [ 0.50588235  0.56470588  0.63529412]\n",
      "   ..., \n",
      "   [ 0.63137255  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.32941176  0.31372549]\n",
      "   [ 0.29803922  0.33333333  0.31764706]\n",
      "   [ 0.30588235  0.33333333  0.32156863]\n",
      "   ..., \n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.28235294  0.29411765]\n",
      "   [ 0.23921569  0.25490196  0.26666667]]\n",
      "\n",
      "  [[ 0.26666667  0.29803922  0.30196078]\n",
      "   [ 0.27058824  0.30196078  0.30588235]\n",
      "   [ 0.28235294  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.29803922  0.31372549  0.3254902 ]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.27843137  0.29411765  0.30588235]]\n",
      "\n",
      "  [[ 0.2627451   0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.29803922  0.30980392]\n",
      "   [ 0.27058824  0.29411765  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30980392  0.32156863]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.28627451  0.30196078  0.31372549]]]]\n"
     ]
    }
   ],
   "source": [
    "filename = 'preprocess_batch_' + str(1) + '.p'\n",
    "features, labels = pickle.load(open(filename, mode='rb'))\n",
    "print(features[0:5,...])\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    (width, height, chann) = image_shape\n",
    "    x = tf.placeholder(tf.float32, [None, width, height, chann], name='x')\n",
    "    print(x.get_shape())\n",
    "    # TODO: Implement Function\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y= tf.placeholder(tf.int8, [None,n_classes], name='y')\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tmp = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(type(x_tensor.get_shape().as_list()[3] ))\n",
    "    print(conv_ksize)\n",
    "    (convk1, convk2) = conv_ksize\n",
    "    (convs1, convs2) = conv_strides\n",
    "    (poolk1, poolk2) = pool_ksize\n",
    "    (pools1, pools2) = pool_strides\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([convk1, convk2,x_tensor.get_shape().as_list()[3],conv_num_outputs ], stddev=0.1, mean=0.0,dtype=tf.float32, seed=None, name=None ))\n",
    "    biases= tf.Variable(tf.constant(0,dtype=tf.float32, shape=[conv_num_outputs]))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=x_tensor, filter= weights, strides = [1,convs1, convs2,1],padding='SAME')\n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer += biases\n",
    "    #print(layer.get_shape().as_list())\n",
    "    \n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer = tf.nn.relu(layer)\n",
    "    layer = tf.nn.max_pool(value=layer,ksize=[1,poolk1,poolk2,1], \n",
    "                           strides = [1,pools1,pools2,1],padding='SAME')\n",
    "    \n",
    "    #print(layer.get_shape().as_list())\n",
    "    #print(conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 10 30 6\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    [batch_size, h,w,c] = x_tensor.get_shape().as_list()\n",
    "    features= h*w*c\n",
    "    \n",
    "    layer = tf.reshape(x_tensor, [-1, features])\n",
    "    print(batch_size,h,w,c)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    layer = tf.nn.relu(layer)\n",
    "    #layer = tf.sigmoid(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convl = conv2d_maxpool(x,20, (5,5), (1,1), (2,2), (2,2))\n",
    "    convl = conv2d_maxpool(convl,40, (5,5), (1,1), (2,2), (2,2))\n",
    "    #convl = conv2d_maxpool(convl,64, (5,5), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    layer = flatten(convl)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    print(layer.get_shape().as_list())\n",
    "    layer = fully_conn(layer,250)\n",
    "    layer = tf.nn.dropout(layer, keep_prob)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,250)\n",
    "#     layer = tf.layers.dropout(layer, 0.5)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    layer= output(layer,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01,epsilon=0.00001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(type(valid_features), type(valid_labels), type(feature_batch), type(label_batch))\n",
    "    #print(\"print_stats valid_features:{} valid_labels:{}\".format(valid_features.shape, valid_labels.shape))\n",
    "    \n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    valid = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features[0:100,...],\n",
    "        y: valid_labels[0:100,...],\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    print('loss:{} acc:{}'.format( loss,valid))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.164783477783203 acc:0.26999998092651367\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.05415678024292 acc:0.3799999952316284\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.9705619812011719 acc:0.38999998569488525\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.8800562620162964 acc:0.4099999666213989\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.8100510835647583 acc:0.3999999761581421\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.72808837890625 acc:0.4099999666213989\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.6664652824401855 acc:0.3999999761581421\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.6265156269073486 acc:0.4299999475479126\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.578696370124817 acc:0.429999977350235\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.5329322814941406 acc:0.429999977350235\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.4884045124053955 acc:0.44999998807907104\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.4457906484603882 acc:0.46000000834465027\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.4059876203536987 acc:0.4599999785423279\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.4077249765396118 acc:0.4599999785423279\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.343083381652832 acc:0.44999998807907104\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.304502248764038 acc:0.46000000834465027\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.2920490503311157 acc:0.44999998807907104\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.263545274734497 acc:0.4699999988079071\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.2414072751998901 acc:0.4399999678134918\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.207359790802002 acc:0.47999998927116394\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.1833378076553345 acc:0.47999998927116394\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.150385856628418 acc:0.47999995946884155\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.1266754865646362 acc:0.47999995946884155\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.1085293292999268 acc:0.5099999904632568\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.0748119354248047 acc:0.48000001907348633\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.0296876430511475 acc:0.49000000953674316\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.0187650918960571 acc:0.5099999904632568\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.004189372062683 acc:0.5199999809265137\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:0.9565466642379761 acc:0.5099999904632568\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:0.9390946626663208 acc:0.5399999618530273\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:0.894492506980896 acc:0.5199999809265137\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:0.8750162124633789 acc:0.5199999809265137\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:0.8748573660850525 acc:0.5299999713897705\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:0.8338221311569214 acc:0.5399999618530273\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:0.8461304903030396 acc:0.5199999809265137\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:0.8262377381324768 acc:0.5199999809265137\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:0.8121901154518127 acc:0.559999942779541\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:0.7921522855758667 acc:0.559999942779541\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:0.7486798167228699 acc:0.5399999618530273\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:0.7366176843643188 acc:0.5399999618530273\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:0.7302287817001343 acc:0.5499999523162842\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:0.6930360794067383 acc:0.5699999332427979\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:0.6880885362625122 acc:0.559999942779541\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:0.6918078064918518 acc:0.5399999618530273\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:0.672795832157135 acc:0.5499999523162842\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:0.6443469524383545 acc:0.5699999332427979\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:0.6342713236808777 acc:0.5699999332427979\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:0.6303519606590271 acc:0.5799999237060547\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:0.6167824268341064 acc:0.5499999523162842\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:0.5975375175476074 acc:0.5799999833106995\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:0.5829063653945923 acc:0.5499999523162842\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:0.564935028553009 acc:0.5299999713897705\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:0.5302915573120117 acc:0.5899999737739563\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:0.5496526956558228 acc:0.559999942779541\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:0.5281123518943787 acc:0.559999942779541\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:0.5088496208190918 acc:0.5799999833106995\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:0.49111852049827576 acc:0.5899999141693115\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:0.5010803937911987 acc:0.5699999332427979\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:0.47226959466934204 acc:0.5699999928474426\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:0.4864444136619568 acc:0.5899999737739563\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:0.45208418369293213 acc:0.5799999833106995\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:0.4391862154006958 acc:0.5899999737739563\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:0.4453544020652771 acc:0.5799999237060547\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:0.40228962898254395 acc:0.5699999332427979\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:0.4144664704799652 acc:0.5899999737739563\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:0.4016917943954468 acc:0.5799999237060547\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.390305757522583 acc:0.559999942779541\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.38478246331214905 acc:0.5799999833106995\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.38800251483917236 acc:0.5699999332427979\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.3621591329574585 acc:0.5899999141693115\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.352291464805603 acc:0.5399999618530273\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.34989285469055176 acc:0.5499999523162842\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.3509916067123413 acc:0.5499999523162842\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.3083513379096985 acc:0.5499999523162842\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.29988884925842285 acc:0.5799999833106995\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.30245548486709595 acc:0.5399999618530273\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.29801303148269653 acc:0.5499999523162842\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.28507256507873535 acc:0.5499999523162842\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.2771596312522888 acc:0.5699999928474426\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.2631343603134155 acc:0.5499999523162842\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.25558072328567505 acc:0.5499999523162842\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.2671261429786682 acc:0.559999942779541\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.24779418110847473 acc:0.5399999618530273\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.24184660613536835 acc:0.559999942779541\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.2171112447977066 acc:0.5499999523162842\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.22485938668251038 acc:0.5399999618530273\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.24161040782928467 acc:0.5499999523162842\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.19732683897018433 acc:0.559999942779541\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.19601836800575256 acc:0.5499999523162842\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.20086687803268433 acc:0.559999942779541\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.21096664667129517 acc:0.5799999237060547\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.17764580249786377 acc:0.5999999046325684\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.16792304813861847 acc:0.5499999523162842\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.17313526570796967 acc:0.559999942779541\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.18207038938999176 acc:0.5399999618530273\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.15934860706329346 acc:0.5399999618530273\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.14577248692512512 acc:0.559999942779541\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.1432374119758606 acc:0.5399999618530273\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.16190844774246216 acc:0.559999942779541\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.1509585976600647 acc:0.559999942779541\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.1431109756231308 acc:0.5799999237060547\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.1330314427614212 acc:0.559999942779541\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.1343686580657959 acc:0.559999942779541\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.13102823495864868 acc:0.5499999523162842\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.1375310719013214 acc:0.5699999332427979\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.11674560606479645 acc:0.5399999618530273\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.12342257797718048 acc:0.559999942779541\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.11492776870727539 acc:0.5799999237060547\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.12252304702997208 acc:0.5399999618530273\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.10641086846590042 acc:0.5899999737739563\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.10783793032169342 acc:0.5399999618530273\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.10544300079345703 acc:0.559999942779541\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.12257300317287445 acc:0.5499999523162842\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.08898453414440155 acc:0.559999942779541\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.08395634591579437 acc:0.559999942779541\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.10381713509559631 acc:0.559999942779541\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.08688945323228836 acc:0.559999942779541\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.08482150733470917 acc:0.5499999523162842\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.09416119009256363 acc:0.5399999618530273\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.07108653336763382 acc:0.5499999523162842\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.08693474531173706 acc:0.5499999523162842\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.08538079261779785 acc:0.5299999713897705\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.09487829357385635 acc:0.5499999523162842\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.08418291062116623 acc:0.5399999618530273\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.07570771872997284 acc:0.5499999523162842\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.06192300468683243 acc:0.559999942779541\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.0745224803686142 acc:0.5499999523162842\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.07151097804307938 acc:0.5499999523162842\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.05335366353392601 acc:0.559999942779541\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.07275904715061188 acc:0.5499999523162842\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.06093335151672363 acc:0.5499999523162842\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.07064877450466156 acc:0.559999942779541\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.06497880816459656 acc:0.5799999237060547\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.04648483172059059 acc:0.5799999237060547\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.052229031920433044 acc:0.5199999809265137\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.05902900546789169 acc:0.5399999618530273\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.053324513137340546 acc:0.5499999523162842\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.05906309187412262 acc:0.5399999618530273\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.06012614443898201 acc:0.559999942779541\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.04717505723237991 acc:0.5699999332427979\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.04191233590245247 acc:0.559999942779541\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.0498838797211647 acc:0.559999942779541\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.041919462382793427 acc:0.5299999713897705\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.04054073616862297 acc:0.5499999523162842\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.037384383380413055 acc:0.559999942779541\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.03965861350297928 acc:0.5399999618530273\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.04659336805343628 acc:0.559999942779541\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.03401456028223038 acc:0.559999942779541\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.03873157501220703 acc:0.5799999237060547\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.035871393978595734 acc:0.5699999332427979\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.03415434807538986 acc:0.5699999332427979\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.03836546838283539 acc:0.559999942779541\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.03737586736679077 acc:0.5399999618530273\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.02850598283112049 acc:0.559999942779541\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.03048885613679886 acc:0.5699999332427979\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.027177201583981514 acc:0.5399999618530273\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.025355633348226547 acc:0.559999942779541\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.030565373599529266 acc:0.5399999618530273\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.03742212429642677 acc:0.559999942779541\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.027309097349643707 acc:0.559999942779541\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.024443671107292175 acc:0.559999942779541\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.020628485828638077 acc:0.5699999332427979\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.02702811360359192 acc:0.5699999332427979\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.020587686449289322 acc:0.5399999618530273\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.020736772567033768 acc:0.5699999928474426\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.021027684211730957 acc:0.5499999523162842\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.02398410066962242 acc:0.5399999618530273\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.026544420048594475 acc:0.559999942779541\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.02144719660282135 acc:0.5600000023841858\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.017859328538179398 acc:0.5099999904632568\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.02203686349093914 acc:0.559999942779541\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.02342047169804573 acc:0.559999942779541\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.017604690045118332 acc:0.5699999928474426\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.015217384323477745 acc:0.5699999928474426\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.014924122020602226 acc:0.559999942779541\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.015809467062354088 acc:0.5899999737739563\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.016535824164748192 acc:0.559999942779541\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.019676469266414642 acc:0.5799999237060547\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.018905673176050186 acc:0.559999942779541\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.014775826595723629 acc:0.5499999523162842\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.015511812642216682 acc:0.5799999833106995\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.015242921188473701 acc:0.5499999523162842\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.011322167702019215 acc:0.559999942779541\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.013583049178123474 acc:0.5699999332427979\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.013568448834121227 acc:0.5799999833106995\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.013606775552034378 acc:0.5499999523162842\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.012411249801516533 acc:0.5499999523162842\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.013373180292546749 acc:0.5699999928474426\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.013688613660633564 acc:0.5699999928474426\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.014206399209797382 acc:0.5600000023841858\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.01106006558984518 acc:0.5499999523162842\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.014721940271556377 acc:0.5499999523162842\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.01107429713010788 acc:0.559999942779541\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.013039730489253998 acc:0.5699999332427979\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.01193639263510704 acc:0.5399999618530273\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.014050578698515892 acc:0.5299999713897705\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.0134152602404356 acc:0.5499999523162842\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.010433511808514595 acc:0.5499999523162842\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.010633809491991997 acc:0.5699999928474426\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.009322118945419788 acc:0.5299999713897705\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.159066677093506 acc:0.26999998092651367\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:1.9141128063201904 acc:0.32999998331069946\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:1.6279646158218384 acc:0.38999998569488525\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:1.6410681009292603 acc:0.3799999952316284\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:1.6921014785766602 acc:0.4599999785423279\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:1.8294450044631958 acc:0.3999999761581421\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:1.5749764442443848 acc:0.41999998688697815\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:1.3218328952789307 acc:0.4399999976158142\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:1.5032296180725098 acc:0.429999977350235\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:1.477388620376587 acc:0.4399999678134918\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.7125756740570068 acc:0.4099999666213989\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:1.5185766220092773 acc:0.4399999976158142\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:1.193320393562317 acc:0.4399999976158142\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:1.3729304075241089 acc:0.4699999690055847\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:1.3571174144744873 acc:0.5\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.5976744890213013 acc:0.44999998807907104\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:1.4438899755477905 acc:0.4599999785423279\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:1.126339077949524 acc:0.4599999785423279\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:1.2388412952423096 acc:0.47999998927116394\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:1.2868578433990479 acc:0.47999995946884155\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.481764554977417 acc:0.4599999785423279\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:1.394415259361267 acc:0.4899999797344208\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:1.030045986175537 acc:0.4899999499320984\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:1.185631513595581 acc:0.49000000953674316\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:1.2116888761520386 acc:0.5299999713897705\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.4547648429870605 acc:0.47999995946884155\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:1.3364782333374023 acc:0.4899999797344208\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:0.9922921657562256 acc:0.47999998927116394\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:1.094831943511963 acc:0.4899999797344208\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:1.161549687385559 acc:0.4899999499320984\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.3654168844223022 acc:0.4899999499320984\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:1.2472848892211914 acc:0.5299999713897705\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:0.9042361974716187 acc:0.5199999809265137\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:1.0472297668457031 acc:0.47999998927116394\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:1.0908032655715942 acc:0.5299999713897705\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.342015027999878 acc:0.559999942779541\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:1.2150895595550537 acc:0.5199999809265137\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:0.8605294227600098 acc:0.5399999618530273\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:0.9944133162498474 acc:0.5299999713897705\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:1.0460240840911865 acc:0.5499999523162842\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.2758814096450806 acc:0.5299999713897705\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:1.1450469493865967 acc:0.5499999523162842\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:0.8654111623764038 acc:0.5699999332427979\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:0.9408910870552063 acc:0.5799999833106995\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:1.0176548957824707 acc:0.559999942779541\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.259670376777649 acc:0.559999942779541\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:1.122275710105896 acc:0.5799999237060547\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:0.8109219074249268 acc:0.5800000429153442\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:0.909913957118988 acc:0.5799999833106995\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:0.96584552526474 acc:0.5699999332427979\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.2223013639450073 acc:0.5799999237060547\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:1.0706965923309326 acc:0.5699999332427979\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:0.8009287118911743 acc:0.5799999833106995\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:0.8602558374404907 acc:0.5799999833106995\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:0.9348269104957581 acc:0.6000000238418579\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.149512529373169 acc:0.5799999237060547\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:1.0226078033447266 acc:0.559999942779541\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:0.7575411796569824 acc:0.6100000143051147\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:0.8315145969390869 acc:0.5899999737739563\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:0.9107168316841125 acc:0.6000000238418579\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.1360764503479004 acc:0.6299999952316284\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:1.0117974281311035 acc:0.5799999237060547\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:0.7622338533401489 acc:0.5699999332427979\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:0.8081036806106567 acc:0.6299999952316284\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:0.8574482798576355 acc:0.6200000047683716\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.0965375900268555 acc:0.5999999642372131\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:0.9647620320320129 acc:0.6000000238418579\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:0.6971080899238586 acc:0.5999999642372131\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:0.7814688682556152 acc:0.6299999952316284\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:0.8353242874145508 acc:0.6299999952316284\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.028914451599121 acc:0.6399999856948853\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:0.9313409328460693 acc:0.6100000143051147\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:0.6839700937271118 acc:0.5999999642372131\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:0.7723214030265808 acc:0.6499999761581421\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:0.8087786436080933 acc:0.6299999952316284\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:0.978919506072998 acc:0.6499999761581421\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:0.9294517040252686 acc:0.5999999642372131\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:0.6697270274162292 acc:0.60999995470047\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:0.7246556282043457 acc:0.6499999761581421\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:0.7575197815895081 acc:0.6499999761581421\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:0.954004168510437 acc:0.6399999856948853\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:0.8612855076789856 acc:0.6299999952316284\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:0.6188071370124817 acc:0.6499999761581421\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:0.7198736667633057 acc:0.6499999761581421\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:0.7653375864028931 acc:0.6599999666213989\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:0.9202874898910522 acc:0.6599999666213989\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:0.8559331893920898 acc:0.6499999761581421\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:0.6019312739372253 acc:0.6599999666213989\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:0.71510249376297 acc:0.6499999761581421\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:0.7268833518028259 acc:0.6799999475479126\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:0.9167538285255432 acc:0.6299999952316284\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:0.8140994310379028 acc:0.6599999666213989\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:0.5945004224777222 acc:0.6599999666213989\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:0.6953396797180176 acc:0.6699999570846558\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:0.6941807270050049 acc:0.6699999570846558\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:0.8958821892738342 acc:0.6399999856948853\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:0.7853748202323914 acc:0.6499999761581421\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:0.5948505401611328 acc:0.6599999666213989\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:0.6436887979507446 acc:0.6899999976158142\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:0.6896700859069824 acc:0.6499999761581421\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:0.8559494018554688 acc:0.6399999856948853\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:0.7795292735099792 acc:0.6399999856948853\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:0.5615905523300171 acc:0.6799999475479126\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:0.6537595391273499 acc:0.6499999761581421\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:0.672512412071228 acc:0.6599999666213989\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:0.8386952877044678 acc:0.6499999761581421\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:0.7454453706741333 acc:0.6399999856948853\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:0.5334432125091553 acc:0.6599999666213989\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:0.6298031806945801 acc:0.6599999666213989\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:0.6603137254714966 acc:0.6699999570846558\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:0.7988162636756897 acc:0.6499999761581421\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:0.7241373062133789 acc:0.6399999856948853\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:0.5230576395988464 acc:0.6499999761581421\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:0.6051161885261536 acc:0.6699999570846558\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:0.6103227138519287 acc:0.6799999475479126\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:0.7913638949394226 acc:0.6499999761581421\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:0.7232736349105835 acc:0.6499999761581421\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:0.5258592963218689 acc:0.6699999570846558\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:0.5901564359664917 acc:0.6899999976158142\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:0.5917588472366333 acc:0.6899999380111694\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:0.7643259167671204 acc:0.6699999570846558\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:0.6855905055999756 acc:0.6599999666213989\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:0.49042731523513794 acc:0.6899999976158142\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:0.5803873538970947 acc:0.6800000071525574\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:0.5818582773208618 acc:0.6799999475479126\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:0.7570295929908752 acc:0.6599999666213989\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:0.6542813181877136 acc:0.6699999570846558\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:0.4730288088321686 acc:0.6800000071525574\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:0.5609104633331299 acc:0.6899999976158142\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:0.5471433401107788 acc:0.7099999785423279\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:0.7258940935134888 acc:0.6699999570846558\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:0.6239042282104492 acc:0.6699999570846558\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:0.466800332069397 acc:0.6799999475479126\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:0.5588234066963196 acc:0.699999988079071\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:0.5502692461013794 acc:0.699999988079071\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:0.6840801239013672 acc:0.6799999475479126\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:0.6122813820838928 acc:0.6899999380111694\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:0.43906131386756897 acc:0.6999999284744263\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:0.5547550320625305 acc:0.6599999666213989\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:0.5281397104263306 acc:0.6999999284744263\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:0.7365776300430298 acc:0.6899999380111694\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:0.6081652045249939 acc:0.699999988079071\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:0.42484691739082336 acc:0.7099999785423279\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:0.5302440524101257 acc:0.7199999690055847\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:0.5282934904098511 acc:0.7299999594688416\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:0.6736563444137573 acc:0.6799999475479126\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:0.5716807842254639 acc:0.6399999856948853\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:0.42149871587753296 acc:0.6899999976158142\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:0.5104255080223083 acc:0.7199999690055847\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:0.4916825592517853 acc:0.7199999690055847\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:0.670391857624054 acc:0.6899999380111694\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:0.5937272906303406 acc:0.699999988079071\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:0.3941480815410614 acc:0.6799999475479126\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:0.499752938747406 acc:0.7099999785423279\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:0.4864512085914612 acc:0.7099999785423279\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:0.6664483547210693 acc:0.699999988079071\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:0.5532116293907166 acc:0.7099999785423279\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:0.38100865483283997 acc:0.6800000071525574\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:0.473129540681839 acc:0.7099999785423279\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:0.46439337730407715 acc:0.7099999785423279\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:0.6568267345428467 acc:0.7199999690055847\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:0.5417971014976501 acc:0.7099999785423279\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:0.37082383036613464 acc:0.699999988079071\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:0.4761770963668823 acc:0.7099999785423279\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:0.4538552463054657 acc:0.7199999690055847\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:0.6419245004653931 acc:0.699999988079071\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:0.5098519921302795 acc:0.6799999475479126\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:0.355184942483902 acc:0.6899999976158142\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:0.47472453117370605 acc:0.7199999690055847\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:0.44227176904678345 acc:0.7199999690055847\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:0.6222292184829712 acc:0.7099999785423279\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:0.5104921460151672 acc:0.699999988079071\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:0.35756561160087585 acc:0.6799999475479126\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:0.4616450071334839 acc:0.7199999690055847\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:0.4365275502204895 acc:0.7299999594688416\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:0.5893344283103943 acc:0.7199999690055847\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:0.5023744106292725 acc:0.6899999976158142\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:0.3371068835258484 acc:0.7299999594688416\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:0.45792216062545776 acc:0.7299999594688416\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:0.43122339248657227 acc:0.699999988079071\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:0.6053158640861511 acc:0.699999988079071\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:0.4760175049304962 acc:0.6799999475479126\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:0.3522716164588928 acc:0.7199999690055847\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:0.4230012893676758 acc:0.7299999594688416\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:0.4227973222732544 acc:0.7299999594688416\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:0.5416445136070251 acc:0.7299999594688416\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:0.46528300642967224 acc:0.7099999785423279\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:0.3268168568611145 acc:0.699999988079071\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:0.4219852387905121 acc:0.7099999785423279\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:0.39817339181900024 acc:0.7299999594688416\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:0.521194338798523 acc:0.7099999785423279\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:0.4427146017551422 acc:0.699999988079071\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:0.33544090390205383 acc:0.6599999666213989\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:0.40931791067123413 acc:0.7299999594688416\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:0.3817296028137207 acc:0.7199999690055847\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:0.5357438325881958 acc:0.7199999690055847\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:0.4341279864311218 acc:0.7299999594688416\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:0.30499404668807983 acc:0.7199999690055847\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:0.41543474793434143 acc:0.7299999594688416\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:0.3799454867839813 acc:0.699999988079071\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:0.4914235472679138 acc:0.7099999785423279\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:0.42234429717063904 acc:0.7099999785423279\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:0.29809653759002686 acc:0.6799999475479126\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:0.3920385241508484 acc:0.7299999594688416\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:0.35474613308906555 acc:0.7399999499320984\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:0.5117559432983398 acc:0.699999988079071\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:0.42003437876701355 acc:0.7199999690055847\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:0.29025930166244507 acc:0.6699999570846558\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:0.38264137506484985 acc:0.7299999594688416\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:0.3600761890411377 acc:0.7299999594688416\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:0.4907548129558563 acc:0.6899999976158142\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:0.4091024398803711 acc:0.7099999785423279\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:0.27642059326171875 acc:0.6800000071525574\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:0.37482380867004395 acc:0.7099999785423279\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:0.35694611072540283 acc:0.7399999499320984\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:0.46304166316986084 acc:0.699999988079071\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:0.3998193144798279 acc:0.6899999380111694\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:0.2806212604045868 acc:0.6699999570846558\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:0.36170101165771484 acc:0.7299999594688416\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:0.3368103504180908 acc:0.7099999785423279\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:0.4630286693572998 acc:0.7199999690055847\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:0.3926807641983032 acc:0.6899999380111694\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:0.27609172463417053 acc:0.6800000071525574\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:0.34890007972717285 acc:0.7199999690055847\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:0.31686559319496155 acc:0.7599999308586121\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:0.449474573135376 acc:0.7299999594688416\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:0.38352805376052856 acc:0.7099999785423279\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:0.28059622645378113 acc:0.699999988079071\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:0.3511448800563812 acc:0.7399999499320984\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:0.3108190596103668 acc:0.7199999690055847\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:0.45606881380081177 acc:0.7099999189376831\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:0.36257171630859375 acc:0.7199999690055847\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:0.26033806800842285 acc:0.7099999785423279\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:0.32929179072380066 acc:0.7199999094009399\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:0.308459609746933 acc:0.699999988079071\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:0.4143991470336914 acc:0.699999988079071\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:0.3723462224006653 acc:0.6699999570846558\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:0.25379645824432373 acc:0.7199999690055847\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:0.31716179847717285 acc:0.6899999380111694\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:0.30465537309646606 acc:0.7299999594688416\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:0.38660261034965515 acc:0.6999999284744263\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:0.34752893447875977 acc:0.6999999284744263\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:0.22565233707427979 acc:0.6699999570846558\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:0.3225097358226776 acc:0.7299999594688416\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:0.29425573348999023 acc:0.7599999308586121\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:0.4062879681587219 acc:0.7199999690055847\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:0.3396587371826172 acc:0.6699999570846558\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:0.23022595047950745 acc:0.6699999570846558\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:0.31040239334106445 acc:0.7199999690055847\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:0.2943240702152252 acc:0.7199999690055847\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:0.3825964331626892 acc:0.7099999785423279\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:0.33085185289382935 acc:0.7099999785423279\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:0.22217313945293427 acc:0.6599999666213989\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:0.32663899660110474 acc:0.7799999713897705\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:0.2928541302680969 acc:0.6699999570846558\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:0.35622987151145935 acc:0.7099999785423279\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:0.35782551765441895 acc:0.6899999380111694\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:0.22146648168563843 acc:0.6800000071525574\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:0.29186391830444336 acc:0.7499999403953552\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:0.2657632827758789 acc:0.7199999690055847\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:0.3554508686065674 acc:0.7199999690055847\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:0.3194112181663513 acc:0.6899999380111694\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:0.2165578007698059 acc:0.6799999475479126\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:0.2845236659049988 acc:0.7199999690055847\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:0.26076191663742065 acc:0.7299999594688416\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:0.37316930294036865 acc:0.7299999594688416\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:0.3062363266944885 acc:0.6799999475479126\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:0.19827285408973694 acc:0.6899999976158142\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:0.2643555700778961 acc:0.699999988079071\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:0.24490121006965637 acc:0.7199999690055847\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:0.37993091344833374 acc:0.7199999690055847\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:0.2757391631603241 acc:0.6799999475479126\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:0.20252679288387299 acc:0.6899999380111694\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:0.27512413263320923 acc:0.7099999785423279\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:0.24772508442401886 acc:0.699999988079071\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:0.3551793098449707 acc:0.7099999785423279\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:0.3009573221206665 acc:0.7199999690055847\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:0.1845635175704956 acc:0.6499999761581421\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:0.2529527544975281 acc:0.7399999499320984\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:0.26499783992767334 acc:0.699999988079071\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:0.3593723177909851 acc:0.6899999380111694\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:0.2809765338897705 acc:0.6999999284744263\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:0.1813357025384903 acc:0.6499999761581421\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:0.2351750135421753 acc:0.7299999594688416\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:0.23832590878009796 acc:0.7099999189376831\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:0.3335167169570923 acc:0.6899999380111694\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:0.24451708793640137 acc:0.6699999570846558\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:0.18088890612125397 acc:0.6699999570846558\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:0.2463064193725586 acc:0.7299999594688416\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:0.23516932129859924 acc:0.7099999189376831\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:0.3050967752933502 acc:0.699999988079071\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:0.26795974373817444 acc:0.699999988079071\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:0.17413444817066193 acc:0.6699999570846558\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:0.2264719158411026 acc:0.75\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:0.23524948954582214 acc:0.7299999594688416\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:0.29788047075271606 acc:0.699999988079071\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:0.2352406084537506 acc:0.7099999785423279\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:0.18447843194007874 acc:0.6599999666213989\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:0.23605453968048096 acc:0.699999988079071\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:0.2273397445678711 acc:0.7099999785423279\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:0.3196773827075958 acc:0.6899999380111694\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:0.25555741786956787 acc:0.7199999690055847\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:0.1719164103269577 acc:0.6799999475479126\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:0.22976422309875488 acc:0.7299999594688416\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:0.20185434818267822 acc:0.7199999690055847\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:0.29362788796424866 acc:0.699999988079071\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:0.2424839437007904 acc:0.6899999380111694\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:0.1582140475511551 acc:0.6799999475479126\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:0.21749931573867798 acc:0.7499999403953552\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:0.20634572207927704 acc:0.7299999594688416\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:0.2872620224952698 acc:0.699999988079071\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:0.2362155318260193 acc:0.7099999785423279\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:0.17585520446300507 acc:0.6599999666213989\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:0.2028258740901947 acc:0.7299999594688416\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:0.21408426761627197 acc:0.7599999308586121\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:0.25903841853141785 acc:0.699999988079071\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:0.2154942750930786 acc:0.7199999690055847\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:0.1465781182050705 acc:0.7099999785423279\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:0.20976194739341736 acc:0.7299999594688416\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:0.18842291831970215 acc:0.7299999594688416\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:0.26585203409194946 acc:0.6899999380111694\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:0.22445319592952728 acc:0.699999988079071\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:0.15844079852104187 acc:0.6799999475479126\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:0.21157735586166382 acc:0.7399999499320984\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:0.18624594807624817 acc:0.7299999594688416\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:0.2580450773239136 acc:0.7099999785423279\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:0.20647010207176208 acc:0.699999988079071\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:0.14983698725700378 acc:0.6899999976158142\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:0.19586780667304993 acc:0.7299999594688416\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:0.19055888056755066 acc:0.7399999499320984\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.23902006447315216 acc:0.7199999690055847\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:0.21894028782844543 acc:0.7199999690055847\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:0.15553045272827148 acc:0.6899999976158142\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:0.1951718032360077 acc:0.7399999499320984\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:0.17910878360271454 acc:0.7299999594688416\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.22530251741409302 acc:0.6899999976158142\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:0.1986984759569168 acc:0.7099999785423279\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:0.14392073452472687 acc:0.6700000166893005\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:0.18190604448318481 acc:0.7499999403953552\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:0.16880227625370026 acc:0.7299999594688416\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.23096361756324768 acc:0.7199999690055847\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:0.19492968916893005 acc:0.7099999785423279\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:0.15622574090957642 acc:0.7099999785423279\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:0.1724139153957367 acc:0.7399999499320984\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:0.17633214592933655 acc:0.7399999499320984\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.22937563061714172 acc:0.7099999785423279\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:0.18942487239837646 acc:0.7299999594688416\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:0.1398836374282837 acc:0.6799999475479126\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:0.17289108037948608 acc:0.7199999690055847\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:0.15779519081115723 acc:0.7299999594688416\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.20426952838897705 acc:0.7199999690055847\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:0.17302587628364563 acc:0.7099999785423279\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:0.13375240564346313 acc:0.6599999666213989\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:0.15951426327228546 acc:0.7199999690055847\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:0.15952540934085846 acc:0.7299999594688416\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.22776955366134644 acc:0.7299999594688416\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:0.1583038717508316 acc:0.7099999785423279\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:0.12291990965604782 acc:0.6599999666213989\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:0.16469328105449677 acc:0.7099999785423279\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:0.17268070578575134 acc:0.7099999785423279\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.1926880180835724 acc:0.6899999380111694\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:0.1610986292362213 acc:0.699999988079071\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:0.12293526530265808 acc:0.6899999380111694\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:0.15494060516357422 acc:0.699999988079071\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:0.16169068217277527 acc:0.7299999594688416\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.19440224766731262 acc:0.7199999690055847\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:0.17926499247550964 acc:0.7099999785423279\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:0.11877937614917755 acc:0.6599999666213989\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:0.148912250995636 acc:0.7199999690055847\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:0.15458518266677856 acc:0.7199999690055847\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.1994057595729828 acc:0.6899999380111694\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:0.1540631353855133 acc:0.699999988079071\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:0.12066326290369034 acc:0.6800000071525574\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:0.15174700319766998 acc:0.699999988079071\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:0.17291966080665588 acc:0.699999988079071\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.19603551924228668 acc:0.7199999690055847\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:0.14887735247612 acc:0.7099999785423279\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:0.10384196788072586 acc:0.6699999570846558\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:0.1452927440404892 acc:0.7399999499320984\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:0.14025194942951202 acc:0.7299999594688416\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.18405088782310486 acc:0.699999988079071\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:0.17432136833667755 acc:0.7199999690055847\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:0.12270450592041016 acc:0.6499999761581421\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:0.13774551451206207 acc:0.7099999785423279\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:0.14251452684402466 acc:0.7199999690055847\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.1766875833272934 acc:0.7199999690055847\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:0.14270900189876556 acc:0.7299999594688416\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:0.11508005112409592 acc:0.6999999284744263\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:0.13455641269683838 acc:0.7199999690055847\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:0.13679102063179016 acc:0.7099999785423279\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.1819627285003662 acc:0.7199999690055847\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:0.1669366955757141 acc:0.7099999785423279\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:0.10701115429401398 acc:0.6599999666213989\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:0.14306984841823578 acc:0.7299999594688416\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:0.14957956969738007 acc:0.7199999690055847\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.15794146060943604 acc:0.7099999785423279\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:0.1599038988351822 acc:0.7299999594688416\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:0.10815320909023285 acc:0.6899999976158142\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:0.14238277077674866 acc:0.6899999380111694\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:0.13942453265190125 acc:0.6899999976158142\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.17026576399803162 acc:0.7099999785423279\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:0.14344529807567596 acc:0.7099999785423279\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:0.1060987189412117 acc:0.6399999856948853\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:0.12562036514282227 acc:0.7199999690055847\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:0.11757767200469971 acc:0.7399999499320984\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.14684268832206726 acc:0.7199999690055847\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:0.1453598290681839 acc:0.6899999380111694\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:0.11078548431396484 acc:0.6699999570846558\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:0.14369913935661316 acc:0.7299999594688416\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:0.1173853725194931 acc:0.7199999690055847\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.1517384946346283 acc:0.6999999284744263\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:0.12990650534629822 acc:0.6899999380111694\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:0.10479405522346497 acc:0.6699999570846558\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:0.1223779022693634 acc:0.7399999499320984\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:0.12206874042749405 acc:0.7199999690055847\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.13099241256713867 acc:0.7099999785423279\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:0.12924519181251526 acc:0.7099999785423279\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:0.10156061500310898 acc:0.6599999666213989\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:0.1282452642917633 acc:0.7299999594688416\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:0.109072744846344 acc:0.7299999594688416\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.12572401762008667 acc:0.7099999785423279\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:0.1227179765701294 acc:0.7299999594688416\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:0.08447322249412537 acc:0.6699999570846558\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:0.11960159987211227 acc:0.7299999594688416\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:0.11702477186918259 acc:0.7199999690055847\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.13025104999542236 acc:0.6999999284744263\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:0.11103089898824692 acc:0.6899999380111694\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:0.08790706098079681 acc:0.699999988079071\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:0.11885375529527664 acc:0.699999988079071\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:0.09964066743850708 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.13229309022426605 acc:0.7099999785423279\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:0.10529084503650665 acc:0.6699999570846558\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:0.08876737952232361 acc:0.6999999284744263\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:0.13271895051002502 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:0.1106605976819992 acc:0.7199999690055847\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.11347228288650513 acc:0.7099999785423279\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:0.10537192225456238 acc:0.699999988079071\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:0.08885277807712555 acc:0.6899999380111694\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:0.12930406630039215 acc:0.7099999785423279\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:0.10111047327518463 acc:0.7199999690055847\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.11618936061859131 acc:0.7199999690055847\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:0.13201379776000977 acc:0.7199999690055847\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:0.09580350667238235 acc:0.6499999761581421\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:0.11071517318487167 acc:0.7199999690055847\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:0.09472613036632538 acc:0.7099999785423279\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.1096138283610344 acc:0.7199999690055847\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:0.13013723492622375 acc:0.7199999690055847\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:0.1004781574010849 acc:0.6499999761581421\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:0.09814132750034332 acc:0.7099999785423279\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:0.10065574944019318 acc:0.7399999499320984\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.11041010171175003 acc:0.7099999785423279\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:0.0960807204246521 acc:0.7099999785423279\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:0.09133081138134003 acc:0.6799999475479126\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:0.12947922945022583 acc:0.699999988079071\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:0.09096384048461914 acc:0.699999988079071\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.10289278626441956 acc:0.7099999189376831\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:0.10931183397769928 acc:0.7099999189376831\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:0.06906996667385101 acc:0.6899999380111694\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:0.11145909130573273 acc:0.7099999785423279\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:0.08740171790122986 acc:0.7199999690055847\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.10196101665496826 acc:0.7299999594688416\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:0.09086787700653076 acc:0.7099999785423279\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:0.07274797558784485 acc:0.6799999475479126\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:0.1049606055021286 acc:0.7299999594688416\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:0.10098518431186676 acc:0.6799999475479126\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.10555129498243332 acc:0.7300000190734863\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:0.093333899974823 acc:0.6899999976158142\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:0.07473582774400711 acc:0.699999988079071\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:0.1216157078742981 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:0.09143835306167603 acc:0.7099999785423279\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.09504462778568268 acc:0.7199999690055847\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:0.09347228705883026 acc:0.7099999785423279\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:0.06130629777908325 acc:0.6899999380111694\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:0.10499618202447891 acc:0.7099999785423279\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:0.09293379634618759 acc:0.6799999475479126\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.08524016290903091 acc:0.7299999594688416\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:0.08885914832353592 acc:0.6800000071525574\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:0.06567023694515228 acc:0.6899999380111694\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:0.08296101540327072 acc:0.699999988079071\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:0.08473873138427734 acc:0.7399999499320984\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.0920185074210167 acc:0.7199999690055847\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:0.06586329638957977 acc:0.6800000071525574\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:0.07026374340057373 acc:0.6699999570846558\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:0.09568421542644501 acc:0.7099999785423279\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:0.08845783770084381 acc:0.699999988079071\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.08381381630897522 acc:0.699999988079071\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:0.0900546982884407 acc:0.6699999570846558\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:0.050566643476486206 acc:0.7099999785423279\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:0.09664623439311981 acc:0.7499999403953552\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:0.07625579088926315 acc:0.6999999284744263\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.0784345492720604 acc:0.7099999785423279\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:0.08382231742143631 acc:0.6899999976158142\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:0.0579800121486187 acc:0.7199999094009399\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:0.09385526925325394 acc:0.7099999785423279\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:0.08409661054611206 acc:0.7299999594688416\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.07500682026147842 acc:0.6999999284744263\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:0.0581711009144783 acc:0.6899999380111694\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:0.06338979303836823 acc:0.6899999380111694\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:0.11007753014564514 acc:0.7199999690055847\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:0.07182086259126663 acc:0.7299999594688416\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.09374061226844788 acc:0.7199999690055847\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:0.06507658213376999 acc:0.6899999976158142\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:0.06413068622350693 acc:0.6999999284744263\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:0.0768248438835144 acc:0.699999988079071\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:0.06887923926115036 acc:0.7099999189376831\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.06792301684617996 acc:0.7299999594688416\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:0.07057181000709534 acc:0.6799999475479126\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:0.04867042973637581 acc:0.7299999594688416\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:0.11114427447319031 acc:0.7299999594688416\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:0.06035146489739418 acc:0.7199999690055847\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.07006388902664185 acc:0.6999999284744263\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:0.06025101616978645 acc:0.6800000071525574\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:0.0643211156129837 acc:0.6899999976158142\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:0.11688641458749771 acc:0.6999999284744263\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:0.05747902765870094 acc:0.7399999499320984\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.06152835488319397 acc:0.6999999284744263\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:0.06302002817392349 acc:0.699999988079071\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:0.05367277190089226 acc:0.6999999284744263\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:0.09335512667894363 acc:0.6999999284744263\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:0.06240902841091156 acc:0.7099999189376831\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.07088151574134827 acc:0.7099999785423279\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:0.06613948196172714 acc:0.6699999570846558\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:0.05487244203686714 acc:0.6799999475479126\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:0.07803825289011002 acc:0.699999988079071\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:0.06869936734437943 acc:0.6899999380111694\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.06462003290653229 acc:0.6999999284744263\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:0.06433326005935669 acc:0.6599999666213989\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:0.04773200303316116 acc:0.6999999284744263\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:0.08487923443317413 acc:0.7099999785423279\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:0.05370601266622543 acc:0.7199999690055847\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.05779106914997101 acc:0.7199999690055847\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:0.059792645275592804 acc:0.6699999570846558\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:0.0435585156083107 acc:0.7199999690055847\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:0.08065497130155563 acc:0.7099999785423279\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:0.049946632236242294 acc:0.7099999189376831\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.06863999366760254 acc:0.6999999284744263\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:0.055363092571496964 acc:0.6899999380111694\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:0.044428691267967224 acc:0.699999988079071\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:0.07244822382926941 acc:0.6899999380111694\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:0.05409197881817818 acc:0.6999999284744263\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.056424662470817566 acc:0.7299999594688416\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:0.056212060153484344 acc:0.6699999570846558\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:0.04612363874912262 acc:0.6599999666213989\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:0.06898035109043121 acc:0.7099999785423279\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:0.05903734639286995 acc:0.7099999785423279\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.06339577585458755 acc:0.7199999690055847\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:0.04631202667951584 acc:0.6699999570846558\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:0.04294275492429733 acc:0.6599999666213989\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:0.07010441273450851 acc:0.6899999380111694\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:0.050570182502269745 acc:0.6899999976158142\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.051829058676958084 acc:0.7199999690055847\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss:0.05235651880502701 acc:0.699999988079071\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss:0.0361141711473465 acc:0.7199999094009399\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss:0.07634815573692322 acc:0.6799999475479126\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss:0.048601336777210236 acc:0.7399999499320984\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.05401021987199783 acc:0.7099999785423279\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss:0.050656914710998535 acc:0.6899999380111694\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss:0.03707842528820038 acc:0.7199999690055847\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss:0.08036218583583832 acc:0.7099999189376831\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss:0.04533810913562775 acc:0.7099999785423279\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.058310866355895996 acc:0.7099999189376831\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss:0.04300094395875931 acc:0.6799999475479126\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss:0.03040342405438423 acc:0.6899999380111694\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss:0.0789029523730278 acc:0.6799999475479126\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss:0.04195622354745865 acc:0.6999999284744263\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.06495875865221024 acc:0.7199999690055847\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss:0.03781777620315552 acc:0.6599999666213989\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss:0.0321127250790596 acc:0.6799999475479126\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss:0.06770102679729462 acc:0.6899999380111694\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss:0.03982648253440857 acc:0.7099999189376831\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.05383467674255371 acc:0.6899999380111694\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss:0.04242462292313576 acc:0.6699999570846558\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss:0.03238026797771454 acc:0.699999988079071\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss:0.05248067528009415 acc:0.6999999284744263\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss:0.04327606037259102 acc:0.7199999690055847\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.04716832935810089 acc:0.699999988079071\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss:0.04411907494068146 acc:0.6899999976158142\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss:0.03168681263923645 acc:0.6999999284744263\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss:0.0751185417175293 acc:0.6599999666213989\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss:0.04111890494823456 acc:0.6999999284744263\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.051340218633413315 acc:0.699999988079071\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss:0.03455977514386177 acc:0.7199999690055847\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss:0.030976254492998123 acc:0.7199999094009399\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss:0.07088877260684967 acc:0.6899999380111694\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss:0.04369843378663063 acc:0.6699999570846558\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.046569980680942535 acc:0.699999988079071\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss:0.04587910696864128 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss:0.026939017698168755 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss:0.04277642071247101 acc:0.7099999189376831\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss:0.03729122132062912 acc:0.7399999499320984\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.042769718915224075 acc:0.7199999690055847\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss:0.03136634826660156 acc:0.6999999284744263\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss:0.028120707720518112 acc:0.7199999690055847\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss:0.06443138420581818 acc:0.7199999690055847\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss:0.031134560704231262 acc:0.7099999785423279\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.05694137141108513 acc:0.699999988079071\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss:0.030638670548796654 acc:0.6899999380111694\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss:0.034465156495571136 acc:0.6999999284744263\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss:0.04538533091545105 acc:0.6699999570846558\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss:0.03794004023075104 acc:0.7199999690055847\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.044053588062524796 acc:0.699999988079071\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss:0.033197395503520966 acc:0.6899999976158142\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss:0.0324028879404068 acc:0.7099999785423279\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss:0.056060999631881714 acc:0.6899999380111694\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss:0.0360979288816452 acc:0.7199999094009399\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.03779294714331627 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss:0.029997793957591057 acc:0.6799999475479126\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss:0.031661514192819595 acc:0.7299999594688416\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss:0.07514280825853348 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss:0.03872509300708771 acc:0.7199999690055847\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.036296918988227844 acc:0.7399999499320984\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss:0.030521485954523087 acc:0.7099999785423279\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss:0.03156900778412819 acc:0.7099999189376831\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss:0.06296829879283905 acc:0.6799999475479126\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss:0.04619528353214264 acc:0.6999999284744263\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.03738227114081383 acc:0.7099999189376831\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss:0.022481385618448257 acc:0.6899999380111694\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss:0.0319012813270092 acc:0.7199999690055847\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss:0.054786812514066696 acc:0.7099999189376831\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss:0.028875764459371567 acc:0.7099999785423279\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.034175679087638855 acc:0.7199999690055847\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss:0.02623787149786949 acc:0.7099999785423279\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss:0.02648073062300682 acc:0.7299999594688416\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss:0.04830838739871979 acc:0.7299999594688416\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss:0.0371973030269146 acc:0.6899999380111694\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.03903843089938164 acc:0.7199999094009399\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss:0.023325813934206963 acc:0.6699999570846558\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss:0.02228454500436783 acc:0.7199999690055847\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss:0.04072733595967293 acc:0.7199999690055847\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss:0.02711981162428856 acc:0.7099999189376831\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.027161911129951477 acc:0.7099999189376831\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss:0.023798691108822823 acc:0.699999988079071\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss:0.028725486248731613 acc:0.7199999690055847\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss:0.051721375435590744 acc:0.6699999570846558\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss:0.033106137067079544 acc:0.7099999785423279\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.04267881438136101 acc:0.7099999189376831\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss:0.02943204902112484 acc:0.6899999976158142\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss:0.026395754888653755 acc:0.6899999380111694\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss:0.05431639775633812 acc:0.6899999380111694\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss:0.02221396379172802 acc:0.6899999976158142\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.031873539090156555 acc:0.6999999284744263\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss:0.03325766324996948 acc:0.6599999666213989\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss:0.021717488765716553 acc:0.6999999284744263\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss:0.043326690793037415 acc:0.7099999189376831\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss:0.03210587799549103 acc:0.6799999475479126\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.025796562433242798 acc:0.7199999690055847\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss:0.029862139374017715 acc:0.6899999380111694\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss:0.0215318463742733 acc:0.699999988079071\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss:0.030645497143268585 acc:0.7199999690055847\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss:0.02752184495329857 acc:0.6799999475479126\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.035558540374040604 acc:0.6999999284744263\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss:0.023955419659614563 acc:0.6999999284744263\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss:0.018068939447402954 acc:0.7099999785423279\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss:0.049201685935258865 acc:0.6899999380111694\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss:0.026130931451916695 acc:0.6899999380111694\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.027692340314388275 acc:0.6699999570846558\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss:0.026131056249141693 acc:0.7099999785423279\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss:0.02221418172121048 acc:0.6999999284744263\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss:0.03668224811553955 acc:0.6999999284744263\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss:0.02760384790599346 acc:0.6999999284744263\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.027828199788928032 acc:0.6799999475479126\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss:0.023906059563159943 acc:0.6699999570846558\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss:0.021152561530470848 acc:0.7199999094009399\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss:0.047038733959198 acc:0.6799999475479126\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss:0.031374491751194 acc:0.6599999666213989\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.03736075758934021 acc:0.6699999570846558\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss:0.027125360444188118 acc:0.6899999380111694\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss:0.02280578203499317 acc:0.6899999380111694\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss:0.03615635633468628 acc:0.6799999475479126\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss:0.03205495700240135 acc:0.7199999690055847\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.025787197053432465 acc:0.6699999570846558\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss:0.0288217943161726 acc:0.6499999761581421\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss:0.018217675387859344 acc:0.7299999594688416\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss:0.03496483713388443 acc:0.7099999785423279\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss:0.022242898121476173 acc:0.6899999380111694\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.02060157060623169 acc:0.6999999284744263\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss:0.025534994900226593 acc:0.6899999380111694\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss:0.022358624264597893 acc:0.7199999690055847\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss:0.029102247208356857 acc:0.6899999380111694\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss:0.02129310742020607 acc:0.6899999380111694\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.022188080474734306 acc:0.7299999594688416\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss:0.01745177060365677 acc:0.7199999094009399\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss:0.02019161358475685 acc:0.7099999785423279\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss:0.04099195450544357 acc:0.6899999380111694\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss:0.016549959778785706 acc:0.7099999189376831\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.02754940092563629 acc:0.6999999284744263\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss:0.023416675627231598 acc:0.699999988079071\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss:0.017346467822790146 acc:0.7099999189376831\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss:0.043034497648477554 acc:0.6899999380111694\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss:0.01809210702776909 acc:0.6999999284744263\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.027943143621087074 acc:0.7199999690055847\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss:0.020802833139896393 acc:0.6999999284744263\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss:0.015490214340388775 acc:0.7099999189376831\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss:0.0281483493745327 acc:0.6999999284744263\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss:0.014651088044047356 acc:0.6699999570846558\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.022946259006857872 acc:0.6999999284744263\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss:0.025673817843198776 acc:0.6899999380111694\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss:0.020514892414212227 acc:0.7199999690055847\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss:0.028935041278600693 acc:0.7199999690055847\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss:0.013054735027253628 acc:0.6999999284744263\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.028284361585974693 acc:0.7199999690055847\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss:0.019667545333504677 acc:0.6899999380111694\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss:0.017091764137148857 acc:0.7199999690055847\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss:0.03760358691215515 acc:0.6700000166893005\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss:0.014248434454202652 acc:0.7299999594688416\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.02466217800974846 acc:0.6899999380111694\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss:0.02030513435602188 acc:0.7199999690055847\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss:0.019138086587190628 acc:0.7199999094009399\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss:0.030585037544369698 acc:0.6999999284744263\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss:0.022979572415351868 acc:0.6899999380111694\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.023744087666273117 acc:0.7199999094009399\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss:0.015549404546618462 acc:0.6799999475479126\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss:0.013382957316935062 acc:0.7099999785423279\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss:0.02508191019296646 acc:0.6799999475479126\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss:0.029224764555692673 acc:0.6899999380111694\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.021709423512220383 acc:0.7299999594688416\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss:0.01631958596408367 acc:0.7199999690055847\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss:0.01516958698630333 acc:0.7199999690055847\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss:0.020011674612760544 acc:0.6999999284744263\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss:0.01941084861755371 acc:0.7299999594688416\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.022151203826069832 acc:0.7099999189376831\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss:0.02079399675130844 acc:0.7099999189376831\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss:0.012449619360268116 acc:0.7199999690055847\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss:0.01863599754869938 acc:0.7099999785423279\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss:0.01453823409974575 acc:0.6899999380111694\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.018195392563939095 acc:0.7199999690055847\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss:0.018213924020528793 acc:0.7099999785423279\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss:0.011174866929650307 acc:0.7099999785423279\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss:0.030460292473435402 acc:0.7299999594688416\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss:0.014395073987543583 acc:0.7099999189376831\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.02919541485607624 acc:0.7099999189376831\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss:0.014509939588606358 acc:0.7199999690055847\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss:0.013882766477763653 acc:0.7199999690055847\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss:0.02574261836707592 acc:0.7099999785423279\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss:0.013729052618145943 acc:0.6899999380111694\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.027025721967220306 acc:0.7299998998641968\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss:0.01380093302577734 acc:0.7299999594688416\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss:0.015927433967590332 acc:0.6999999284744263\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss:0.036178190261125565 acc:0.7099999785423279\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss:0.013235021382570267 acc:0.6899999380111694\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.020955169573426247 acc:0.7299999594688416\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss:0.015624326653778553 acc:0.7299999594688416\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss:0.016273774206638336 acc:0.7199999094009399\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss:0.02182925119996071 acc:0.7199999690055847\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss:0.013526475057005882 acc:0.6699999570846558\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.019863029941916466 acc:0.7199999690055847\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss:0.012125871144235134 acc:0.7199999690055847\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss:0.01480773650109768 acc:0.7099999189376831\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss:0.025054581463336945 acc:0.6999999284744263\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss:0.009773226454854012 acc:0.6799999475479126\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.02540741115808487 acc:0.6799999475479126\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss:0.01446104422211647 acc:0.7099999189376831\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss:0.01297609880566597 acc:0.6999999284744263\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss:0.036304812878370285 acc:0.7199999690055847\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss:0.01375049352645874 acc:0.7099999189376831\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.021959349513053894 acc:0.7399999499320984\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss:0.015852967277169228 acc:0.7099999189376831\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss:0.012215978465974331 acc:0.6899999380111694\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss:0.02787308394908905 acc:0.6899999380111694\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss:0.010511614382266998 acc:0.6799999475479126\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.014876799657940865 acc:0.7199999690055847\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss:0.01177795510739088 acc:0.699999988079071\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss:0.0155167356133461 acc:0.6899999976158142\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss:0.023528290912508965 acc:0.6999999284744263\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss:0.010851489380002022 acc:0.7099999189376831\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.014104200527071953 acc:0.7299999594688416\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss:0.010879723355174065 acc:0.6799999475479126\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss:0.01244070753455162 acc:0.7099999189376831\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss:0.02750520408153534 acc:0.6999999284744263\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss:0.00863180123269558 acc:0.6999999284744263\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.011595213785767555 acc:0.6899999380111694\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss:0.010027779266238213 acc:0.7099999785423279\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss:0.008264889940619469 acc:0.6699999570846558\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss:0.03421758487820625 acc:0.6899999380111694\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss:0.011545229703187943 acc:0.6999999284744263\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.0185083020478487 acc:0.7099999785423279\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss:0.013210800476372242 acc:0.6799999475479126\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss:0.008839108049869537 acc:0.6999999284744263\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss:0.0283722672611475 acc:0.7199999094009399\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss:0.008907102048397064 acc:0.6999999284744263\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.014607800170779228 acc:0.6999999284744263\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss:0.014285789802670479 acc:0.6999999284744263\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss:0.012309310957789421 acc:0.6800000071525574\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss:0.01776280254125595 acc:0.7199999690055847\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss:0.009143670089542866 acc:0.7199999690055847\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.012037011794745922 acc:0.6899999380111694\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss:0.009850237518548965 acc:0.7099999785423279\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss:0.005186202935874462 acc:0.6899999380111694\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss:0.02964421920478344 acc:0.6899999380111694\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss:0.01242055557668209 acc:0.6999999284744263\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.02082650177180767 acc:0.7099999189376831\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss:0.012913838028907776 acc:0.6999999284744263\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss:0.006347103044390678 acc:0.6700000166893005\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss:0.017070673406124115 acc:0.7099999189376831\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss:0.006584011018276215 acc:0.7199999094009399\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.01008554082363844 acc:0.6899999380111694\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss:0.01731606014072895 acc:0.7199999690055847\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss:0.007194193080067635 acc:0.7099999189376831\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss:0.016986580565571785 acc:0.6999999284744263\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss:0.00977138802409172 acc:0.6899999380111694\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.009572751820087433 acc:0.6699999570846558\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss:0.012922205962240696 acc:0.7099999189376831\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss:0.007479561958462 acc:0.7099999189376831\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss:0.015653066337108612 acc:0.6999999284744263\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss:0.005345920566469431 acc:0.6899999380111694\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.022425876930356026 acc:0.6899999380111694\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss:0.011209620162844658 acc:0.6899999380111694\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss:0.008360720239579678 acc:0.6599999666213989\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss:0.027441412210464478 acc:0.6599999666213989\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss:0.007311555556952953 acc:0.6699999570846558\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.013836804777383804 acc:0.6699999570846558\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss:0.0118565009906888 acc:0.6899999380111694\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss:0.009940274059772491 acc:0.6699999570846558\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss:0.018911238759756088 acc:0.6899999380111694\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss:0.008376452140510082 acc:0.6899999380111694\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.01566579006612301 acc:0.6799999475479126\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss:0.015648363158106804 acc:0.6999999284744263\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss:0.007962035946547985 acc:0.7299999594688416\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss:0.023746924474835396 acc:0.6799999475479126\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss:0.004942446015775204 acc:0.6899999380111694\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.012332973070442677 acc:0.6699999570846558\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss:0.01028431300073862 acc:0.6899999380111694\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss:0.005972454324364662 acc:0.7199999094009399\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss:0.012488954700529575 acc:0.7099999189376831\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss:0.009188571013510227 acc:0.7099999785423279\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.013505132868885994 acc:0.6999999284744263\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss:0.011315718293190002 acc:0.6899999976158142\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss:0.011650235392153263 acc:0.6899999380111694\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss:0.011791771277785301 acc:0.6899999976158142\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss:0.0059456778690218925 acc:0.7099999785423279\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.012424111366271973 acc:0.6899999380111694\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss:0.009801467880606651 acc:0.699999988079071\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss:0.007699546869844198 acc:0.6699999570846558\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss:0.019801989197731018 acc:0.7099999189376831\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss:0.007774785161018372 acc:0.7199999690055847\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.015567082911729813 acc:0.7199999094009399\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss:0.009544388391077518 acc:0.699999988079071\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss:0.0067752450704574585 acc:0.6899999380111694\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss:0.013720225542783737 acc:0.6999999284744263\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss:0.00797678716480732 acc:0.6799999475479126\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.01321525126695633 acc:0.6999999284744263\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss:0.008732341229915619 acc:0.6899999380111694\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss:0.0058439490385353565 acc:0.6899999380111694\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss:0.02227458916604519 acc:0.6999999284744263\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss:0.005681264214217663 acc:0.6899999380111694\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.010422003455460072 acc:0.7099999189376831\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss:0.012797895818948746 acc:0.7199999094009399\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss:0.006101091857999563 acc:0.7299999594688416\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss:0.018389837816357613 acc:0.7199999690055847\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss:0.005125474184751511 acc:0.6799999475479126\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.012404642067849636 acc:0.7099999189376831\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss:0.0081856744363904 acc:0.7199999690055847\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss:0.004649846348911524 acc:0.7099999189376831\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss:0.01739954948425293 acc:0.6999999284744263\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss:0.005495772697031498 acc:0.6999999284744263\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.009976199828088284 acc:0.6599999666213989\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss:0.0070861089043319225 acc:0.6899999380111694\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss:0.007088041864335537 acc:0.6699999570846558\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss:0.012020152993500233 acc:0.7199999690055847\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss:0.0033344398252665997 acc:0.7099999189376831\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.012888304889202118 acc:0.7099999189376831\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss:0.00819340068846941 acc:0.6799999475479126\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss:0.00665642973035574 acc:0.6999999284744263\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss:0.022218696773052216 acc:0.699999988079071\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss:0.005703190807253122 acc:0.6999999284744263\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.010235084220767021 acc:0.6999999284744263\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss:0.012223409488797188 acc:0.6999999284744263\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss:0.004322187975049019 acc:0.699999988079071\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss:0.012515794485807419 acc:0.7099999189376831\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss:0.004449286498129368 acc:0.7099999189376831\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.011051654815673828 acc:0.6499999761581421\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss:0.008519912138581276 acc:0.7199999094009399\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss:0.004708149470388889 acc:0.7299999594688416\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss:0.020808609202504158 acc:0.6999999284744263\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss:0.006791206542402506 acc:0.699999988079071\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.006681338883936405 acc:0.7099999189376831\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss:0.007992549799382687 acc:0.6999999284744263\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss:0.005176429636776447 acc:0.7099999189376831\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss:0.012891402468085289 acc:0.7199999690055847\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss:0.004431930370628834 acc:0.6999999284744263\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.00877860002219677 acc:0.7199999094009399\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss:0.009771255776286125 acc:0.7099999189376831\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss:0.005327874328941107 acc:0.6999999284744263\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss:0.016725897789001465 acc:0.7199999094009399\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss:0.00388335925526917 acc:0.6999999284744263\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.011638696305453777 acc:0.6899999380111694\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss:0.005144178867340088 acc:0.6899999380111694\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss:0.003982903901487589 acc:0.6899999380111694\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss:0.013922140933573246 acc:0.6999999284744263\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss:0.004453146830201149 acc:0.6999999284744263\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.007004157640039921 acc:0.6799999475479126\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss:0.006030302960425615 acc:0.7199999690055847\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss:0.003465327899903059 acc:0.699999988079071\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss:0.016527025029063225 acc:0.699999988079071\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss:0.0034695048816502094 acc:0.6799999475479126\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.00929349847137928 acc:0.6999999284744263\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss:0.006517818197607994 acc:0.7099999785423279\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss:0.0036471765488386154 acc:0.7299999594688416\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss:0.013453690335154533 acc:0.7199999094009399\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss:0.004027504473924637 acc:0.6899999380111694\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.01122036762535572 acc:0.7199999690055847\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss:0.0044930013827979565 acc:0.7199999094009399\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss:0.004299604333937168 acc:0.6799999475479126\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss:0.013219627551734447 acc:0.7199999690055847\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss:0.0037511810660362244 acc:0.6999999284744263\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.0072275185957551 acc:0.6999999284744263\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss:0.005757822655141354 acc:0.6999999284744263\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss:0.004336192738264799 acc:0.6999999284744263\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss:0.013977508060634136 acc:0.6799999475479126\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss:0.004669110290706158 acc:0.7199999690055847\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.010289942845702171 acc:0.7099999189376831\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss:0.005172707140445709 acc:0.7199999690055847\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss:0.0045653777197003365 acc:0.7099999189376831\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss:0.013542033731937408 acc:0.6999999284744263\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss:0.0031087754759937525 acc:0.7199999094009399\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.006390873342752457 acc:0.6999999284744263\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss:0.004820150323212147 acc:0.7199999690055847\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss:0.003970099613070488 acc:0.6899999976158142\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss:0.011242404580116272 acc:0.6999999284744263\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss:0.003966469783335924 acc:0.7099999785423279\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.005981156602501869 acc:0.6999999284744263\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss:0.00541963754221797 acc:0.6899999380111694\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss:0.002455794485285878 acc:0.7199999690055847\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss:0.007418171502649784 acc:0.7299999594688416\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss:0.0035969396121799946 acc:0.6999999284744263\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.009471027180552483 acc:0.7199999690055847\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss:0.0040729474276304245 acc:0.6999999284744263\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss:0.005994915030896664 acc:0.7099999785423279\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss:0.005732248537242413 acc:0.7299999594688416\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss:0.0032459350768476725 acc:0.6799999475479126\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.006807289086282253 acc:0.7099999189376831\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss:0.007526458706706762 acc:0.7099999785423279\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss:0.006424007471650839 acc:0.6899999380111694\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss:0.019579287618398666 acc:0.6999999284744263\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss:0.002674603369086981 acc:0.6999999284744263\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.006326550617814064 acc:0.6899999380111694\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss:0.005169367417693138 acc:0.7100000381469727\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss:0.005333025008440018 acc:0.6699999570846558\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss:0.009008045308291912 acc:0.6999999284744263\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss:0.0026827624533325434 acc:0.6699999570846558\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.014677600935101509 acc:0.6899999380111694\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss:0.0053429934196174145 acc:0.6899999380111694\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss:0.00404331786558032 acc:0.6599999666213989\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss:0.009084055200219154 acc:0.6999999284744263\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss:0.002113854046911001 acc:0.6599999666213989\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.01111468393355608 acc:0.6999999284744263\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss:0.007584960199892521 acc:0.699999988079071\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss:0.0037803335580974817 acc:0.6899999976158142\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss:0.011675908230245113 acc:0.6799999475479126\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss:0.0025166745763272047 acc:0.6899999380111694\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.008038775064051151 acc:0.7099999189376831\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss:0.008272256702184677 acc:0.6899999380111694\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss:0.003409878583624959 acc:0.6800000071525574\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss:0.008681492879986763 acc:0.6899999380111694\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss:0.001668143318966031 acc:0.6799999475479126\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.007872326299548149 acc:0.6799999475479126\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss:0.00615028478205204 acc:0.6999999284744263\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss:0.005781949497759342 acc:0.6799999475479126\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss:0.014565655030310154 acc:0.6799999475479126\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss:0.003817582968622446 acc:0.7099999785423279\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.007591255009174347 acc:0.6799999475479126\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss:0.004795486573129892 acc:0.6899999380111694\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss:0.00423463573679328 acc:0.6899999976158142\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss:0.015509437769651413 acc:0.6699999570846558\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss:0.0025812785606831312 acc:0.6999999284744263\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.007135756313800812 acc:0.699999988079071\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss:0.0043735201470553875 acc:0.7099999189376831\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss:0.006284164730459452 acc:0.6799999475479126\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss:0.013124623335897923 acc:0.6899999380111694\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss:0.0025207982398569584 acc:0.6899999380111694\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.0047474633902311325 acc:0.6999999284744263\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss:0.004373920150101185 acc:0.6999999284744263\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss:0.0030659716576337814 acc:0.6799999475479126\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss:0.010970396921038628 acc:0.7199999690055847\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss:0.002261944580823183 acc:0.6799999475479126\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.011564688757061958 acc:0.7199999690055847\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss:0.0036676356103271246 acc:0.6899999380111694\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss:0.005158203653991222 acc:0.7099999785423279\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss:0.0077810208313167095 acc:0.7199999094009399\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss:0.0024249772541224957 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.006994621828198433 acc:0.6799999475479126\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss:0.003947938792407513 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss:0.008329971693456173 acc:0.699999988079071\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss:0.010860606096684933 acc:0.7099999189376831\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss:0.0022124038077890873 acc:0.7099999189376831\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.00868737231940031 acc:0.6999999284744263\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss:0.003601629752665758 acc:0.7099999189376831\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss:0.0024812878109514713 acc:0.6999999284744263\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss:0.008227712474763393 acc:0.7199999690055847\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss:0.0020601090509444475 acc:0.6999999284744263\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.01391119696199894 acc:0.6999999284744263\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss:0.004494199063628912 acc:0.6899999380111694\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss:0.0030216441955417395 acc:0.6799999475479126\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss:0.01075858622789383 acc:0.6899999380111694\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss:0.002457899274304509 acc:0.6699999570846558\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.004979007411748171 acc:0.6899999380111694\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss:0.005186330992728472 acc:0.75\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss:0.0026506376452744007 acc:0.6799999475479126\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss:0.012527462095022202 acc:0.6599999666213989\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss:0.0022200634703040123 acc:0.6999999284744263\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6972531847133758\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrH6UGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f126f2b3f28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
