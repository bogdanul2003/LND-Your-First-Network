{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:20, 8.49MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 0:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+4ZgtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f74153a29e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 0\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    tmp=np.ndarray(x.shape,dtype=float)\n",
    "    tmp[:] = x\n",
    "    tmp[...] /= 255.0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    ret = np.zeros((len(x),10))\n",
    "    for idx,lbl in enumerate(x):\n",
    "        ret[idx,lbl] = 1\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " [[[ 0.10980392  0.09803922  0.03921569]\n",
      "   [ 0.14509804  0.13333333  0.0745098 ]\n",
      "   [ 0.14901961  0.1372549   0.07843137]\n",
      "   ..., \n",
      "   [ 0.29803922  0.2627451   0.15294118]\n",
      "   [ 0.31764706  0.28235294  0.16862745]\n",
      "   [ 0.33333333  0.29803922  0.18431373]]\n",
      "\n",
      "  [[ 0.12941176  0.10980392  0.05098039]\n",
      "   [ 0.13333333  0.11764706  0.05490196]\n",
      "   [ 0.1254902   0.10588235  0.04705882]\n",
      "   ..., \n",
      "   [ 0.37254902  0.32156863  0.21568627]\n",
      "   [ 0.37647059  0.32156863  0.21960784]\n",
      "   [ 0.33333333  0.28235294  0.17647059]]\n",
      "\n",
      "  [[ 0.15294118  0.1254902   0.05882353]\n",
      "   [ 0.15686275  0.12941176  0.06666667]\n",
      "   [ 0.22352941  0.19607843  0.12941176]\n",
      "   ..., \n",
      "   [ 0.36470588  0.29803922  0.20392157]\n",
      "   [ 0.41960784  0.34901961  0.25882353]\n",
      "   [ 0.37254902  0.30196078  0.21176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.3254902   0.28627451  0.20392157]\n",
      "   [ 0.34117647  0.30196078  0.21960784]\n",
      "   [ 0.32941176  0.29019608  0.20392157]\n",
      "   ..., \n",
      "   [ 0.38823529  0.36470588  0.2745098 ]\n",
      "   [ 0.35294118  0.32941176  0.23921569]\n",
      "   [ 0.31764706  0.29411765  0.20392157]]\n",
      "\n",
      "  [[ 0.34509804  0.28235294  0.2       ]\n",
      "   [ 0.35294118  0.29019608  0.20392157]\n",
      "   [ 0.36470588  0.30196078  0.21960784]\n",
      "   ..., \n",
      "   [ 0.31372549  0.29019608  0.20784314]\n",
      "   [ 0.29803922  0.2745098   0.19215686]\n",
      "   [ 0.32156863  0.29803922  0.21568627]]\n",
      "\n",
      "  [[ 0.38039216  0.30588235  0.21960784]\n",
      "   [ 0.36862745  0.29411765  0.20784314]\n",
      "   [ 0.36470588  0.29411765  0.20784314]\n",
      "   ..., \n",
      "   [ 0.21176471  0.18431373  0.10980392]\n",
      "   [ 0.24705882  0.21960784  0.14509804]\n",
      "   [ 0.28235294  0.25490196  0.18039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.66666667  0.70588235  0.77647059]\n",
      "   [ 0.65882353  0.69803922  0.76862745]\n",
      "   [ 0.69411765  0.7254902   0.79607843]\n",
      "   ..., \n",
      "   [ 0.63529412  0.70196078  0.84313725]\n",
      "   [ 0.61960784  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]]\n",
      "\n",
      "  [[ 0.65882353  0.70980392  0.77647059]\n",
      "   [ 0.6745098   0.7254902   0.78823529]\n",
      "   [ 0.67058824  0.71764706  0.78431373]\n",
      "   ..., \n",
      "   [ 0.62352941  0.69411765  0.83137255]\n",
      "   [ 0.61176471  0.69019608  0.82745098]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  [[ 0.60392157  0.66666667  0.72941176]\n",
      "   [ 0.58431373  0.64705882  0.70980392]\n",
      "   [ 0.50588235  0.56470588  0.63529412]\n",
      "   ..., \n",
      "   [ 0.63137255  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.32941176  0.31372549]\n",
      "   [ 0.29803922  0.33333333  0.31764706]\n",
      "   [ 0.30588235  0.33333333  0.32156863]\n",
      "   ..., \n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.28235294  0.29411765]\n",
      "   [ 0.23921569  0.25490196  0.26666667]]\n",
      "\n",
      "  [[ 0.26666667  0.29803922  0.30196078]\n",
      "   [ 0.27058824  0.30196078  0.30588235]\n",
      "   [ 0.28235294  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.29803922  0.31372549  0.3254902 ]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.27843137  0.29411765  0.30588235]]\n",
      "\n",
      "  [[ 0.2627451   0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.29803922  0.30980392]\n",
      "   [ 0.27058824  0.29411765  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30980392  0.32156863]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.28627451  0.30196078  0.31372549]]]]\n"
     ]
    }
   ],
   "source": [
    "filename = 'preprocess_batch_' + str(1) + '.p'\n",
    "features, labels = pickle.load(open(filename, mode='rb'))\n",
    "print(features[0:5,...])\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    (width, height, chann) = image_shape\n",
    "    x = tf.placeholder(tf.float32, [None, width, height, chann], name='x')\n",
    "    print(x.get_shape())\n",
    "    # TODO: Implement Function\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y= tf.placeholder(tf.int8, [None,n_classes], name='y')\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tmp = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(type(x_tensor.get_shape().as_list()[3] ))\n",
    "    print(conv_ksize)\n",
    "    (convk1, convk2) = conv_ksize\n",
    "    (convs1, convs2) = conv_strides\n",
    "    (poolk1, poolk2) = pool_ksize\n",
    "    (pools1, pools2) = pool_strides\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([convk1, convk2,x_tensor.get_shape().as_list()[3],conv_num_outputs ], stddev=0.1, mean=0.0,dtype=tf.float32, seed=None, name=None ))\n",
    "    biases= tf.Variable(tf.constant(0,dtype=tf.float32, shape=[conv_num_outputs]))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=x_tensor, filter= weights, strides = [1,convs1, convs2,1],padding='SAME')\n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer += biases\n",
    "    #print(layer.get_shape().as_list())\n",
    "    \n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer = tf.nn.max_pool(value=layer,ksize=[1,poolk1,poolk2,1], \n",
    "                           strides = [1,pools1,pools2,1],padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    #print(layer.get_shape().as_list())\n",
    "    #print(conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 10 30 6\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    [batch_size, h,w,c] = x_tensor.get_shape().as_list()\n",
    "    features= h*w*c\n",
    "    \n",
    "    layer = tf.reshape(x_tensor, [-1, features])\n",
    "    print(batch_size,h,w,c)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    layer = fully_conn(x_tensor, num_outputs)\n",
    "    layer = tf.nn.softmax(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convl = conv2d_maxpool(x,20, (5,5), (1,1), (2,2), (2,2))\n",
    "    convl = conv2d_maxpool(convl,40, (5,5), (1,1), (2,2), (2,2))\n",
    "    #convl = conv2d_maxpool(convl,64, (5,5), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    layer = flatten(convl)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    print(layer.get_shape().as_list())\n",
    "    layer = fully_conn(layer,250)\n",
    "    layer = tf.nn.dropout(layer, keep_prob)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,250)\n",
    "#     layer = tf.layers.dropout(layer, 0.5)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    layer= output(layer,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01,epsilon=0.00001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(type(valid_features), type(valid_labels), type(feature_batch), type(label_batch))\n",
    "    #print(\"print_stats valid_features:{} valid_labels:{}\".format(valid_features.shape, valid_labels.shape))\n",
    "    \n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    valid = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features[0:100,...],\n",
    "        y: valid_labels[0:100,...],\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    print('loss:{} acc:{}'.format( loss,valid))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.257741689682007 acc:0.1599999964237213\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.2323858737945557 acc:0.22999998927116394\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:2.190135955810547 acc:0.26999998092651367\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:2.157015323638916 acc:0.23999999463558197\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:2.128551959991455 acc:0.26999998092651367\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:2.1158294677734375 acc:0.26999998092651367\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:2.1098639965057373 acc:0.2799999713897705\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:2.0954062938690186 acc:0.29999998211860657\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:2.0854997634887695 acc:0.29999998211860657\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:2.0822324752807617 acc:0.3499999940395355\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:2.0751991271972656 acc:0.3199999928474426\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:2.0580763816833496 acc:0.3199999928474426\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:2.0672082901000977 acc:0.34999996423721313\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:2.034928798675537 acc:0.3499999940395355\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:2.0297417640686035 acc:0.35999998450279236\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:2.021099090576172 acc:0.34999996423721313\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:2.035439968109131 acc:0.3499999940395355\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:2.029798984527588 acc:0.38999998569488525\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:2.034402847290039 acc:0.3799999952316284\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:2.014312267303467 acc:0.4099999666213989\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:2.0060555934906006 acc:0.4099999666213989\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.9869496822357178 acc:0.4299999475479126\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.9785685539245605 acc:0.4099999666213989\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.9687469005584717 acc:0.4399999976158142\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.9659446477890015 acc:0.41999995708465576\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.9643776416778564 acc:0.4099999666213989\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.9537206888198853 acc:0.41999995708465576\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.9309711456298828 acc:0.4099999666213989\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.9450184106826782 acc:0.4399999976158142\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:1.9258744716644287 acc:0.44999998807907104\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:1.9350005388259888 acc:0.4299999475479126\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:1.9288792610168457 acc:0.43999993801116943\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:1.9049021005630493 acc:0.4399999976158142\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:1.9309757947921753 acc:0.4599999785423279\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:1.9170746803283691 acc:0.4699999690055847\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:1.8967130184173584 acc:0.4399999976158142\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:1.9118237495422363 acc:0.4399999976158142\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:1.897232174873352 acc:0.4699999690055847\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:1.8959121704101562 acc:0.43999993801116943\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:1.8846487998962402 acc:0.4599999785423279\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:1.8940373659133911 acc:0.44999995827674866\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:1.9077752828598022 acc:0.4599999785423279\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:1.8772566318511963 acc:0.4699999988079071\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:1.8714932203292847 acc:0.4599999785423279\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:1.858699083328247 acc:0.4699999690055847\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:1.8661218881607056 acc:0.4599999785423279\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:1.856133222579956 acc:0.4699999988079071\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:1.856123447418213 acc:0.4699999988079071\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:1.8690814971923828 acc:0.4699999690055847\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:1.853324294090271 acc:0.4699999690055847\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:1.842935562133789 acc:0.4599999785423279\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:1.852091670036316 acc:0.46000000834465027\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:1.846368432044983 acc:0.47999995946884155\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:1.8360588550567627 acc:0.4699999690055847\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:1.8537657260894775 acc:0.4699999690055847\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:1.8439199924468994 acc:0.5\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:1.8372433185577393 acc:0.49000000953674316\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:1.8352689743041992 acc:0.49000000953674316\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:1.8365342617034912 acc:0.4699999988079071\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:1.8334194421768188 acc:0.4899999797344208\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:1.820500135421753 acc:0.47999998927116394\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:1.84120774269104 acc:0.49000003933906555\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:1.8209353685379028 acc:0.5\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:1.822190761566162 acc:0.48000001907348633\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:1.8356642723083496 acc:0.5\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:1.8097691535949707 acc:0.5\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:1.8169004917144775 acc:0.5\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:1.8039886951446533 acc:0.49000000953674316\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:1.816986322402954 acc:0.5099999904632568\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:1.8082648515701294 acc:0.5199999809265137\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:1.7948904037475586 acc:0.49000003933906555\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:1.8111146688461304 acc:0.5099999904632568\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:1.7950495481491089 acc:0.5199999809265137\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:1.7898508310317993 acc:0.5099999904632568\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:1.7947373390197754 acc:0.5099999904632568\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:1.7861816883087158 acc:0.5\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:1.7850980758666992 acc:0.4699999988079071\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:1.7867298126220703 acc:0.5199999809265137\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:1.7935048341751099 acc:0.5399999618530273\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:1.7893104553222656 acc:0.5199999809265137\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:1.778982400894165 acc:0.5\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:1.7719700336456299 acc:0.4599999785423279\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:1.7924549579620361 acc:0.5\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:1.7698980569839478 acc:0.5199999809265137\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:1.7596373558044434 acc:0.5099999904632568\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:1.7706739902496338 acc:0.5499999523162842\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:1.7671582698822021 acc:0.5099999904632568\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:1.7518576383590698 acc:0.5299999713897705\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:1.7492258548736572 acc:0.49000000953674316\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:1.7473868131637573 acc:0.5299999713897705\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:1.7423237562179565 acc:0.5199999809265137\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:1.7532166242599487 acc:0.5\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:1.7528076171875 acc:0.5199999809265137\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:1.7387030124664307 acc:0.5399999618530273\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:1.7480432987213135 acc:0.5099999904632568\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:1.7430286407470703 acc:0.5199999809265137\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:1.7431122064590454 acc:0.5199999809265137\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:1.7570499181747437 acc:0.5099999904632568\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:1.730976939201355 acc:0.5399999618530273\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:1.7548177242279053 acc:0.5299999713897705\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:1.7417117357254028 acc:0.5199999809265137\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:1.7457225322723389 acc:0.5299999713897705\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:1.742114782333374 acc:0.5399999618530273\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:1.738095760345459 acc:0.5399999618530273\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:1.742073893547058 acc:0.5299999713897705\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:1.7423418760299683 acc:0.5\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:1.7270982265472412 acc:0.559999942779541\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:1.7229270935058594 acc:0.5499999523162842\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:1.7255109548568726 acc:0.5099999904632568\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:1.7269341945648193 acc:0.5299999713897705\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:1.7253738641738892 acc:0.5499999523162842\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:1.745622158050537 acc:0.559999942779541\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:1.7292119264602661 acc:0.559999942779541\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:1.7342182397842407 acc:0.5199999809265137\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:1.7135584354400635 acc:0.5299999713897705\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:1.712715983390808 acc:0.559999942779541\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:1.7108596563339233 acc:0.559999942779541\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:1.7056283950805664 acc:0.5699999928474426\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:1.7049247026443481 acc:0.5600000023841858\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:1.7092199325561523 acc:0.5499999523162842\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:1.7078258991241455 acc:0.5299999713897705\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:1.7161836624145508 acc:0.5499999523162842\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:1.7086378335952759 acc:0.5499999523162842\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:1.704350471496582 acc:0.5600000023841858\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:1.696422815322876 acc:0.5199999809265137\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:1.6896579265594482 acc:0.5699999928474426\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:1.6890959739685059 acc:0.5799999237060547\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:1.7033212184906006 acc:0.5399999618530273\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:1.6865344047546387 acc:0.5399999618530273\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:1.6901893615722656 acc:0.559999942779541\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:1.694754958152771 acc:0.5799999237060547\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:1.6834709644317627 acc:0.5999999642372131\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:1.677156686782837 acc:0.5899999737739563\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:1.681334376335144 acc:0.559999942779541\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:1.694251537322998 acc:0.559999942779541\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:1.6754097938537598 acc:0.5499999523162842\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:1.6691548824310303 acc:0.60999995470047\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:1.6637446880340576 acc:0.5699999332427979\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:1.6736575365066528 acc:0.559999942779541\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:1.666115641593933 acc:0.5699999332427979\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:1.672745943069458 acc:0.5699999332427979\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:1.6690095663070679 acc:0.5499999523162842\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:1.6648778915405273 acc:0.60999995470047\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:1.6651384830474854 acc:0.5799999237060547\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:1.6633766889572144 acc:0.5999999642372131\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:1.6600029468536377 acc:0.60999995470047\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:1.6590607166290283 acc:0.5799999237060547\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:1.6633801460266113 acc:0.5999999642372131\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:1.66042160987854 acc:0.5899999737739563\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:1.6573426723480225 acc:0.6200000047683716\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:1.663413166999817 acc:0.5899999737739563\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:1.6606868505477905 acc:0.6200000047683716\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:1.6621215343475342 acc:0.5699999332427979\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:1.6608424186706543 acc:0.5899999737739563\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:1.658437728881836 acc:0.5799999237060547\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:1.6615248918533325 acc:0.5799999237060547\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:1.6551506519317627 acc:0.559999942779541\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:1.657485008239746 acc:0.60999995470047\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:1.6513338088989258 acc:0.6199999451637268\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:1.663304090499878 acc:0.6299999952316284\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:1.6587111949920654 acc:0.5899999141693115\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:1.653982400894165 acc:0.6200000047683716\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:1.6541458368301392 acc:0.60999995470047\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:1.6461290121078491 acc:0.6200000047683716\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:1.6476942300796509 acc:0.60999995470047\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:1.6470565795898438 acc:0.60999995470047\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:1.6542292833328247 acc:0.6200000047683716\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:1.653592586517334 acc:0.5799999833106995\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:1.6392240524291992 acc:0.6199999451637268\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:1.6502361297607422 acc:0.6100000143051147\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:1.65162193775177 acc:0.5999999642372131\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:1.6424930095672607 acc:0.60999995470047\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:1.637941837310791 acc:0.5899999737739563\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:1.6422505378723145 acc:0.6199999451637268\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:1.6481730937957764 acc:0.5999999642372131\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:1.6461420059204102 acc:0.60999995470047\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:1.6429047584533691 acc:0.6199999451637268\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:1.6450700759887695 acc:0.6000000238418579\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:1.638375997543335 acc:0.6200000047683716\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:1.6393578052520752 acc:0.5899999737739563\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:1.6391654014587402 acc:0.5799999833106995\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:1.6406537294387817 acc:0.6199999451637268\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:1.6375510692596436 acc:0.5899999141693115\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:1.6384304761886597 acc:0.60999995470047\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:1.6394858360290527 acc:0.5899999141693115\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:1.6385407447814941 acc:0.6299999952316284\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:1.6394824981689453 acc:0.5899999737739563\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:1.636864185333252 acc:0.6000000238418579\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:1.6418170928955078 acc:0.60999995470047\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:1.6381731033325195 acc:0.60999995470047\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:1.6381837129592896 acc:0.5899999141693115\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:1.639846682548523 acc:0.559999942779541\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:1.6383335590362549 acc:0.5999999642372131\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:1.6385208368301392 acc:0.6200000047683716\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:1.6377089023590088 acc:0.5999999642372131\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:1.6371798515319824 acc:0.5999999642372131\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:1.6356364488601685 acc:0.5999999642372131\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:1.6368809938430786 acc:0.6200000047683716\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:1.635246753692627 acc:0.5899999737739563\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:1.631209373474121 acc:0.5999999642372131\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.261754274368286 acc:0.25\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:2.200549602508545 acc:0.3499999940395355\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:2.1004600524902344 acc:0.3699999749660492\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:2.109332323074341 acc:0.35999998450279236\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:2.098647117614746 acc:0.3699999749660492\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.123870372772217 acc:0.38999998569488525\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:2.125131845474243 acc:0.3999999761581421\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:2.0512208938598633 acc:0.3999999761581421\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:2.0362496376037598 acc:0.3999999761581421\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:2.0736122131347656 acc:0.38999998569488525\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:2.0651092529296875 acc:0.429999977350235\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:2.054715871810913 acc:0.35999995470046997\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:1.9643588066101074 acc:0.4099999666213989\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:2.045360565185547 acc:0.3699999749660492\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:2.011026620864868 acc:0.4099999964237213\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:2.0199739933013916 acc:0.429999977350235\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:2.0428152084350586 acc:0.3799999952316284\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:1.9515042304992676 acc:0.4099999666213989\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:2.018428325653076 acc:0.38999998569488525\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:1.9702938795089722 acc:0.3999999761581421\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.9805493354797363 acc:0.4399999678134918\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:2.0136001110076904 acc:0.41999998688697815\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:1.9313398599624634 acc:0.429999977350235\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:2.001128911972046 acc:0.42000001668930054\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:1.966538906097412 acc:0.4099999666213989\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.9402683973312378 acc:0.38999998569488525\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:1.995485544204712 acc:0.3999999761581421\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:1.9047417640686035 acc:0.4299999475479126\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:1.9904550313949585 acc:0.4599999785423279\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:1.9517674446105957 acc:0.4399999678134918\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.9321790933609009 acc:0.4099999666213989\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:1.9680149555206299 acc:0.429999977350235\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:1.902638554573059 acc:0.44999995827674866\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:1.9448997974395752 acc:0.4699999988079071\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:1.9287049770355225 acc:0.4599999785423279\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.8810348510742188 acc:0.44999998807907104\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:1.929566502571106 acc:0.4699999690055847\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:1.8827542066574097 acc:0.4599999487400055\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:1.9450428485870361 acc:0.4399999976158142\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:1.9251137971878052 acc:0.47999995946884155\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.907477617263794 acc:0.44999998807907104\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:1.921766757965088 acc:0.4699999690055847\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:1.8833527565002441 acc:0.47999995946884155\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:1.918947458267212 acc:0.44999998807907104\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:1.9175633192062378 acc:0.4599999785423279\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.8595448732376099 acc:0.44999998807907104\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:1.9397133588790894 acc:0.4399999678134918\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:1.886505126953125 acc:0.4899999499320984\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:1.892515778541565 acc:0.48000001907348633\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:1.8875943422317505 acc:0.5099999904632568\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.91365385055542 acc:0.4699999690055847\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:1.9153743982315063 acc:0.4699999988079071\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:1.8859344720840454 acc:0.5199999809265137\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:1.8829425573349 acc:0.5199999809265137\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:1.885805368423462 acc:0.5\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.8607547283172607 acc:0.4699999690055847\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:1.8998736143112183 acc:0.5199999809265137\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:1.8662497997283936 acc:0.4699999690055847\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:1.8664039373397827 acc:0.5299999713897705\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:1.862064003944397 acc:0.5199999809265137\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.8547075986862183 acc:0.4899999797344208\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:1.8968069553375244 acc:0.5199999809265137\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:1.8435847759246826 acc:0.47999995946884155\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:1.86009681224823 acc:0.559999942779541\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:1.8519057035446167 acc:0.5\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.8494288921356201 acc:0.5199999809265137\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:1.8990771770477295 acc:0.4999999701976776\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:1.843534231185913 acc:0.5099999904632568\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:1.8396332263946533 acc:0.5199999809265137\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:1.8443008661270142 acc:0.5299999713897705\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.825918197631836 acc:0.4899999797344208\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:1.9033441543579102 acc:0.4999999701976776\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:1.8675485849380493 acc:0.4999999701976776\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:1.8500752449035645 acc:0.5099999904632568\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:1.8397296667099 acc:0.5\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.8767714500427246 acc:0.5199999809265137\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:1.8430793285369873 acc:0.5600000023841858\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:1.861472487449646 acc:0.5399999618530273\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:1.8169348239898682 acc:0.5399999618530273\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:1.8352224826812744 acc:0.5299999713897705\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.836365818977356 acc:0.5399999618530273\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:1.8622334003448486 acc:0.5600000023841858\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:1.8398032188415527 acc:0.5099999904632568\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:1.8116085529327393 acc:0.60999995470047\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:1.815800666809082 acc:0.5199999809265137\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.8391375541687012 acc:0.550000011920929\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:1.8676846027374268 acc:0.5399999618530273\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:1.8434185981750488 acc:0.5899999737739563\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:1.7787339687347412 acc:0.5999999642372131\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:1.799553394317627 acc:0.60999995470047\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.8427083492279053 acc:0.5499999523162842\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:1.8558547496795654 acc:0.559999942779541\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:1.8411738872528076 acc:0.5299999713897705\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:1.8020398616790771 acc:0.6100000143051147\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:1.7941257953643799 acc:0.5999999642372131\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.8417130708694458 acc:0.60999995470047\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:1.8423198461532593 acc:0.550000011920929\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:1.8401540517807007 acc:0.5399999618530273\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:1.7741594314575195 acc:0.5899999737739563\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:1.8073372840881348 acc:0.5600000023841858\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.8110594749450684 acc:0.5399999618530273\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:1.8187918663024902 acc:0.5699999928474426\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:1.8198184967041016 acc:0.5799999833106995\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:1.7587556838989258 acc:0.60999995470047\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:1.8203545808792114 acc:0.5699999928474426\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.8327876329421997 acc:0.60999995470047\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:1.8322861194610596 acc:0.5699999928474426\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:1.789271354675293 acc:0.5799999833106995\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:1.7622027397155762 acc:0.6399999856948853\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:1.7884607315063477 acc:0.559999942779541\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.8391377925872803 acc:0.5899999737739563\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:1.8356239795684814 acc:0.5299999713897705\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:1.7878162860870361 acc:0.559999942779541\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:1.7756319046020508 acc:0.5499999523162842\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:1.7677481174468994 acc:0.5699999928474426\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.8311278820037842 acc:0.5699999332427979\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:1.824124813079834 acc:0.5799999237060547\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:1.7902634143829346 acc:0.5499999523162842\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:1.7775044441223145 acc:0.6100000143051147\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:1.788381576538086 acc:0.5899999737739563\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.8384544849395752 acc:0.5799999833106995\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:1.810694694519043 acc:0.5799999237060547\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:1.7829409837722778 acc:0.5600000023841858\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:1.769148349761963 acc:0.5899999737739563\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:1.77192223072052 acc:0.5799999833106995\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.7991373538970947 acc:0.5600000023841858\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:1.8098214864730835 acc:0.5499999523162842\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:1.8075549602508545 acc:0.5299999713897705\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:1.7520568370819092 acc:0.5899999141693115\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:1.7596875429153442 acc:0.5799999833106995\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.815447211265564 acc:0.5699999928474426\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:1.8075026273727417 acc:0.5799999833106995\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:1.7786729335784912 acc:0.5699999928474426\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:1.7438328266143799 acc:0.5999999642372131\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:1.7749273777008057 acc:0.5899999737739563\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.8074901103973389 acc:0.5799999833106995\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:1.8046902418136597 acc:0.5699999332427979\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:1.7754788398742676 acc:0.5600000023841858\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:1.744626522064209 acc:0.5899999737739563\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:1.749718189239502 acc:0.5899999737739563\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.7822186946868896 acc:0.5399999618530273\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:1.8032517433166504 acc:0.5699999332427979\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:1.7581099271774292 acc:0.5699999332427979\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:1.7465012073516846 acc:0.60999995470047\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:1.7284929752349854 acc:0.5999999642372131\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:1.8129678964614868 acc:0.6199999451637268\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:1.780574083328247 acc:0.6200000047683716\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:1.7555441856384277 acc:0.5799999833106995\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:1.7338953018188477 acc:0.6199999451637268\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:1.7350502014160156 acc:0.60999995470047\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:1.7765882015228271 acc:0.60999995470047\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:1.7883186340332031 acc:0.60999995470047\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:1.7526865005493164 acc:0.60999995470047\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:1.735948085784912 acc:0.5999999642372131\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:1.738692045211792 acc:0.5899999737739563\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:1.7692656517028809 acc:0.6199999451637268\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:1.781130075454712 acc:0.6299999356269836\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:1.7293055057525635 acc:0.5499999523162842\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:1.7297770977020264 acc:0.6200000047683716\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:1.75543212890625 acc:0.5899999737739563\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:1.7542939186096191 acc:0.5999999642372131\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:1.7761609554290771 acc:0.6299999952316284\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:1.743671178817749 acc:0.5399999618530273\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:1.7424920797348022 acc:0.5799999237060547\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:1.7492492198944092 acc:0.5999999642372131\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:1.7610795497894287 acc:0.6299999952316284\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:1.776170253753662 acc:0.60999995470047\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:1.7177209854125977 acc:0.5399999618530273\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:1.751122236251831 acc:0.5999999642372131\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:1.750941514968872 acc:0.5899999737739563\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:1.7231948375701904 acc:0.6100000143051147\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:1.7655627727508545 acc:0.5899999737739563\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:1.7235788106918335 acc:0.5799999833106995\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:1.718026876449585 acc:0.6200000047683716\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:1.7372833490371704 acc:0.5999999642372131\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:1.7342896461486816 acc:0.5899999737739563\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:1.7707858085632324 acc:0.5499999523162842\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:1.7235852479934692 acc:0.559999942779541\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:1.721192717552185 acc:0.6200000047683716\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:1.723884105682373 acc:0.5799999833106995\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:1.7553163766860962 acc:0.60999995470047\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:1.7588834762573242 acc:0.6100000143051147\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:1.7300381660461426 acc:0.5699999928474426\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:1.7181861400604248 acc:0.5899999737739563\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:1.7174774408340454 acc:0.5999999642372131\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:1.7608716487884521 acc:0.6299999356269836\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:1.7458125352859497 acc:0.60999995470047\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:1.7151423692703247 acc:0.5899999737739563\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:1.71687650680542 acc:0.5999999642372131\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:1.7299896478652954 acc:0.5799999833106995\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:1.7394964694976807 acc:0.6199999451637268\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:1.754475712776184 acc:0.5999999642372131\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:1.681578516960144 acc:0.60999995470047\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:1.7265079021453857 acc:0.5799999833106995\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:1.6946556568145752 acc:0.60999995470047\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:1.7672691345214844 acc:0.5799999833106995\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:1.7604005336761475 acc:0.6399999856948853\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:1.7127070426940918 acc:0.5899999141693115\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:1.7346153259277344 acc:0.6200000047683716\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:1.703779935836792 acc:0.6399999856948853\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:1.7445995807647705 acc:0.5699999928474426\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:1.751387119293213 acc:0.6200000047683716\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:1.6807103157043457 acc:0.5699999928474426\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:1.6958314180374146 acc:0.5999999642372131\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:1.6892186403274536 acc:0.5799999833106995\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:1.713395357131958 acc:0.5899999737739563\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:1.7550902366638184 acc:0.6100000143051147\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:1.6670173406600952 acc:0.6100000143051147\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:1.7097399234771729 acc:0.5999999642372131\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:1.6853575706481934 acc:0.5899999737739563\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:1.7432174682617188 acc:0.5699999928474426\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:1.734070062637329 acc:0.5699999928474426\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:1.6736180782318115 acc:0.6399999856948853\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:1.704012155532837 acc:0.60999995470047\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:1.6973118782043457 acc:0.6299999952316284\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:1.7177921533584595 acc:0.60999995470047\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:1.7142884731292725 acc:0.5899999737739563\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:1.6613528728485107 acc:0.6299999952316284\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:1.6970877647399902 acc:0.60999995470047\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:1.6956820487976074 acc:0.60999995470047\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:1.7217694520950317 acc:0.6199999451637268\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:1.7305353879928589 acc:0.60999995470047\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:1.6585063934326172 acc:0.5899999141693115\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:1.691709041595459 acc:0.6299999952316284\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:1.6882891654968262 acc:0.6199999451637268\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:1.7213408946990967 acc:0.6199999451637268\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:1.6965153217315674 acc:0.6499999761581421\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:1.6439108848571777 acc:0.60999995470047\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:1.6919124126434326 acc:0.6399999856948853\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:1.6663075685501099 acc:0.5999999642372131\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:1.7243428230285645 acc:0.6200000047683716\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:1.729846477508545 acc:0.6499999761581421\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:1.6518089771270752 acc:0.6200000047683716\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:1.6989158391952515 acc:0.6699999570846558\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:1.6706507205963135 acc:0.6599999666213989\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:1.7118980884552002 acc:0.6599999666213989\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:1.7110592126846313 acc:0.6199999451637268\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:1.668432593345642 acc:0.6399999856948853\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:1.6879932880401611 acc:0.6599999666213989\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:1.6753058433532715 acc:0.6299999952316284\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:1.7192747592926025 acc:0.6299999952316284\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:1.7319004535675049 acc:0.5899999737739563\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:1.631568193435669 acc:0.6200000047683716\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:1.7023017406463623 acc:0.60999995470047\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:1.6527618169784546 acc:0.6399999856948853\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:1.7195616960525513 acc:0.6200000047683716\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:1.7217586040496826 acc:0.6399999856948853\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:1.6626988649368286 acc:0.6299999356269836\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:1.70866060256958 acc:0.5899999737739563\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:1.6434831619262695 acc:0.6699999570846558\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:1.7146492004394531 acc:0.6299999952316284\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:1.6969714164733887 acc:0.5999999642372131\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:1.6400928497314453 acc:0.6299999952316284\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:1.67942476272583 acc:0.6599999666213989\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:1.6571519374847412 acc:0.6199999451637268\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:1.706591248512268 acc:0.5999999642372131\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:1.7009797096252441 acc:0.6299999356269836\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:1.6479065418243408 acc:0.6200000047683716\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:1.694464921951294 acc:0.6299999952316284\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:1.6348150968551636 acc:0.6200000047683716\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:1.6892950534820557 acc:0.60999995470047\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:1.684294581413269 acc:0.6399999856948853\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:1.63535737991333 acc:0.60999995470047\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:1.687948226928711 acc:0.6199999451637268\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:1.6248646974563599 acc:0.6599999666213989\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:1.7182496786117554 acc:0.6499999761581421\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:1.7012373208999634 acc:0.6499999761581421\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:1.6362193822860718 acc:0.6000000238418579\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:1.6725642681121826 acc:0.6399999856948853\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:1.627109408378601 acc:0.6699999570846558\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:1.704266905784607 acc:0.6399999856948853\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:1.687281847000122 acc:0.6299999952316284\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:1.628467082977295 acc:0.6799999475479126\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:1.6831110715866089 acc:0.6299999952316284\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:1.6212141513824463 acc:0.6599999666213989\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:1.6897156238555908 acc:0.6499999761581421\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:1.6903836727142334 acc:0.6699999570846558\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:1.6333872079849243 acc:0.6399999260902405\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:1.6684017181396484 acc:0.6599999666213989\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:1.6264601945877075 acc:0.6499999761581421\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:1.6759607791900635 acc:0.6599999666213989\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:1.6711276769638062 acc:0.5899999737739563\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:1.6256921291351318 acc:0.5999999642372131\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:1.6786489486694336 acc:0.60999995470047\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:1.6155729293823242 acc:0.6299999952316284\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:1.6956653594970703 acc:0.6399999856948853\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:1.6765915155410767 acc:0.6399999856948853\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:1.6135542392730713 acc:0.6299999952316284\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:1.662575364112854 acc:0.6399999260902405\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:1.6153450012207031 acc:0.6299999952316284\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:1.6774386167526245 acc:0.5999999642372131\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:1.6890597343444824 acc:0.7099999189376831\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:1.620600700378418 acc:0.6100000143051147\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:1.647825002670288 acc:0.6299999952316284\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:1.6038942337036133 acc:0.6599999666213989\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:1.6859509944915771 acc:0.6299999952316284\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:1.6820086240768433 acc:0.6699999570846558\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:1.609397053718567 acc:0.6199999451637268\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:1.6551685333251953 acc:0.6499999761581421\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:1.609726071357727 acc:0.6399999856948853\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:1.666369080543518 acc:0.6499999761581421\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:1.6833148002624512 acc:0.6399999856948853\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:1.6092568635940552 acc:0.6499999761581421\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:1.6657224893569946 acc:0.6399999856948853\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:1.595548391342163 acc:0.6599999666213989\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:1.6918895244598389 acc:0.6399999856948853\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:1.6618056297302246 acc:0.6899999976158142\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:1.607546091079712 acc:0.6499999761581421\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:1.6728990077972412 acc:0.6299999952316284\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:1.586839199066162 acc:0.6699999570846558\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:1.663266658782959 acc:0.6499999761581421\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:1.6667872667312622 acc:0.6599999666213989\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:1.6145353317260742 acc:0.6399999856948853\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:1.665529489517212 acc:0.6799999475479126\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:1.6007975339889526 acc:0.6699999570846558\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:1.651247262954712 acc:0.6899999380111694\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:1.643674373626709 acc:0.6299999952316284\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:1.632431983947754 acc:0.6499999761581421\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:1.681471586227417 acc:0.6399999856948853\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:1.5983052253723145 acc:0.6599999666213989\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:1.6543112993240356 acc:0.6599999666213989\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:1.6658430099487305 acc:0.6599999666213989\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:1.5823078155517578 acc:0.6799999475479126\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:1.6618001461029053 acc:0.6599999666213989\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:1.586562156677246 acc:0.6800000071525574\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:1.659285545349121 acc:0.6499999761581421\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:1.6530441045761108 acc:0.6699999570846558\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:1.585810661315918 acc:0.6799999475479126\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:1.6621944904327393 acc:0.6299999952316284\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:1.6091772317886353 acc:0.6899999380111694\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:1.6660280227661133 acc:0.5999999642372131\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:1.635844349861145 acc:0.6899999976158142\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:1.5776281356811523 acc:0.6699999570846558\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:1.6559464931488037 acc:0.6699999570846558\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:1.5858798027038574 acc:0.6499999761581421\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:1.6551600694656372 acc:0.6399999856948853\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:1.6643043756484985 acc:0.6799999475479126\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:1.596886396408081 acc:0.6599999666213989\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:1.6664578914642334 acc:0.6799999475479126\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:1.580952763557434 acc:0.6499999761581421\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:1.64884614944458 acc:0.6699999570846558\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:1.6662917137145996 acc:0.6999999284744263\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:1.5712904930114746 acc:0.6699999570846558\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:1.6338114738464355 acc:0.6799999475479126\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:1.5833325386047363 acc:0.6499999761581421\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:1.6593294143676758 acc:0.6200000047683716\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:1.6691120862960815 acc:0.6699999570846558\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:1.5866061449050903 acc:0.6499999761581421\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:1.6357803344726562 acc:0.6299999952316284\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:1.57455575466156 acc:0.699999988079071\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:1.66034734249115 acc:0.6399999856948853\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:1.6618818044662476 acc:0.6699999570846558\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:1.5707732439041138 acc:0.6699999570846558\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:1.6389646530151367 acc:0.6899999380111694\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:1.5787479877471924 acc:0.6499999761581421\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:1.6523157358169556 acc:0.6699999570846558\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:1.667184829711914 acc:0.6899999976158142\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:1.5639700889587402 acc:0.6499999761581421\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:1.6456793546676636 acc:0.6799999475479126\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:1.5641566514968872 acc:0.6899999380111694\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:1.6614570617675781 acc:0.6200000047683716\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:1.650618076324463 acc:0.699999988079071\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:1.5588672161102295 acc:0.6899999380111694\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:1.6366772651672363 acc:0.6899999380111694\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:1.5711709260940552 acc:0.6799999475479126\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:1.647375226020813 acc:0.6799999475479126\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:1.6363201141357422 acc:0.7099999785423279\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:1.5565383434295654 acc:0.7199999690055847\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:1.6433757543563843 acc:0.6799999475479126\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:1.571256399154663 acc:0.6699999570846558\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:1.655970811843872 acc:0.6699999570846558\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:1.6446505784988403 acc:0.699999988079071\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:1.5723350048065186 acc:0.6599999666213989\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:1.6599438190460205 acc:0.6599999666213989\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:1.565417766571045 acc:0.6599999666213989\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:1.652402400970459 acc:0.6399999856948853\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:1.640434980392456 acc:0.699999988079071\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:1.5678987503051758 acc:0.6599999666213989\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:1.6403499841690063 acc:0.6699999570846558\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:1.5637977123260498 acc:0.6899999380111694\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:1.6651300191879272 acc:0.6899999380111694\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:1.6461127996444702 acc:0.6999999284744263\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:1.5693387985229492 acc:0.6599999666213989\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:1.6593677997589111 acc:0.6599999666213989\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:1.5637538433074951 acc:0.699999988079071\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:1.6469404697418213 acc:0.6599999666213989\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:1.6430833339691162 acc:0.6699999570846558\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:1.5481117963790894 acc:0.6699999570846558\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:1.6386733055114746 acc:0.6699999570846558\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:1.5744569301605225 acc:0.6999999284744263\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:1.6577160358428955 acc:0.6599999666213989\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:1.62312650680542 acc:0.6799999475479126\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:1.553958773612976 acc:0.6799999475479126\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:1.6198081970214844 acc:0.6899999976158142\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:1.5627974271774292 acc:0.6899999380111694\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:1.6565245389938354 acc:0.6299999952316284\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:1.630741834640503 acc:0.7099999785423279\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:1.56005859375 acc:0.6699999570846558\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:1.6645680665969849 acc:0.6299999952316284\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:1.5527056455612183 acc:0.6799999475479126\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:1.6422908306121826 acc:0.6899999976158142\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:1.6304633617401123 acc:0.7199999690055847\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:1.5576918125152588 acc:0.6899999380111694\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:1.6478551626205444 acc:0.6899999380111694\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:1.5496491193771362 acc:0.7099999189376831\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:1.655844807624817 acc:0.5999999642372131\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:1.6331247091293335 acc:0.699999988079071\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:1.549940586090088 acc:0.699999988079071\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:1.640578269958496 acc:0.6899999380111694\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:1.5560863018035889 acc:0.699999988079071\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:1.6514052152633667 acc:0.6599999666213989\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:1.6137462854385376 acc:0.6999999284744263\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:1.5445902347564697 acc:0.6599999666213989\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:1.6391878128051758 acc:0.6999999284744263\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:1.5563502311706543 acc:0.7099999785423279\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:1.6554925441741943 acc:0.6699999570846558\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:1.6236987113952637 acc:0.699999988079071\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:1.5560119152069092 acc:0.7199999690055847\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:1.6358897686004639 acc:0.6699999570846558\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:1.5437160730361938 acc:0.7200000286102295\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:1.6418769359588623 acc:0.6899999380111694\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:1.6231995820999146 acc:0.6899999380111694\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:1.5596460103988647 acc:0.6699999570846558\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:1.6374447345733643 acc:0.699999988079071\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:1.5400002002716064 acc:0.7099999785423279\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:1.6516741514205933 acc:0.6899999380111694\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:1.6211109161376953 acc:0.6799999475479126\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:1.5393508672714233 acc:0.6799999475479126\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:1.643431305885315 acc:0.6799999475479126\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:1.5481256246566772 acc:0.6799999475479126\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:1.6495838165283203 acc:0.6599999666213989\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:1.6136363744735718 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:1.5470123291015625 acc:0.6999999284744263\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:1.6178327798843384 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:1.5378236770629883 acc:0.6899999380111694\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:1.647763729095459 acc:0.6899999380111694\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:1.6156246662139893 acc:0.6899999976158142\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:1.5413845777511597 acc:0.7199999690055847\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:1.6392743587493896 acc:0.6599999666213989\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:1.5359140634536743 acc:0.6899999380111694\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:1.639055609703064 acc:0.6699999570846558\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:1.6067516803741455 acc:0.6800000071525574\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:1.5437219142913818 acc:0.6499999761581421\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:1.6257084608078003 acc:0.6899999976158142\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:1.5381464958190918 acc:0.699999988079071\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:1.637221336364746 acc:0.699999988079071\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:1.6189260482788086 acc:0.699999988079071\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:1.5437734127044678 acc:0.6899999976158142\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:1.617304801940918 acc:0.699999988079071\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:1.5415923595428467 acc:0.7399999499320984\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:1.6425426006317139 acc:0.6899999976158142\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:1.605236291885376 acc:0.7299999594688416\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:1.5407761335372925 acc:0.699999988079071\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:1.6228750944137573 acc:0.6799999475479126\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:1.5315732955932617 acc:0.7199999690055847\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:1.6404939889907837 acc:0.6799999475479126\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:1.6039620637893677 acc:0.699999988079071\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:1.5339443683624268 acc:0.6599999666213989\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:1.6355538368225098 acc:0.6699999570846558\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:1.5427050590515137 acc:0.6899999976158142\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:1.6377944946289062 acc:0.6599999666213989\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:1.595412254333496 acc:0.7199999690055847\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:1.5353217124938965 acc:0.6799999475479126\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:1.637451410293579 acc:0.7299999594688416\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:1.5269197225570679 acc:0.7099999785423279\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:1.6386070251464844 acc:0.6799999475479126\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:1.5985323190689087 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:1.5239715576171875 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:1.6161928176879883 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:1.532171607017517 acc:0.699999988079071\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:1.6422619819641113 acc:0.6899999380111694\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:1.609466791152954 acc:0.7199999690055847\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:1.5167316198349 acc:0.7099999785423279\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:1.6226857900619507 acc:0.6899999976158142\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:1.5227973461151123 acc:0.7399999499320984\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:1.6445789337158203 acc:0.6899999976158142\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:1.6002357006072998 acc:0.7099999785423279\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:1.5220454931259155 acc:0.7099999785423279\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:1.623870611190796 acc:0.7299999594688416\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:1.5306966304779053 acc:0.7099999189376831\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:1.6377251148223877 acc:0.7099999785423279\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:1.5934840440750122 acc:0.7199999690055847\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:1.5251866579055786 acc:0.6699999570846558\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:1.6280038356781006 acc:0.6799999475479126\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:1.5320523977279663 acc:0.6899999380111694\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:1.63715398311615 acc:0.6599999666213989\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:1.5994415283203125 acc:0.7099999189376831\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:1.5255694389343262 acc:0.699999988079071\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:1.624150276184082 acc:0.6899999976158142\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:1.532472848892212 acc:0.6899999380111694\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:1.6383508443832397 acc:0.6999999284744263\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:1.5948154926300049 acc:0.7199999690055847\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:1.5213254690170288 acc:0.75\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:1.6319209337234497 acc:0.6999999284744263\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:1.5291844606399536 acc:0.7199999690055847\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:1.6331229209899902 acc:0.6299999952316284\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:1.5967427492141724 acc:0.7399999499320984\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:1.5210857391357422 acc:0.6699999570846558\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:1.617537021636963 acc:0.699999988079071\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:1.535344123840332 acc:0.6799999475479126\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:1.642852783203125 acc:0.6699999570846558\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:1.5908596515655518 acc:0.699999988079071\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:1.5177054405212402 acc:0.6999999284744263\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:1.6189889907836914 acc:0.7099999785423279\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:1.530592679977417 acc:0.75\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:1.6410573720932007 acc:0.6399999856948853\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:1.5999865531921387 acc:0.7499999403953552\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:1.5146418809890747 acc:0.6799999475479126\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:1.6213575601577759 acc:0.6799999475479126\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:1.5325971841812134 acc:0.7099999785423279\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:1.6390345096588135 acc:0.6799999475479126\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:1.5863358974456787 acc:0.7400000095367432\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:1.5241458415985107 acc:0.6899999380111694\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:1.6148641109466553 acc:0.6899999976158142\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:1.5325663089752197 acc:0.7299999594688416\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:1.639070749282837 acc:0.6999999284744263\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:1.5807677507400513 acc:0.7299999594688416\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:1.536435842514038 acc:0.7199999690055847\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:1.6139041185379028 acc:0.6899999976158142\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:1.5272127389907837 acc:0.7099999785423279\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:1.6401938199996948 acc:0.6699999570846558\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:1.5976336002349854 acc:0.7099999785423279\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:1.5198978185653687 acc:0.6899999380111694\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:1.617289423942566 acc:0.7099999785423279\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:1.5255117416381836 acc:0.7099999785423279\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:1.6393264532089233 acc:0.6899999380111694\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:1.575247049331665 acc:0.7299999594688416\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:1.5195469856262207 acc:0.699999988079071\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:1.6182903051376343 acc:0.699999988079071\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:1.5288662910461426 acc:0.6899999380111694\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:1.647932767868042 acc:0.7099999785423279\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:1.5759457349777222 acc:0.7299999594688416\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:1.5242054462432861 acc:0.6799999475479126\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:1.618408203125 acc:0.6799999475479126\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:1.523551344871521 acc:0.7199999690055847\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:1.6376304626464844 acc:0.6899999380111694\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:1.5801692008972168 acc:0.7299999594688416\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:1.5223920345306396 acc:0.7399999499320984\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:1.6161997318267822 acc:0.7299999594688416\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:1.5216565132141113 acc:0.7299999594688416\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:1.6367714405059814 acc:0.7199999690055847\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:1.5746058225631714 acc:0.7399999499320984\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:1.514349341392517 acc:0.7499999403953552\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:1.6219291687011719 acc:0.7300000190734863\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:1.52218759059906 acc:0.7099999189376831\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:1.640549659729004 acc:0.6700000166893005\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:1.56620192527771 acc:0.6899999976158142\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:1.5119531154632568 acc:0.7400000095367432\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:1.6076774597167969 acc:0.6899999380111694\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:1.5158084630966187 acc:0.7299999594688416\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:1.6414413452148438 acc:0.6899999380111694\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss:1.5658142566680908 acc:0.7099999785423279\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss:1.5125688314437866 acc:0.7099999189376831\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss:1.6146360635757446 acc:0.7399999499320984\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss:1.5206735134124756 acc:0.7399999499320984\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:1.6359626054763794 acc:0.699999988079071\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss:1.5682697296142578 acc:0.7300000190734863\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss:1.514272928237915 acc:0.7299999594688416\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss:1.6041816473007202 acc:0.7099999785423279\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss:1.516883373260498 acc:0.75\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:1.6317501068115234 acc:0.7099999785423279\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss:1.5523333549499512 acc:0.7599999904632568\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss:1.5144426822662354 acc:0.7400000095367432\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss:1.6032135486602783 acc:0.7099999785423279\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss:1.519930124282837 acc:0.7400000095367432\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:1.620991587638855 acc:0.7299999594688416\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss:1.565626859664917 acc:0.7299999594688416\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss:1.514378309249878 acc:0.7299999594688416\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss:1.5968518257141113 acc:0.7299999594688416\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss:1.5214866399765015 acc:0.7499999403953552\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:1.6378782987594604 acc:0.7099999785423279\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss:1.5657496452331543 acc:0.7499999403953552\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss:1.5135804414749146 acc:0.7199999690055847\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss:1.6109120845794678 acc:0.699999988079071\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss:1.5185182094573975 acc:0.7599999904632568\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:1.6314935684204102 acc:0.6899999380111694\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss:1.5628265142440796 acc:0.7599999904632568\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss:1.5201265811920166 acc:0.6899999380111694\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss:1.6017847061157227 acc:0.7299999594688416\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss:1.5150837898254395 acc:0.7399999499320984\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:1.6365997791290283 acc:0.7199999690055847\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss:1.567542552947998 acc:0.75\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss:1.5146503448486328 acc:0.7599999904632568\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss:1.5985215902328491 acc:0.6999999284744263\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss:1.5151543617248535 acc:0.7599999904632568\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:1.635357141494751 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss:1.5590672492980957 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss:1.5100140571594238 acc:0.7400000095367432\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss:1.5983859300613403 acc:0.7399999499320984\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss:1.514484167098999 acc:0.699999988079071\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:1.6338534355163574 acc:0.7199999690055847\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss:1.5614869594573975 acc:0.7399999499320984\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss:1.513965368270874 acc:0.7099999785423279\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss:1.5953353643417358 acc:0.7299999594688416\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss:1.5222580432891846 acc:0.7099999785423279\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:1.6383792161941528 acc:0.7299999594688416\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss:1.5648022890090942 acc:0.7199999690055847\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss:1.512204647064209 acc:0.7199999690055847\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss:1.6035315990447998 acc:0.7099999785423279\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss:1.5141239166259766 acc:0.7399999499320984\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:1.6373822689056396 acc:0.7099999785423279\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss:1.5532426834106445 acc:0.75\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss:1.5140284299850464 acc:0.6899999380111694\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss:1.5905624628067017 acc:0.7199999690055847\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss:1.5152842998504639 acc:0.7199999690055847\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:1.6393787860870361 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss:1.5602809190750122 acc:0.7399999499320984\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss:1.5109177827835083 acc:0.7299999594688416\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss:1.5955880880355835 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss:1.513594388961792 acc:0.7299999594688416\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:1.6312475204467773 acc:0.7299999594688416\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss:1.564961552619934 acc:0.7599999904632568\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss:1.5111761093139648 acc:0.7300000190734863\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss:1.5967116355895996 acc:0.7199999690055847\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss:1.5125584602355957 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:1.6326161623001099 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss:1.5631425380706787 acc:0.7599999904632568\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss:1.5104413032531738 acc:0.699999988079071\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss:1.599069356918335 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss:1.5134093761444092 acc:0.7199999690055847\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:1.629682183265686 acc:0.7400000095367432\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss:1.5507084131240845 acc:0.7099999785423279\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss:1.5119965076446533 acc:0.7099999785423279\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss:1.5920934677124023 acc:0.699999988079071\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss:1.5121541023254395 acc:0.7299999594688416\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:1.6419520378112793 acc:0.7299999594688416\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss:1.562175989151001 acc:0.7099999189376831\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss:1.5058612823486328 acc:0.7199999690055847\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss:1.5968068838119507 acc:0.699999988079071\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss:1.5109422206878662 acc:0.7400000095367432\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:1.6354765892028809 acc:0.6699999570846558\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss:1.561537504196167 acc:0.7400000095367432\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss:1.5123088359832764 acc:0.699999988079071\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss:1.5878612995147705 acc:0.699999988079071\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss:1.5136991739273071 acc:0.7399999499320984\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:1.632510781288147 acc:0.7299999594688416\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss:1.5460853576660156 acc:0.7299999594688416\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss:1.5110859870910645 acc:0.699999988079071\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss:1.587697148323059 acc:0.7099999785423279\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss:1.5117019414901733 acc:0.7399999499320984\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:1.630912184715271 acc:0.7199999690055847\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss:1.562265396118164 acc:0.699999988079071\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss:1.511810064315796 acc:0.7299999594688416\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss:1.5866215229034424 acc:0.7099999785423279\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss:1.4954838752746582 acc:0.7299999594688416\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:1.6268614530563354 acc:0.7099999785423279\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss:1.545492172241211 acc:0.7199999690055847\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss:1.5072078704833984 acc:0.7099999785423279\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss:1.5771605968475342 acc:0.6899999976158142\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss:1.4982916116714478 acc:0.7099999785423279\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:1.6337381601333618 acc:0.6899999976158142\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss:1.560270071029663 acc:0.7299999594688416\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss:1.5087529420852661 acc:0.7399999499320984\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss:1.5787497758865356 acc:0.7499999403953552\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss:1.503631591796875 acc:0.7199999690055847\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:1.6286144256591797 acc:0.6899999380111694\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss:1.5553309917449951 acc:0.75\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss:1.5095912218093872 acc:0.7299999594688416\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss:1.5887045860290527 acc:0.6899999976158142\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss:1.5053980350494385 acc:0.7099999785423279\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:1.6409549713134766 acc:0.6799999475479126\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss:1.5558218955993652 acc:0.7400000095367432\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss:1.5075089931488037 acc:0.7099999785423279\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss:1.585649013519287 acc:0.699999988079071\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss:1.5051054954528809 acc:0.7099999785423279\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:1.6186792850494385 acc:0.7099999785423279\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss:1.5638774633407593 acc:0.7199999690055847\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss:1.5102171897888184 acc:0.699999988079071\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss:1.5753612518310547 acc:0.6799999475479126\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss:1.5012738704681396 acc:0.7299999594688416\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:1.6222736835479736 acc:0.7299999594688416\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss:1.5481454133987427 acc:0.7199999690055847\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss:1.5111831426620483 acc:0.6899999380111694\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss:1.5822880268096924 acc:0.7199999690055847\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss:1.5098258256912231 acc:0.7299999594688416\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:1.6289082765579224 acc:0.7199999690055847\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss:1.563633918762207 acc:0.7400000095367432\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss:1.508012294769287 acc:0.6800000071525574\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss:1.5779712200164795 acc:0.6899999976158142\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss:1.5111973285675049 acc:0.7199999690055847\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:1.604464054107666 acc:0.7199999690055847\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss:1.544814109802246 acc:0.75\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss:1.50601327419281 acc:0.7199999690055847\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss:1.5746843814849854 acc:0.7199999690055847\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss:1.5123765468597412 acc:0.7299999594688416\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:1.6190040111541748 acc:0.6499999761581421\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss:1.550781488418579 acc:0.7400000095367432\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss:1.509626865386963 acc:0.7299998998641968\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss:1.5862120389938354 acc:0.7099999189376831\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss:1.5112528800964355 acc:0.7399999499320984\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:1.6217647790908813 acc:0.7199999690055847\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss:1.5388261079788208 acc:0.75\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss:1.5108801126480103 acc:0.7099999785423279\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss:1.5728178024291992 acc:0.7099999785423279\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss:1.5046831369400024 acc:0.75\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:1.6274805068969727 acc:0.6899999976158142\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss:1.543199896812439 acc:0.7299999594688416\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss:1.511130928993225 acc:0.7099999189376831\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss:1.5714386701583862 acc:0.6800000071525574\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss:1.513288974761963 acc:0.7499999403953552\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:1.6227123737335205 acc:0.699999988079071\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss:1.5440645217895508 acc:0.7400000095367432\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss:1.5076584815979004 acc:0.699999988079071\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss:1.572908639907837 acc:0.7200000286102295\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss:1.5138113498687744 acc:0.7199999690055847\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:1.615173578262329 acc:0.7299999594688416\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss:1.543177843093872 acc:0.7299999594688416\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss:1.5051052570343018 acc:0.699999988079071\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss:1.5759806632995605 acc:0.6699999570846558\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss:1.510026454925537 acc:0.7099999189376831\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:1.614262580871582 acc:0.7099999785423279\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss:1.5429402589797974 acc:0.7300000190734863\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss:1.5100551843643188 acc:0.6799999475479126\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss:1.5732982158660889 acc:0.7099999785423279\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss:1.5119805335998535 acc:0.7099999785423279\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:1.6220194101333618 acc:0.7300000190734863\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss:1.5408210754394531 acc:0.7399999499320984\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss:1.5121654272079468 acc:0.7199999690055847\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss:1.5648691654205322 acc:0.7199999690055847\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss:1.5126988887786865 acc:0.6800000071525574\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:1.6057765483856201 acc:0.6899999976158142\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss:1.5395681858062744 acc:0.7299999594688416\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss:1.5091512203216553 acc:0.6999999284744263\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss:1.5695366859436035 acc:0.6899999380111694\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss:1.511367678642273 acc:0.7399999499320984\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:1.614081621170044 acc:0.6800000071525574\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss:1.543454647064209 acc:0.75\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss:1.5106152296066284 acc:0.7099999189376831\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss:1.5687248706817627 acc:0.7199999094009399\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss:1.51095712184906 acc:0.699999988079071\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:1.6254369020462036 acc:0.7099999785423279\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss:1.5634956359863281 acc:0.7400000095367432\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss:1.5093529224395752 acc:0.7199999690055847\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss:1.5745556354522705 acc:0.7399999499320984\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss:1.5109683275222778 acc:0.7299999594688416\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:1.632487416267395 acc:0.7099999785423279\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss:1.5619537830352783 acc:0.7400000095367432\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss:1.510355830192566 acc:0.6899999380111694\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss:1.5651659965515137 acc:0.7299999594688416\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss:1.5137920379638672 acc:0.7099999785423279\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:1.6198599338531494 acc:0.7499999403953552\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss:1.5465980768203735 acc:0.75\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss:1.5078593492507935 acc:0.7099999785423279\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss:1.5680863857269287 acc:0.75\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss:1.5080745220184326 acc:0.7099999785423279\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:1.6191775798797607 acc:0.6799999475479126\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss:1.5518817901611328 acc:0.7400000095367432\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss:1.5102022886276245 acc:0.7199999690055847\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss:1.5701313018798828 acc:0.7299999594688416\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss:1.5065488815307617 acc:0.699999988079071\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:1.6013240814208984 acc:0.7499999403953552\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss:1.546301245689392 acc:0.7499999403953552\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss:1.511713981628418 acc:0.699999988079071\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss:1.5636401176452637 acc:0.7200000286102295\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss:1.5103974342346191 acc:0.7599999904632568\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:1.6005291938781738 acc:0.7199999690055847\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss:1.5400975942611694 acc:0.7299999594688416\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss:1.5084573030471802 acc:0.7199999690055847\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss:1.5638865232467651 acc:0.7299999594688416\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss:1.51289701461792 acc:0.6999999284744263\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:1.601462721824646 acc:0.7300000190734863\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss:1.5385615825653076 acc:0.7200000286102295\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss:1.5093061923980713 acc:0.699999988079071\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss:1.5642343759536743 acc:0.7199999690055847\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss:1.5043625831604004 acc:0.6999999284744263\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:1.6109318733215332 acc:0.7099999785423279\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss:1.5537352561950684 acc:0.7599999308586121\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss:1.5027276277542114 acc:0.7399999499320984\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss:1.5625500679016113 acc:0.7499999403953552\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss:1.5015921592712402 acc:0.7300000190734863\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:1.5948106050491333 acc:0.7499999403953552\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss:1.5447776317596436 acc:0.7399999499320984\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss:1.4916192293167114 acc:0.7199999690055847\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss:1.5633121728897095 acc:0.7399999499320984\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss:1.5070643424987793 acc:0.7099999785423279\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:1.6095939874649048 acc:0.6999999284744263\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss:1.5366531610488892 acc:0.7399999499320984\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss:1.5046148300170898 acc:0.7099999189376831\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss:1.5675432682037354 acc:0.7299999594688416\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss:1.5008463859558105 acc:0.6999999284744263\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:1.5860786437988281 acc:0.6899999380111694\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss:1.5560572147369385 acc:0.7499999403953552\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss:1.48905348777771 acc:0.7200000286102295\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss:1.563506007194519 acc:0.7199999690055847\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss:1.4973483085632324 acc:0.7399999499320984\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:1.587406873703003 acc:0.7099999785423279\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss:1.5533186197280884 acc:0.7400000095367432\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss:1.4908523559570312 acc:0.6799999475479126\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss:1.5611131191253662 acc:0.7199999690055847\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss:1.5087711811065674 acc:0.7299999594688416\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:1.585702657699585 acc:0.6599999666213989\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss:1.536392331123352 acc:0.7199999690055847\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss:1.4915584325790405 acc:0.7299999594688416\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss:1.5638316869735718 acc:0.699999988079071\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss:1.495869755744934 acc:0.6999999284744263\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:1.5877866744995117 acc:0.7199999690055847\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss:1.5429381132125854 acc:0.7099999189376831\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss:1.4913640022277832 acc:0.6999999284744263\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss:1.5618624687194824 acc:0.7199999690055847\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss:1.5017952919006348 acc:0.6899999380111694\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:1.5889122486114502 acc:0.6699999570846558\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss:1.5365664958953857 acc:0.75\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss:1.4907350540161133 acc:0.699999988079071\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss:1.5625606775283813 acc:0.7299999594688416\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss:1.4935452938079834 acc:0.699999988079071\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:1.5914053916931152 acc:0.6899999380111694\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss:1.5544434785842896 acc:0.7599999904632568\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss:1.510469913482666 acc:0.7299999594688416\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss:1.5634138584136963 acc:0.7299999594688416\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss:1.4929989576339722 acc:0.7199999690055847\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:1.5856411457061768 acc:0.6799999475479126\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss:1.5425000190734863 acc:0.7400000095367432\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss:1.4977844953536987 acc:0.6999999284744263\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss:1.562485933303833 acc:0.7300000190734863\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss:1.4900362491607666 acc:0.6899999380111694\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:1.5903117656707764 acc:0.7099999785423279\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss:1.551838755607605 acc:0.7199999690055847\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss:1.5006039142608643 acc:0.6899999380111694\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss:1.56038236618042 acc:0.7199999690055847\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss:1.5014207363128662 acc:0.7099999189376831\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:1.5858734846115112 acc:0.7099999785423279\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss:1.537200927734375 acc:0.7599999904632568\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss:1.4876652956008911 acc:0.7400000095367432\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss:1.5610089302062988 acc:0.7099999189376831\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss:1.4943132400512695 acc:0.7199999690055847\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:1.5871634483337402 acc:0.699999988079071\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss:1.5377540588378906 acc:0.7400000095367432\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss:1.4875516891479492 acc:0.6899999380111694\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss:1.562547206878662 acc:0.7299999594688416\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss:1.503446102142334 acc:0.7299999594688416\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:1.5898492336273193 acc:0.7399999499320984\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss:1.5366911888122559 acc:0.7399999499320984\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss:1.4903297424316406 acc:0.6899999380111694\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss:1.5615986585617065 acc:0.75\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss:1.5091207027435303 acc:0.7299999594688416\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:1.5854874849319458 acc:0.6999999284744263\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss:1.5370832681655884 acc:0.75\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss:1.4985963106155396 acc:0.7099999189376831\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss:1.5613024234771729 acc:0.699999988079071\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss:1.498748779296875 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:1.5987141132354736 acc:0.6799999475479126\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss:1.5375481843948364 acc:0.7399999499320984\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss:1.4949541091918945 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss:1.5633915662765503 acc:0.7099999785423279\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss:1.5003587007522583 acc:0.7400000095367432\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:1.6008641719818115 acc:0.6899999380111694\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss:1.5425634384155273 acc:0.7400000095367432\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss:1.4870027303695679 acc:0.7299999594688416\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss:1.5607445240020752 acc:0.6999999284744263\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss:1.5023460388183594 acc:0.7399999499320984\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:1.5893268585205078 acc:0.699999988079071\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss:1.5375170707702637 acc:0.7299999594688416\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss:1.4916679859161377 acc:0.6899999976158142\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss:1.5620535612106323 acc:0.7200000286102295\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss:1.489695429801941 acc:0.7499999403953552\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:1.5955886840820312 acc:0.7400000095367432\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss:1.5370500087738037 acc:0.7400000095367432\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss:1.4874939918518066 acc:0.7199999690055847\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss:1.560898780822754 acc:0.7299999594688416\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss:1.5085453987121582 acc:0.7199999690055847\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:1.592960238456726 acc:0.7199999690055847\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss:1.5379400253295898 acc:0.7199999690055847\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss:1.4889848232269287 acc:0.6999999284744263\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss:1.5614631175994873 acc:0.699999988079071\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss:1.4936795234680176 acc:0.7099999189376831\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:1.5867388248443604 acc:0.7199999690055847\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss:1.5377287864685059 acc:0.7399999499320984\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss:1.4873936176300049 acc:0.7199999094009399\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss:1.5605370998382568 acc:0.699999988079071\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss:1.5007044076919556 acc:0.7199999690055847\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:1.593017816543579 acc:0.699999988079071\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss:1.5405811071395874 acc:0.7300000190734863\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss:1.4868106842041016 acc:0.6799999475479126\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss:1.5623195171356201 acc:0.7199999690055847\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss:1.5075814723968506 acc:0.7299999594688416\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:1.5954376459121704 acc:0.7099999785423279\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss:1.5443307161331177 acc:0.7199999690055847\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss:1.486768364906311 acc:0.7299999594688416\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss:1.5628254413604736 acc:0.6899999380111694\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss:1.4949488639831543 acc:0.6899999380111694\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:1.5926926136016846 acc:0.7299999594688416\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss:1.5369411706924438 acc:0.7299999594688416\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss:1.4876785278320312 acc:0.7199999690055847\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss:1.5628796815872192 acc:0.7499999403953552\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss:1.4966095685958862 acc:0.6999999284744263\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:1.6000549793243408 acc:0.7099999785423279\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss:1.5378785133361816 acc:0.7399999499320984\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss:1.4868433475494385 acc:0.7199999690055847\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss:1.5595593452453613 acc:0.7400000095367432\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss:1.5022670030593872 acc:0.7099999785423279\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:1.5909240245819092 acc:0.7099999785423279\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss:1.5385403633117676 acc:0.7399999499320984\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss:1.4973315000534058 acc:0.7299999594688416\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss:1.5604122877120972 acc:0.7299999594688416\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss:1.4914942979812622 acc:0.7199999690055847\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:1.5863394737243652 acc:0.7099999189376831\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss:1.538946509361267 acc:0.75\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss:1.4994559288024902 acc:0.7299999594688416\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss:1.5611587762832642 acc:0.7299999594688416\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss:1.5029942989349365 acc:0.7099999785423279\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:1.5870835781097412 acc:0.6899999976158142\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss:1.5373265743255615 acc:0.7499999403953552\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss:1.4872885942459106 acc:0.6899999380111694\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss:1.5602483749389648 acc:0.7299999594688416\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss:1.4911274909973145 acc:0.7199999690055847\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:1.5865042209625244 acc:0.7399999499320984\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss:1.538069725036621 acc:0.7199999690055847\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss:1.486565351486206 acc:0.7199999690055847\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss:1.560374140739441 acc:0.7199999690055847\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss:1.4897592067718506 acc:0.7199999094009399\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:1.5849945545196533 acc:0.7199999094009399\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss:1.5357046127319336 acc:0.7199999690055847\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss:1.4879703521728516 acc:0.7399999499320984\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss:1.5632481575012207 acc:0.7299999594688416\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss:1.497248649597168 acc:0.7299999594688416\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:1.5871930122375488 acc:0.699999988079071\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss:1.5378806591033936 acc:0.75\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss:1.4864239692687988 acc:0.7199999690055847\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss:1.5599262714385986 acc:0.75\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss:1.497678518295288 acc:0.7300000190734863\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:1.5860130786895752 acc:0.7099999189376831\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss:1.5361354351043701 acc:0.7599999904632568\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss:1.4863930940628052 acc:0.7099999189376831\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss:1.5614426136016846 acc:0.7099999785423279\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss:1.5120906829833984 acc:0.7099999189376831\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:1.5846809148788452 acc:0.7099999785423279\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss:1.536447286605835 acc:0.7399999499320984\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss:1.486666202545166 acc:0.699999988079071\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss:1.560824990272522 acc:0.7199999690055847\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss:1.5024381875991821 acc:0.7099999785423279\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:1.5877046585083008 acc:0.6499999761581421\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss:1.536289930343628 acc:0.7599999904632568\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss:1.4867055416107178 acc:0.7199999690055847\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss:1.560896396636963 acc:0.6899999380111694\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss:1.4907140731811523 acc:0.7299999594688416\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:1.59388267993927 acc:0.7499999403953552\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss:1.536916971206665 acc:0.7699999809265137\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss:1.4868664741516113 acc:0.6899999380111694\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss:1.5602936744689941 acc:0.7199999690055847\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss:1.4911623001098633 acc:0.7099999785423279\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:1.587601900100708 acc:0.6799999475479126\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss:1.5358126163482666 acc:0.7499999403953552\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss:1.4864513874053955 acc:0.6999999284744263\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss:1.5609080791473389 acc:0.7199999690055847\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss:1.4874019622802734 acc:0.7099999189376831\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:1.5849132537841797 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss:1.5359044075012207 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss:1.4864784479141235 acc:0.7499999403953552\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss:1.5601493120193481 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss:1.493459701538086 acc:0.7499999403953552\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:1.5853387117385864 acc:0.7399999499320984\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss:1.5364959239959717 acc:0.7400000095367432\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss:1.486333966255188 acc:0.7300000190734863\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss:1.5613517761230469 acc:0.7099999785423279\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss:1.4870169162750244 acc:0.6799999475479126\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:1.5848853588104248 acc:0.7199999690055847\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss:1.537556529045105 acc:0.7699999809265137\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss:1.4864203929901123 acc:0.7399999499320984\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss:1.5616341829299927 acc:0.75\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss:1.4868543148040771 acc:0.7299999594688416\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:1.5845354795455933 acc:0.6999999284744263\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss:1.5473350286483765 acc:0.7400000095367432\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss:1.486429214477539 acc:0.7399999499320984\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss:1.559904932975769 acc:0.7299999594688416\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss:1.4874773025512695 acc:0.7399999499320984\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:1.5849460363388062 acc:0.7599999904632568\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss:1.5360809564590454 acc:0.7199999690055847\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss:1.486386775970459 acc:0.7199999690055847\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss:1.5603418350219727 acc:0.7199999690055847\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss:1.4874358177185059 acc:0.6999999284744263\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:1.5853872299194336 acc:0.7199999690055847\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss:1.5366418361663818 acc:0.7299999594688416\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss:1.4870060682296753 acc:0.75\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss:1.5613267421722412 acc:0.6899999380111694\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss:1.4904894828796387 acc:0.699999988079071\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:1.5852272510528564 acc:0.7399999499320984\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss:1.5370405912399292 acc:0.7299999594688416\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss:1.486582636833191 acc:0.7400000095367432\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss:1.5604416131973267 acc:0.7099999785423279\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss:1.4968281984329224 acc:0.7300000190734863\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:1.5852017402648926 acc:0.6999999284744263\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss:1.535406231880188 acc:0.7299999594688416\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss:1.4870047569274902 acc:0.7399999499320984\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss:1.5608161687850952 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss:1.4867913722991943 acc:0.7199999690055847\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:1.5841403007507324 acc:0.7199999094009399\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss:1.5355473756790161 acc:0.699999988079071\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss:1.4863544702529907 acc:0.7399999499320984\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss:1.5604867935180664 acc:0.7199999690055847\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss:1.4866151809692383 acc:0.7099999785423279\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:1.5851248502731323 acc:0.6699999570846558\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss:1.5356850624084473 acc:0.7399999499320984\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss:1.4864051342010498 acc:0.7399999499320984\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss:1.5609771013259888 acc:0.699999988079071\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss:1.4873154163360596 acc:0.6999999284744263\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:1.5885975360870361 acc:0.7099999785423279\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss:1.5363454818725586 acc:0.7499999403953552\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss:1.4865176677703857 acc:0.7399999499320984\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss:1.5615131855010986 acc:0.7099999785423279\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss:1.4868078231811523 acc:0.7300000190734863\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7019307324840764\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOBYRERERCRQci4iIiIgE/w8ixxPmf1KUZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e0113d9e8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
