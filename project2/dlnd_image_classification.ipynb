{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:46, 3.69MB/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 0:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+4ZgtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f2474dc18>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 0\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    tmp=np.ndarray(x.shape,dtype=float)\n",
    "    tmp[:] = x\n",
    "    tmp[...] /= 255.0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    ret = np.zeros((len(x),10))\n",
    "    for idx,lbl in enumerate(x):\n",
    "        ret[idx,lbl] = 1\n",
    "        \n",
    "    return ret\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " [[[ 0.10980392  0.09803922  0.03921569]\n",
      "   [ 0.14509804  0.13333333  0.0745098 ]\n",
      "   [ 0.14901961  0.1372549   0.07843137]\n",
      "   ..., \n",
      "   [ 0.29803922  0.2627451   0.15294118]\n",
      "   [ 0.31764706  0.28235294  0.16862745]\n",
      "   [ 0.33333333  0.29803922  0.18431373]]\n",
      "\n",
      "  [[ 0.12941176  0.10980392  0.05098039]\n",
      "   [ 0.13333333  0.11764706  0.05490196]\n",
      "   [ 0.1254902   0.10588235  0.04705882]\n",
      "   ..., \n",
      "   [ 0.37254902  0.32156863  0.21568627]\n",
      "   [ 0.37647059  0.32156863  0.21960784]\n",
      "   [ 0.33333333  0.28235294  0.17647059]]\n",
      "\n",
      "  [[ 0.15294118  0.1254902   0.05882353]\n",
      "   [ 0.15686275  0.12941176  0.06666667]\n",
      "   [ 0.22352941  0.19607843  0.12941176]\n",
      "   ..., \n",
      "   [ 0.36470588  0.29803922  0.20392157]\n",
      "   [ 0.41960784  0.34901961  0.25882353]\n",
      "   [ 0.37254902  0.30196078  0.21176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.3254902   0.28627451  0.20392157]\n",
      "   [ 0.34117647  0.30196078  0.21960784]\n",
      "   [ 0.32941176  0.29019608  0.20392157]\n",
      "   ..., \n",
      "   [ 0.38823529  0.36470588  0.2745098 ]\n",
      "   [ 0.35294118  0.32941176  0.23921569]\n",
      "   [ 0.31764706  0.29411765  0.20392157]]\n",
      "\n",
      "  [[ 0.34509804  0.28235294  0.2       ]\n",
      "   [ 0.35294118  0.29019608  0.20392157]\n",
      "   [ 0.36470588  0.30196078  0.21960784]\n",
      "   ..., \n",
      "   [ 0.31372549  0.29019608  0.20784314]\n",
      "   [ 0.29803922  0.2745098   0.19215686]\n",
      "   [ 0.32156863  0.29803922  0.21568627]]\n",
      "\n",
      "  [[ 0.38039216  0.30588235  0.21960784]\n",
      "   [ 0.36862745  0.29411765  0.20784314]\n",
      "   [ 0.36470588  0.29411765  0.20784314]\n",
      "   ..., \n",
      "   [ 0.21176471  0.18431373  0.10980392]\n",
      "   [ 0.24705882  0.21960784  0.14509804]\n",
      "   [ 0.28235294  0.25490196  0.18039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.66666667  0.70588235  0.77647059]\n",
      "   [ 0.65882353  0.69803922  0.76862745]\n",
      "   [ 0.69411765  0.7254902   0.79607843]\n",
      "   ..., \n",
      "   [ 0.63529412  0.70196078  0.84313725]\n",
      "   [ 0.61960784  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]]\n",
      "\n",
      "  [[ 0.65882353  0.70980392  0.77647059]\n",
      "   [ 0.6745098   0.7254902   0.78823529]\n",
      "   [ 0.67058824  0.71764706  0.78431373]\n",
      "   ..., \n",
      "   [ 0.62352941  0.69411765  0.83137255]\n",
      "   [ 0.61176471  0.69019608  0.82745098]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  [[ 0.60392157  0.66666667  0.72941176]\n",
      "   [ 0.58431373  0.64705882  0.70980392]\n",
      "   [ 0.50588235  0.56470588  0.63529412]\n",
      "   ..., \n",
      "   [ 0.63137255  0.69803922  0.83921569]\n",
      "   [ 0.61568627  0.69411765  0.83137255]\n",
      "   [ 0.60392157  0.68235294  0.81960784]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.32941176  0.31372549]\n",
      "   [ 0.29803922  0.33333333  0.31764706]\n",
      "   [ 0.30588235  0.33333333  0.32156863]\n",
      "   ..., \n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.28235294  0.29411765]\n",
      "   [ 0.23921569  0.25490196  0.26666667]]\n",
      "\n",
      "  [[ 0.26666667  0.29803922  0.30196078]\n",
      "   [ 0.27058824  0.30196078  0.30588235]\n",
      "   [ 0.28235294  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.29803922  0.31372549  0.3254902 ]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.27843137  0.29411765  0.30588235]]\n",
      "\n",
      "  [[ 0.2627451   0.29411765  0.30588235]\n",
      "   [ 0.26666667  0.29803922  0.30980392]\n",
      "   [ 0.27058824  0.29411765  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30980392  0.32156863]\n",
      "   [ 0.27843137  0.29411765  0.30588235]\n",
      "   [ 0.28627451  0.30196078  0.31372549]]]]\n"
     ]
    }
   ],
   "source": [
    "filename = 'preprocess_batch_' + str(1) + '.p'\n",
    "features, labels = pickle.load(open(filename, mode='rb'))\n",
    "print(features[0:5,...])\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    (width, height, chann) = image_shape\n",
    "    x = tf.placeholder(tf.float32, [None, width, height, chann], name='x')\n",
    "    print(x.get_shape())\n",
    "    # TODO: Implement Function\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    y= tf.placeholder(tf.int8, [None,n_classes], name='y')\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tmp = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(type(x_tensor.get_shape().as_list()[3] ))\n",
    "    print(conv_ksize)\n",
    "    (convk1, convk2) = conv_ksize\n",
    "    (convs1, convs2) = conv_strides\n",
    "    (poolk1, poolk2) = pool_ksize\n",
    "    (pools1, pools2) = pool_strides\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([convk1, convk2,x_tensor.get_shape().as_list()[3],conv_num_outputs ], stddev=0.1, mean=0.0,dtype=tf.float32, seed=None, name=None ))\n",
    "    biases= tf.Variable(tf.constant(0,dtype=tf.float32, shape=[conv_num_outputs]))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=x_tensor, filter= weights, strides = [1,convs1, convs2,1],padding='SAME')\n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer += biases\n",
    "    #print(layer.get_shape().as_list())\n",
    "    \n",
    "    #print(layer.get_shape().as_list())\n",
    "    layer = tf.nn.max_pool(value=layer,ksize=[1,poolk1,poolk2,1], \n",
    "                           strides = [1,pools1,pools2,1],padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    #print(layer.get_shape().as_list())\n",
    "    #print(conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    return layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 10 30 6\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    [batch_size, h,w,c] = x_tensor.get_shape().as_list()\n",
    "    features= h*w*c\n",
    "    \n",
    "    layer = tf.reshape(x_tensor, [-1, features])\n",
    "    print(batch_size,h,w,c)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    #layer = tf.nn.relu(layer)\n",
    "    layer = tf.nn.sigmoid(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    numins = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal([numins, num_outputs], stddev=0.1, mean=0.0, dtype=tf.float32, seed=None, name=None))\n",
    "    biases = tf.Variable(tf.constant(0, dtype=tf.float32,shape=[num_outputs]))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor,weights) + biases\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "(5, 5)\n",
      "(5, 5)\n",
      "None 8 8 40\n",
      "[None, 2560]\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convl = conv2d_maxpool(x,20, (5,5), (1,1), (2,2), (2,2))\n",
    "    convl = conv2d_maxpool(convl,40, (5,5), (1,1), (2,2), (2,2))\n",
    "    #convl = conv2d_maxpool(convl,64, (5,5), (1,1), (2,2), (1,1))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    layer = flatten(convl)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    print(layer.get_shape().as_list())\n",
    "    layer = fully_conn(layer,250)\n",
    "    layer = tf.nn.dropout(layer, keep_prob)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,50)\n",
    "#     layer = fully_conn(layer,250)\n",
    "#     layer = tf.layers.dropout(layer, 0.5)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    layer= output(layer,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.01,epsilon=0.00001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "    # TODO: Implement Function\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(type(valid_features), type(valid_labels), type(feature_batch), type(label_batch))\n",
    "    #print(\"print_stats valid_features:{} valid_labels:{}\".format(valid_features.shape, valid_labels.shape))\n",
    "    \n",
    "    \n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    valid = sess.run(accuracy, feed_dict={\n",
    "        x: valid_features[0:100,...],\n",
    "        y: valid_labels[0:100,...],\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    print('loss:{} acc:{}'.format( loss,valid))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.214082717895508 acc:0.23999999463558197\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.1046571731567383 acc:0.31999996304512024\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:2.0632243156433105 acc:0.3499999940395355\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:2.0108089447021484 acc:0.3799999952316284\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.9602479934692383 acc:0.3799999952316284\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.9005932807922363 acc:0.3799999952316284\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.8946516513824463 acc:0.4099999964237213\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.8377745151519775 acc:0.37999996542930603\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.8306454420089722 acc:0.4099999666213989\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.8145322799682617 acc:0.3999999761581421\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.7745850086212158 acc:0.38999998569488525\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.7608470916748047 acc:0.4300000071525574\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.7493131160736084 acc:0.3999999761581421\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.7187004089355469 acc:0.41999995708465576\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.698387861251831 acc:0.4099999666213989\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.6901929378509521 acc:0.4299999475479126\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.6470985412597656 acc:0.4399999976158142\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.6381909847259521 acc:0.4699999690055847\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.6300289630889893 acc:0.4399999976158142\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.6284332275390625 acc:0.44999998807907104\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.5980663299560547 acc:0.47999998927116394\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.5678675174713135 acc:0.44999995827674866\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.5409153699874878 acc:0.4699999988079071\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.5371204614639282 acc:0.44999998807907104\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.5069420337677002 acc:0.4899999797344208\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.4845640659332275 acc:0.4699999988079071\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.4732763767242432 acc:0.4699999690055847\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.4748997688293457 acc:0.46000000834465027\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.4324768781661987 acc:0.4699999988079071\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:1.4278686046600342 acc:0.4899999797344208\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:1.4096884727478027 acc:0.46000000834465027\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:1.4066861867904663 acc:0.4399999976158142\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:1.4018478393554688 acc:0.48000001907348633\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:1.3678535223007202 acc:0.4699999988079071\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:1.3594762086868286 acc:0.4899999797344208\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:1.3444628715515137 acc:0.5\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:1.3233753442764282 acc:0.4700000286102295\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:1.3149170875549316 acc:0.5\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:1.2854043245315552 acc:0.49000000953674316\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:1.3010344505310059 acc:0.5\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:1.2745661735534668 acc:0.5099999904632568\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:1.2643702030181885 acc:0.49000000953674316\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:1.248939037322998 acc:0.5\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:1.24395751953125 acc:0.5\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:1.2102129459381104 acc:0.4699999690055847\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:1.2063488960266113 acc:0.49000000953674316\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:1.1999733448028564 acc:0.5\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:1.1877613067626953 acc:0.4999999701976776\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:1.172470211982727 acc:0.5\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:1.1685144901275635 acc:0.5\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:1.1838144063949585 acc:0.5299999713897705\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:1.1747350692749023 acc:0.5199999809265137\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:1.1707592010498047 acc:0.5199999809265137\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:1.1235557794570923 acc:0.4899999797344208\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:1.1145896911621094 acc:0.5199999809265137\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:1.1263694763183594 acc:0.5\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:1.089379906654358 acc:0.5199999809265137\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:1.109941840171814 acc:0.5099999904632568\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:1.0724213123321533 acc:0.5199999809265137\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:1.0782742500305176 acc:0.5299999713897705\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:1.0422471761703491 acc:0.5099999904632568\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:1.0623009204864502 acc:0.5299999713897705\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:1.018873691558838 acc:0.5199999809265137\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:1.0255917310714722 acc:0.5199999809265137\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:1.0081064701080322 acc:0.5099999904632568\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:1.0188047885894775 acc:0.5099999904632568\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.9894553422927856 acc:0.5199999809265137\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.9862833023071289 acc:0.5299999713897705\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.9816049337387085 acc:0.5499999523162842\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.9680342078208923 acc:0.5099999904632568\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.9772384166717529 acc:0.5399999618530273\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.9478083252906799 acc:0.5399999618530273\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.9379963874816895 acc:0.5299999713897705\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.9418673515319824 acc:0.5299999713897705\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.9218600988388062 acc:0.5499999523162842\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.9260720014572144 acc:0.5499999523162842\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.8841512203216553 acc:0.5299999713897705\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.8826183080673218 acc:0.5600000023841858\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.8977311849594116 acc:0.559999942779541\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.8759006261825562 acc:0.5499999523162842\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.8706715106964111 acc:0.5399999618530273\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.8702058792114258 acc:0.5399999618530273\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.8732636570930481 acc:0.559999942779541\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.8539000153541565 acc:0.5499999523162842\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.8402267694473267 acc:0.559999942779541\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.8208011984825134 acc:0.559999942779541\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.8167842626571655 acc:0.5499999523162842\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.8411597013473511 acc:0.5499999523162842\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.8067472577095032 acc:0.5499999523162842\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.799101710319519 acc:0.559999942779541\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.7948064804077148 acc:0.559999942779541\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.7828758955001831 acc:0.559999942779541\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.781455397605896 acc:0.5499999523162842\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.7847819328308105 acc:0.5699999928474426\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.7714752554893494 acc:0.559999942779541\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.7487862706184387 acc:0.559999942779541\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.7533199191093445 acc:0.5699999928474426\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.7300512790679932 acc:0.559999942779541\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.7322989106178284 acc:0.5699999928474426\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.713192343711853 acc:0.5699999332427979\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.7398383617401123 acc:0.5499999523162842\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.716999888420105 acc:0.5499999523162842\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.7142623662948608 acc:0.559999942779541\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.6781991124153137 acc:0.5499999523162842\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.6929152011871338 acc:0.5699999332427979\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.6887935996055603 acc:0.5699999332427979\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.6676136255264282 acc:0.559999942779541\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.6663762927055359 acc:0.5699999332427979\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.6522533893585205 acc:0.5699999928474426\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.6453535556793213 acc:0.559999942779541\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.6245192289352417 acc:0.5699999332427979\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.6089975237846375 acc:0.5799999237060547\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.6407404541969299 acc:0.559999942779541\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.6269742846488953 acc:0.5699999928474426\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.5952534675598145 acc:0.5699999332427979\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.6073219180107117 acc:0.5799999237060547\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.6014959216117859 acc:0.559999942779541\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.5720590353012085 acc:0.5799999833106995\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.571104884147644 acc:0.5699999332427979\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.569527268409729 acc:0.5499999523162842\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.5602591037750244 acc:0.5699999928474426\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.5583339333534241 acc:0.5799999833106995\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.5492402911186218 acc:0.5899999737739563\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.5292956233024597 acc:0.5600000023841858\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.5513774752616882 acc:0.5699999332427979\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.5341859459877014 acc:0.5799999833106995\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.523773193359375 acc:0.5799999833106995\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.5220803618431091 acc:0.5799999833106995\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.5061655044555664 acc:0.5899999737739563\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.5156896710395813 acc:0.5799999833106995\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.513427197933197 acc:0.5699999928474426\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.5038945078849792 acc:0.5799999833106995\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.4971572756767273 acc:0.5799999833106995\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.4769871234893799 acc:0.5699999332427979\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.4901272654533386 acc:0.5799999833106995\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.467763215303421 acc:0.5799999833106995\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.46753671765327454 acc:0.5899999737739563\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.456315279006958 acc:0.559999942779541\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.4537339210510254 acc:0.5899999737739563\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.4445810914039612 acc:0.5699999332427979\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.4412667155265808 acc:0.5900000333786011\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.43315380811691284 acc:0.5699999332427979\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.440033882856369 acc:0.5799999833106995\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.4326649308204651 acc:0.559999942779541\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.4125477373600006 acc:0.5799999833106995\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.42307567596435547 acc:0.5699999928474426\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.42345893383026123 acc:0.5799999833106995\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.4152780771255493 acc:0.5799999833106995\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.41596436500549316 acc:0.5799999833106995\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.39263370633125305 acc:0.5799999833106995\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.38720160722732544 acc:0.5799999833106995\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.3762775957584381 acc:0.5799999833106995\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.3673688769340515 acc:0.5799999833106995\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.3661074638366699 acc:0.5899999737739563\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.38402223587036133 acc:0.6000000238418579\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.35934603214263916 acc:0.5699999928474426\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.3509942293167114 acc:0.5699999928474426\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.3616080582141876 acc:0.6000000238418579\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.3560872972011566 acc:0.5899999737739563\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.3428196907043457 acc:0.5699999332427979\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.33721426129341125 acc:0.5899999737739563\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.33541566133499146 acc:0.5699999332427979\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.3358314633369446 acc:0.5799999833106995\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.33075159788131714 acc:0.5799999833106995\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.3111051321029663 acc:0.60999995470047\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.3164792060852051 acc:0.6000000238418579\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.3325885534286499 acc:0.6000000238418579\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.3051692843437195 acc:0.6000000238418579\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.3211073577404022 acc:0.6100000143051147\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.3063667416572571 acc:0.6100000143051147\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.2972831726074219 acc:0.6000000238418579\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.28858035802841187 acc:0.5899999737739563\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.28250330686569214 acc:0.5799999833106995\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.2955175042152405 acc:0.6000000238418579\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.3010636270046234 acc:0.559999942779541\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.2787851095199585 acc:0.5799999833106995\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.2853901982307434 acc:0.6100000143051147\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.26399117708206177 acc:0.6200000047683716\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.2643016576766968 acc:0.5900000333786011\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.2653670907020569 acc:0.6299999952316284\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.25940006971359253 acc:0.6000000238418579\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.2673265039920807 acc:0.5899999737739563\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.254977285861969 acc:0.6200000047683716\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.25221091508865356 acc:0.5799999833106995\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.243303120136261 acc:0.6100000143051147\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.24435172975063324 acc:0.6100000143051147\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.23898640275001526 acc:0.5899999737739563\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.238683819770813 acc:0.60999995470047\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.2298153042793274 acc:0.5999999642372131\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.22648465633392334 acc:0.5999999642372131\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.23398935794830322 acc:0.6100000143051147\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.21895140409469604 acc:0.5999999642372131\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.23142342269420624 acc:0.5999999642372131\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.21183758974075317 acc:0.6200000047683716\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.20702707767486572 acc:0.5999999642372131\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.20764003694057465 acc:0.6100000143051147\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.2116093933582306 acc:0.6000000238418579\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.20112718641757965 acc:0.60999995470047\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.2041003257036209 acc:0.6200000047683716\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.18760919570922852 acc:0.6200000047683716\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.1708292961120605 acc:0.2799999713897705\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:2.0099284648895264 acc:0.23999999463558197\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:1.6664421558380127 acc:0.3399999737739563\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:1.7069522142410278 acc:0.3799999952316284\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:1.7629681825637817 acc:0.4000000059604645\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:1.9113197326660156 acc:0.38999998569488525\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:1.6629520654678345 acc:0.37999996542930603\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:1.4263439178466797 acc:0.4099999964237213\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:1.5476493835449219 acc:0.4399999678134918\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:1.6580005884170532 acc:0.4599999487400055\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.8066420555114746 acc:0.429999977350235\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:1.558178424835205 acc:0.41999998688697815\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:1.2979373931884766 acc:0.4299999475479126\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:1.4633615016937256 acc:0.4399999976158142\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:1.5548368692398071 acc:0.4699999988079071\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.7270584106445312 acc:0.4099999666213989\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:1.4703787565231323 acc:0.4699999690055847\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:1.2017334699630737 acc:0.42000001668930054\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:1.4193544387817383 acc:0.44999998807907104\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:1.488037347793579 acc:0.44999995827674866\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.653140902519226 acc:0.44999998807907104\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:1.417041540145874 acc:0.4599999785423279\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:1.147085189819336 acc:0.4399999976158142\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:1.3428021669387817 acc:0.47999998927116394\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:1.4231072664260864 acc:0.47999998927116394\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.5712722539901733 acc:0.47999998927116394\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:1.4164869785308838 acc:0.4699999988079071\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:1.0975033044815063 acc:0.4899999797344208\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:1.3020951747894287 acc:0.4999999701976776\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:1.3747482299804688 acc:0.5\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.524935245513916 acc:0.5199999809265137\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:1.3341103792190552 acc:0.5099999904632568\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:1.0377854108810425 acc:0.47999998927116394\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:1.245474934577942 acc:0.5199999809265137\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:1.3494231700897217 acc:0.5199999809265137\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.4794074296951294 acc:0.5\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:1.2885886430740356 acc:0.4899999797344208\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:1.0201865434646606 acc:0.5099999904632568\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:1.234616994857788 acc:0.5199999809265137\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:1.3037991523742676 acc:0.5099999904632568\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.4563723802566528 acc:0.4999999701976776\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:1.2637667655944824 acc:0.5099999904632568\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:0.9727311134338379 acc:0.5299999713897705\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:1.1665480136871338 acc:0.5099999904632568\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:1.2780907154083252 acc:0.5299999713897705\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.4482675790786743 acc:0.5199999809265137\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:1.237521767616272 acc:0.5199999809265137\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:0.9483323097229004 acc:0.5299999713897705\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:1.1391408443450928 acc:0.5\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:1.2500109672546387 acc:0.5299999713897705\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.3918380737304688 acc:0.5199999809265137\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:1.2164431810379028 acc:0.5399999618530273\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:0.9250256419181824 acc:0.4999999701976776\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:1.114314317703247 acc:0.5299999713897705\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:1.228064775466919 acc:0.5499999523162842\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.3741869926452637 acc:0.5299999713897705\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:1.1567332744598389 acc:0.5199999809265137\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:0.9120087027549744 acc:0.5299999713897705\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:1.089491367340088 acc:0.550000011920929\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:1.1914143562316895 acc:0.5499999523162842\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.3197047710418701 acc:0.5499999523162842\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:1.1488337516784668 acc:0.5600000023841858\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:0.8849136829376221 acc:0.5199999809265137\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:1.063349962234497 acc:0.5499999523162842\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:1.161409854888916 acc:0.5499999523162842\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.3106980323791504 acc:0.5499999523162842\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:1.1336861848831177 acc:0.5799999833106995\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:0.8623526692390442 acc:0.5399999618530273\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:1.0318548679351807 acc:0.5600000023841858\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:1.145381212234497 acc:0.5399999618530273\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.2846486568450928 acc:0.5499999523162842\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:1.0889121294021606 acc:0.5699999332427979\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:0.8473203778266907 acc:0.5299999713897705\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:1.0129142999649048 acc:0.5699999928474426\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:1.1160297393798828 acc:0.5799999833106995\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.2640376091003418 acc:0.6100000143051147\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:1.0518810749053955 acc:0.559999942779541\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:0.8350299000740051 acc:0.559999942779541\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:0.9912171363830566 acc:0.5699999928474426\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:1.0996594429016113 acc:0.5699999928474426\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.218950867652893 acc:0.5699999928474426\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:1.0380593538284302 acc:0.5699999928474426\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:0.8111846446990967 acc:0.559999942779541\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:0.9646368026733398 acc:0.5900000333786011\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:1.0696712732315063 acc:0.5899999737739563\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.1756236553192139 acc:0.5600000023841858\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:1.0290619134902954 acc:0.5899999737739563\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:0.8075051307678223 acc:0.5600000023841858\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:0.9362319707870483 acc:0.5999999642372131\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss:1.0526618957519531 acc:0.5799999833106995\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.1938750743865967 acc:0.5799999833106995\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:0.9975972771644592 acc:0.5799999833106995\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:0.7863718867301941 acc:0.559999942779541\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:0.9234654903411865 acc:0.6100000143051147\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:1.0286939144134521 acc:0.60999995470047\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.1807900667190552 acc:0.5799999833106995\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:0.9745083451271057 acc:0.5799999833106995\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:0.7614242434501648 acc:0.5600000023841858\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:0.9141699075698853 acc:0.5699999928474426\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:1.0262526273727417 acc:0.5999999642372131\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.1187188625335693 acc:0.6200000047683716\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:0.9733350276947021 acc:0.5799999833106995\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:0.7521620988845825 acc:0.5799999833106995\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:0.8896501064300537 acc:0.5999999642372131\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:0.9883863925933838 acc:0.559999942779541\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.1391459703445435 acc:0.6100000143051147\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:0.911987841129303 acc:0.5899999737739563\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:0.7488827705383301 acc:0.5799999833106995\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:0.8764632940292358 acc:0.5999999642372131\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:0.9635086059570312 acc:0.60999995470047\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.101069450378418 acc:0.6200000047683716\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:0.9255877137184143 acc:0.6299999952316284\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:0.74773108959198 acc:0.6100000143051147\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:0.8587300777435303 acc:0.6699999570846558\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:0.9659422636032104 acc:0.60999995470047\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.0878360271453857 acc:0.6200000047683716\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:0.8912309408187866 acc:0.5899999737739563\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:0.7191386818885803 acc:0.5899999737739563\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:0.8514946699142456 acc:0.6100000143051147\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:0.9235169291496277 acc:0.6100000143051147\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.0634419918060303 acc:0.6399999856948853\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:0.8471264839172363 acc:0.5999999642372131\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:0.7161272764205933 acc:0.5799999833106995\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:0.8244855999946594 acc:0.6200000047683716\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:0.9210031032562256 acc:0.6499999761581421\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.0031871795654297 acc:0.6100000143051147\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:0.856310248374939 acc:0.6299999952316284\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:0.7065951228141785 acc:0.6000000238418579\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:0.8077526688575745 acc:0.6100000143051147\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:0.9108865261077881 acc:0.6399999856948853\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.0312910079956055 acc:0.6399999856948853\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:0.8518292903900146 acc:0.6599999666213989\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:0.7107294797897339 acc:0.6299999952316284\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:0.8136253356933594 acc:0.6299999952316284\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:0.8689373135566711 acc:0.5999999642372131\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.0083081722259521 acc:0.6499999761581421\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:0.8133012652397156 acc:0.6399999856948853\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:0.678180456161499 acc:0.6200000047683716\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:0.7849476337432861 acc:0.6299999952316284\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:0.8553817272186279 acc:0.6800000071525574\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.003709316253662 acc:0.6699999570846558\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:0.7870516777038574 acc:0.6699999570846558\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:0.678677499294281 acc:0.6000000238418579\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:0.7867237329483032 acc:0.6499999761581421\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:0.8335443139076233 acc:0.6399999856948853\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:0.9671190977096558 acc:0.6700000166893005\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:0.7815649509429932 acc:0.6699999570846558\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:0.6693450212478638 acc:0.6200000047683716\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:0.781329333782196 acc:0.6599999666213989\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:0.8142659664154053 acc:0.6499999761581421\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:0.9475544691085815 acc:0.6699999570846558\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:0.7896336317062378 acc:0.6799999475479126\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:0.6674594879150391 acc:0.6499999761581421\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:0.7577852010726929 acc:0.6399999856948853\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:0.8070555329322815 acc:0.6599999666213989\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:0.8990645408630371 acc:0.6799999475479126\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:0.7726093530654907 acc:0.6499999761581421\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:0.6509012579917908 acc:0.6399999260902405\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:0.7676043510437012 acc:0.6599999666213989\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:0.7798152565956116 acc:0.6599999666213989\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:0.9339787364006042 acc:0.6699999570846558\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:0.7441602945327759 acc:0.6599999666213989\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:0.6367080211639404 acc:0.6199999451637268\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:0.7347401976585388 acc:0.6699999570846558\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:0.8042197227478027 acc:0.6599999666213989\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:0.8985778093338013 acc:0.6800000071525574\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:0.7274118065834045 acc:0.6799999475479126\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:0.6305018663406372 acc:0.6399999260902405\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:0.7261254787445068 acc:0.6599999666213989\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:0.7532327175140381 acc:0.6699999570846558\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:0.8834059238433838 acc:0.6799999475479126\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:0.7458051443099976 acc:0.6799999475479126\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:0.6214748620986938 acc:0.6299999356269836\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:0.737531304359436 acc:0.6699999570846558\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:0.7353543639183044 acc:0.6599999666213989\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:0.8360191583633423 acc:0.6399999856948853\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:0.7277827262878418 acc:0.6599999666213989\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:0.597977340221405 acc:0.6399999856948853\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss:0.7217657566070557 acc:0.6599999666213989\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:0.7139067053794861 acc:0.6699999570846558\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:0.8744051456451416 acc:0.6699999570846558\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:0.6835997104644775 acc:0.6599999666213989\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:0.5868703722953796 acc:0.60999995470047\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:0.6870037317276001 acc:0.6599999666213989\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:0.6914802193641663 acc:0.6699999570846558\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:0.8469947576522827 acc:0.6499999761581421\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:0.6748048067092896 acc:0.6599999666213989\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:0.5950748920440674 acc:0.6299999356269836\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:0.6983442306518555 acc:0.6899999380111694\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:0.6796519160270691 acc:0.6699999570846558\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:0.8330149054527283 acc:0.6899999380111694\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:0.6583665609359741 acc:0.6799999475479126\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:0.5827100276947021 acc:0.6299999356269836\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:0.6823521852493286 acc:0.6699999570846558\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:0.6608949899673462 acc:0.6699999570846558\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:0.790131688117981 acc:0.6799999475479126\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:0.6610124707221985 acc:0.6599999666213989\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:0.5641892552375793 acc:0.6299999356269836\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:0.663378894329071 acc:0.6599999666213989\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:0.6717292070388794 acc:0.6599999666213989\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:0.7979004383087158 acc:0.6699999570846558\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:0.682964563369751 acc:0.6599999666213989\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:0.5481733679771423 acc:0.6499999761581421\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:0.6582075357437134 acc:0.6599999666213989\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:0.6401592493057251 acc:0.6799999475479126\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:0.784072995185852 acc:0.6499999761581421\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:0.6384370923042297 acc:0.6599999666213989\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:0.5614229440689087 acc:0.6199999451637268\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:0.6554282307624817 acc:0.6599999666213989\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:0.6331525444984436 acc:0.6699999570846558\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:0.7666851878166199 acc:0.6599999666213989\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:0.6141058206558228 acc:0.6599999666213989\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:0.5502539873123169 acc:0.6299999356269836\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:0.6468250751495361 acc:0.6799999475479126\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:0.611164927482605 acc:0.6699999570846558\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:0.755680501461029 acc:0.6599999666213989\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:0.5992889404296875 acc:0.6599999666213989\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:0.5515850782394409 acc:0.6399999856948853\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:0.628199577331543 acc:0.6599999666213989\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:0.6103394627571106 acc:0.6699999570846558\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:0.7490752935409546 acc:0.6499999761581421\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:0.5952949523925781 acc:0.6599999666213989\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:0.5142942070960999 acc:0.6799999475479126\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:0.5996521711349487 acc:0.6699999570846558\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:0.6190351247787476 acc:0.6799999475479126\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:0.7265501618385315 acc:0.6799999475479126\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:0.5824253559112549 acc:0.6999999284744263\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:0.5096853375434875 acc:0.6699999570846558\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:0.6056722402572632 acc:0.6599999666213989\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:0.6009984016418457 acc:0.6799999475479126\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:0.7194079160690308 acc:0.6899999380111694\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:0.5669487714767456 acc:0.6699999570846558\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:0.511565625667572 acc:0.6199999451637268\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:0.6041902303695679 acc:0.6899999380111694\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:0.5565366744995117 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:0.7213798761367798 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:0.5597543120384216 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:0.48999083042144775 acc:0.6299999356269836\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:0.5861020088195801 acc:0.6799999475479126\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:0.5623511075973511 acc:0.6799999475479126\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:0.7249499559402466 acc:0.6899999380111694\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:0.5573254823684692 acc:0.6799999475479126\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:0.48633357882499695 acc:0.6199999451637268\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:0.5842034816741943 acc:0.6699999570846558\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:0.5535463094711304 acc:0.6699999570846558\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:0.6856422424316406 acc:0.6899999380111694\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:0.5478647351264954 acc:0.6799999475479126\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:0.4742403030395508 acc:0.6499999761581421\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:0.5568036437034607 acc:0.6699999570846558\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:0.533045768737793 acc:0.6599999666213989\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss:0.7030910849571228 acc:0.6899999380111694\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss:0.5236258506774902 acc:0.6599999666213989\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss:0.48889845609664917 acc:0.6299999356269836\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss:0.5685216188430786 acc:0.6999999284744263\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss:0.5358905792236328 acc:0.7099999189376831\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss:0.6837613582611084 acc:0.699999988079071\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss:0.5192464590072632 acc:0.6999999284744263\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss:0.45378994941711426 acc:0.6699999570846558\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss:0.5577137470245361 acc:0.6699999570846558\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss:0.5313901901245117 acc:0.6799999475479126\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss:0.6563805341720581 acc:0.6899999380111694\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss:0.5280413627624512 acc:0.6999999284744263\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss:0.46673837304115295 acc:0.6399999260902405\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss:0.5445302128791809 acc:0.7099999189376831\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss:0.5077921152114868 acc:0.6699999570846558\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss:0.6734113693237305 acc:0.6599999666213989\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss:0.497348427772522 acc:0.6799999475479126\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss:0.44420862197875977 acc:0.6399999856948853\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss:0.5512257218360901 acc:0.6799999475479126\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss:0.49587589502334595 acc:0.6799999475479126\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss:0.6436517834663391 acc:0.6899999380111694\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss:0.4852718114852905 acc:0.6899999380111694\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss:0.43101054430007935 acc:0.6399999856948853\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss:0.508692741394043 acc:0.6799999475479126\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss:0.5015760660171509 acc:0.6699999570846558\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss:0.6210469007492065 acc:0.6799999475479126\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss:0.4775402247905731 acc:0.6799999475479126\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss:0.4142834544181824 acc:0.6399999260902405\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss:0.5162302255630493 acc:0.6499999761581421\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss:0.4878849387168884 acc:0.6899999380111694\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss:0.6244576573371887 acc:0.6999999284744263\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss:0.459404855966568 acc:0.6999999284744263\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss:0.4186971187591553 acc:0.6299999356269836\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss:0.5086444616317749 acc:0.6899999380111694\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss:0.4790692925453186 acc:0.6799999475479126\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss:0.6328350305557251 acc:0.6799999475479126\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss:0.4442441463470459 acc:0.7099999189376831\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss:0.40989774465560913 acc:0.6499999761581421\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss:0.5144612193107605 acc:0.6899999976158142\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss:0.4660112261772156 acc:0.6699999570846558\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss:0.6214010119438171 acc:0.6699999570846558\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss:0.444019079208374 acc:0.6899999976158142\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss:0.40599513053894043 acc:0.6399999260902405\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss:0.48986274003982544 acc:0.6699999570846558\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss:0.47441476583480835 acc:0.6499999761581421\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss:0.5971737504005432 acc:0.699999988079071\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss:0.46146005392074585 acc:0.7099999189376831\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss:0.37839066982269287 acc:0.6299999356269836\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss:0.49022334814071655 acc:0.699999988079071\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss:0.44523322582244873 acc:0.6699999570846558\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss:0.583473801612854 acc:0.699999988079071\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss:0.42942386865615845 acc:0.7099999189376831\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss:0.38271334767341614 acc:0.6399999260902405\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss:0.48201531171798706 acc:0.7099999785423279\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss:0.45076456665992737 acc:0.6899999976158142\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss:0.5871127247810364 acc:0.7299999594688416\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss:0.4303867220878601 acc:0.7099999189376831\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss:0.39601075649261475 acc:0.6399999856948853\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss:0.4573136568069458 acc:0.7099999785423279\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss:0.4343123137950897 acc:0.6800000071525574\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss:0.5986241102218628 acc:0.6999999284744263\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss:0.40711918473243713 acc:0.6899999380111694\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss:0.37987828254699707 acc:0.6499999761581421\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss:0.4726606011390686 acc:0.7199999690055847\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss:0.4266197085380554 acc:0.6799999475479126\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss:0.5448465347290039 acc:0.7300000190734863\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss:0.4331328868865967 acc:0.7099999189376831\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss:0.36808666586875916 acc:0.6199999451637268\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss:0.445039302110672 acc:0.699999988079071\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss:0.4184202551841736 acc:0.6899999380111694\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss:0.564314603805542 acc:0.7199999690055847\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss:0.398922324180603 acc:0.7199999094009399\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss:0.36253875494003296 acc:0.6299999356269836\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss:0.42628324031829834 acc:0.6999999284744263\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss:0.423284649848938 acc:0.6899999380111694\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss:0.5338447690010071 acc:0.7099999785423279\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss:0.3971191942691803 acc:0.6999999284744263\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss:0.35305407643318176 acc:0.6399999856948853\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss:0.4387683868408203 acc:0.7199999690055847\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss:0.42410191893577576 acc:0.6899999380111694\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss:0.5486643314361572 acc:0.699999988079071\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss:0.397849977016449 acc:0.7199999094009399\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss:0.3538268506526947 acc:0.6499999761581421\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss:0.4484398663043976 acc:0.6899999976158142\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss:0.40590187907218933 acc:0.6999999284744263\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss:0.5176296830177307 acc:0.6999999284744263\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss:0.3819868266582489 acc:0.7099999785423279\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss:0.3373487591743469 acc:0.6499999761581421\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss:0.41965997219085693 acc:0.6899999380111694\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss:0.4113743305206299 acc:0.6799999475479126\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss:0.49312540888786316 acc:0.699999988079071\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss:0.36504465341567993 acc:0.6899999380111694\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss:0.3391013741493225 acc:0.6699999570846558\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss:0.41155505180358887 acc:0.7199999690055847\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss:0.3853800594806671 acc:0.6699999570846558\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss:0.5440783500671387 acc:0.7199999690055847\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss:0.3702712059020996 acc:0.7099999189376831\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss:0.32078129053115845 acc:0.6599999666213989\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss:0.41627639532089233 acc:0.699999988079071\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss:0.3913041949272156 acc:0.6899999380111694\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss:0.5173091292381287 acc:0.6899999380111694\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss:0.36340638995170593 acc:0.6899999380111694\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss:0.3311602771282196 acc:0.6399999856948853\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss:0.4005489945411682 acc:0.6899999380111694\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss:0.3894721269607544 acc:0.6899999380111694\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss:0.5283094644546509 acc:0.6799999475479126\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss:0.34651249647140503 acc:0.699999988079071\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss:0.3222275972366333 acc:0.6399999260902405\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss:0.417987585067749 acc:0.6999999284744263\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss:0.3656872808933258 acc:0.6599999666213989\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss:0.4996916353702545 acc:0.7299999594688416\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss:0.3556211590766907 acc:0.7199999690055847\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss:0.31718289852142334 acc:0.6199999451637268\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss:0.4036470949649811 acc:0.7099999785423279\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss:0.374504029750824 acc:0.6599999666213989\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss:0.4930073618888855 acc:0.7099999785423279\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss:0.3429269790649414 acc:0.7199999690055847\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss:0.31524044275283813 acc:0.6499999761581421\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss:0.3930400609970093 acc:0.7099999785423279\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss:0.34753867983818054 acc:0.6699999570846558\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss:0.46337005496025085 acc:0.7199999690055847\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss:0.32748132944107056 acc:0.7099999785423279\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss:0.3019963800907135 acc:0.6499999761581421\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss:0.38760894536972046 acc:0.6999999284744263\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss:0.3373742699623108 acc:0.6999999284744263\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss:0.45507052540779114 acc:0.7299999594688416\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss:0.32965442538261414 acc:0.7099999189376831\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss:0.2967482805252075 acc:0.6599999666213989\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss:0.3550281524658203 acc:0.699999988079071\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss:0.34167519211769104 acc:0.6799999475479126\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss:0.4613306224346161 acc:0.6999999284744263\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss:0.3175109624862671 acc:0.7199999690055847\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss:0.29616087675094604 acc:0.6799999475479126\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss:0.36343634128570557 acc:0.7199999690055847\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss:0.3427669107913971 acc:0.6799999475479126\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss:0.43017885088920593 acc:0.7299999594688416\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss:0.3094324767589569 acc:0.7299999594688416\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss:0.29096508026123047 acc:0.6799999475479126\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss:0.3537176251411438 acc:0.7499999403953552\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss:0.3237488865852356 acc:0.6799999475479126\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss:0.4243018627166748 acc:0.6999999284744263\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss:0.3021584749221802 acc:0.7199999690055847\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss:0.28094199299812317 acc:0.6699999570846558\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss:0.3659733533859253 acc:0.7099999189376831\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss:0.30285224318504333 acc:0.6899999380111694\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss:0.4497753977775574 acc:0.6799999475479126\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss:0.29801949858665466 acc:0.6899999380111694\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss:0.2785818576812744 acc:0.6699999570846558\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss:0.34747982025146484 acc:0.7399999499320984\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss:0.31500300765037537 acc:0.6899999380111694\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss:0.4101982116699219 acc:0.7199999690055847\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss:0.29586419463157654 acc:0.7199999690055847\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss:0.27851077914237976 acc:0.6799999475479126\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss:0.3597475290298462 acc:0.7199999690055847\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss:0.3015845715999603 acc:0.6999999284744263\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss:0.41913890838623047 acc:0.7199999690055847\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss:0.2914985418319702 acc:0.6999999284744263\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss:0.27161991596221924 acc:0.6799999475479126\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss:0.3211987018585205 acc:0.7199999690055847\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss:0.30816012620925903 acc:0.6799999475479126\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss:0.4026796221733093 acc:0.7299999594688416\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss:0.2761334180831909 acc:0.7099999785423279\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss:0.2677074670791626 acc:0.6899999380111694\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss:0.3200345039367676 acc:0.7099999189376831\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss:0.3030617833137512 acc:0.6799999475479126\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss:0.4026973247528076 acc:0.7299999594688416\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss:0.2730640470981598 acc:0.7300000190734863\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss:0.2714572846889496 acc:0.6699999570846558\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss:0.31734636425971985 acc:0.7099999785423279\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss:0.2817152440547943 acc:0.6699999570846558\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss:0.3955238461494446 acc:0.7399999499320984\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss:0.27647867798805237 acc:0.7199999690055847\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss:0.2617924213409424 acc:0.6499999761581421\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss:0.302493155002594 acc:0.7199999690055847\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss:0.27529269456863403 acc:0.6699999570846558\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss:0.37906038761138916 acc:0.7099999785423279\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss:0.2648067772388458 acc:0.7199999690055847\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss:0.24501749873161316 acc:0.6699999570846558\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss:0.3033011257648468 acc:0.7099999785423279\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss:0.2755623459815979 acc:0.6899999380111694\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss:0.39536380767822266 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss:0.2629793584346771 acc:0.7199999690055847\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss:0.25483348965644836 acc:0.6699999570846558\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss:0.29900020360946655 acc:0.7099999785423279\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss:0.2733001112937927 acc:0.6799999475479126\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss:0.3352435231208801 acc:0.6999999284744263\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss:0.24114039540290833 acc:0.7099999189376831\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss:0.23335884511470795 acc:0.6599999666213989\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss:0.28465861082077026 acc:0.7299999594688416\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss:0.2672277092933655 acc:0.6999999284744263\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss:0.3500378429889679 acc:0.7299999594688416\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss:0.23919370770454407 acc:0.7099999785423279\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss:0.23606279492378235 acc:0.6899999380111694\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss:0.2737385332584381 acc:0.7399999499320984\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss:0.27021902799606323 acc:0.7199999094009399\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss:0.3576759994029999 acc:0.7199999690055847\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss:0.24546711146831512 acc:0.7099999189376831\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss:0.2317778468132019 acc:0.6399999856948853\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss:0.272549033164978 acc:0.699999988079071\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss:0.26161590218544006 acc:0.6999999284744263\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss:0.34252026677131653 acc:0.7099999189376831\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss:0.2473525106906891 acc:0.7299999594688416\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss:0.21990329027175903 acc:0.6699999570846558\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss:0.27871808409690857 acc:0.7199999690055847\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss:0.2504502534866333 acc:0.6699999570846558\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss:0.32815274596214294 acc:0.6899999380111694\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss:0.23685868084430695 acc:0.6999999284744263\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss:0.24141883850097656 acc:0.6699999570846558\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss:0.2856209874153137 acc:0.699999988079071\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss:0.24198433756828308 acc:0.6899999380111694\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss:0.3267725706100464 acc:0.7199999690055847\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss:0.2259792983531952 acc:0.699999988079071\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss:0.22529053688049316 acc:0.6499999761581421\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss:0.26724621653556824 acc:0.7099999785423279\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss:0.23460310697555542 acc:0.6899999380111694\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss:0.3477504551410675 acc:0.6899999380111694\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss:0.22997339069843292 acc:0.7199999690055847\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss:0.21188296377658844 acc:0.6699999570846558\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss:0.24697721004486084 acc:0.7099999189376831\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss:0.2386230230331421 acc:0.6799999475479126\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss:0.3305986821651459 acc:0.7099999785423279\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss:0.21102465689182281 acc:0.7299999594688416\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss:0.21567651629447937 acc:0.6799999475479126\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss:0.2517363429069519 acc:0.7399999499320984\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss:0.21572378277778625 acc:0.6799999475479126\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss:0.31065070629119873 acc:0.699999988079071\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss:0.2047072947025299 acc:0.6899999380111694\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss:0.2037241905927658 acc:0.6799999475479126\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss:0.24723510444164276 acc:0.7299999594688416\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss:0.22807878255844116 acc:0.6699999570846558\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss:0.30321571230888367 acc:0.7399999499320984\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss:0.2105119228363037 acc:0.7299999594688416\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss:0.21141815185546875 acc:0.6799999475479126\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss:0.24348556995391846 acc:0.6899999380111694\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss:0.23743540048599243 acc:0.6899999380111694\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss:0.29475659132003784 acc:0.7299999594688416\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss:0.1963111311197281 acc:0.7099999189376831\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss:0.20506703853607178 acc:0.6399999856948853\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss:0.23291447758674622 acc:0.7199999690055847\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss:0.2132842242717743 acc:0.6899999976158142\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss:0.29460519552230835 acc:0.7099999785423279\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss:0.19003960490226746 acc:0.75\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss:0.20174935460090637 acc:0.6699999570846558\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss:0.23271557688713074 acc:0.7099999189376831\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss:0.23475149273872375 acc:0.6799999475479126\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss:0.29339832067489624 acc:0.7199999690055847\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss:0.19215571880340576 acc:0.7299999594688416\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss:0.18954437971115112 acc:0.6399999856948853\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss:0.24181018769741058 acc:0.7099999189376831\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss:0.2217167168855667 acc:0.6799999475479126\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss:0.2664797604084015 acc:0.7399999499320984\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss:0.18888381123542786 acc:0.6999999284744263\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss:0.19278323650360107 acc:0.6899999380111694\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss:0.21489527821540833 acc:0.7299999594688416\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss:0.202491894364357 acc:0.6899999380111694\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss:0.33216553926467896 acc:0.7099999189376831\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss:0.17433443665504456 acc:0.7199999690055847\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss:0.1944461166858673 acc:0.6599999666213989\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss:0.22627924382686615 acc:0.7099999189376831\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss:0.20190446078777313 acc:0.6899999380111694\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss:0.2817578613758087 acc:0.7199999690055847\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss:0.1834283173084259 acc:0.7300000190734863\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss:0.18553170561790466 acc:0.6699999570846558\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss:0.23336482048034668 acc:0.7299999594688416\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss:0.21374282240867615 acc:0.6899999380111694\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss:0.2833021283149719 acc:0.7099999785423279\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss:0.17767073214054108 acc:0.7199999094009399\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss:0.17369943857192993 acc:0.6899999380111694\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss:0.21787846088409424 acc:0.6999999284744263\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss:0.20728056132793427 acc:0.6699999570846558\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss:0.2669992446899414 acc:0.7099999785423279\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss:0.17122489213943481 acc:0.6999999284744263\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss:0.16360971331596375 acc:0.6599999666213989\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss:0.20235061645507812 acc:0.7199999690055847\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss:0.19429433345794678 acc:0.6899999380111694\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss:0.256197065114975 acc:0.7299999594688416\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss:0.16823060810565948 acc:0.7199999690055847\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss:0.18044611811637878 acc:0.6799999475479126\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss:0.21390989422798157 acc:0.7299999594688416\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss:0.19989165663719177 acc:0.6899999380111694\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss:0.28229784965515137 acc:0.6999999284744263\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss:0.1596531867980957 acc:0.75\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss:0.16276110708713531 acc:0.6599999666213989\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss:0.21086788177490234 acc:0.7199999690055847\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss:0.1945314109325409 acc:0.699999988079071\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss:0.2556895911693573 acc:0.699999988079071\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss:0.15986767411231995 acc:0.6999999284744263\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss:0.1594220995903015 acc:0.6899999380111694\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss:0.20877134799957275 acc:0.7099999785423279\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss:0.18815018236637115 acc:0.7099999189376831\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss:0.2507576048374176 acc:0.7199999690055847\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss:0.16799059510231018 acc:0.7299999594688416\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss:0.14650702476501465 acc:0.6899999380111694\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss:0.21988752484321594 acc:0.6999999284744263\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss:0.19462254643440247 acc:0.699999988079071\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss:0.22986477613449097 acc:0.7199999690055847\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss:0.16424588859081268 acc:0.7200000286102295\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss:0.150702565908432 acc:0.6799999475479126\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss:0.19533732533454895 acc:0.7099999189376831\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss:0.1784164160490036 acc:0.6799999475479126\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss:0.22704601287841797 acc:0.7199999094009399\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss:0.17255859076976776 acc:0.7099999189376831\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss:0.15902701020240784 acc:0.6899999380111694\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss:0.1964329481124878 acc:0.6999999284744263\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss:0.1709258258342743 acc:0.6999999284744263\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss:0.23500844836235046 acc:0.7199999690055847\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss:0.14799247682094574 acc:0.7299999594688416\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss:0.14671778678894043 acc:0.6699999570846558\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss:0.19478139281272888 acc:0.7199999094009399\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss:0.16875752806663513 acc:0.6799999475479126\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss:0.23775126039981842 acc:0.7299999594688416\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss:0.14215470850467682 acc:0.7299999594688416\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss:0.14269188046455383 acc:0.6799999475479126\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss:0.18707837164402008 acc:0.6999999284744263\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss:0.16467013955116272 acc:0.6799999475479126\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss:0.2276923656463623 acc:0.7399999499320984\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss:0.14267075061798096 acc:0.7299999594688416\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss:0.13697931170463562 acc:0.7099999785423279\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss:0.17869284749031067 acc:0.6899999380111694\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss:0.17846259474754333 acc:0.699999988079071\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss:0.2163580358028412 acc:0.7299999594688416\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss:0.13795514404773712 acc:0.7399999499320984\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss:0.1410350799560547 acc:0.6999999284744263\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss:0.18429940938949585 acc:0.7199999690055847\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss:0.15669777989387512 acc:0.6999999284744263\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss:0.22153189778327942 acc:0.7399999499320984\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss:0.13004618883132935 acc:0.7099999785423279\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss:0.14017462730407715 acc:0.6999999284744263\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss:0.1784292459487915 acc:0.7299999594688416\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss:0.15025314688682556 acc:0.6999999284744263\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss:0.20577767491340637 acc:0.7399999499320984\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss:0.1284538060426712 acc:0.7099999189376831\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss:0.1277032494544983 acc:0.6899999380111694\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss:0.1771581918001175 acc:0.6899999380111694\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss:0.1551927924156189 acc:0.7199999690055847\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss:0.21494725346565247 acc:0.7099999785423279\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss:0.11743128299713135 acc:0.7399999499320984\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss:0.13135100901126862 acc:0.6899999380111694\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss:0.16909649968147278 acc:0.7299999594688416\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss:0.15835635364055634 acc:0.6699999570846558\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss:0.2096317559480667 acc:0.6799999475479126\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss:0.1339649260044098 acc:0.7099999785423279\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss:0.13004201650619507 acc:0.6899999380111694\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss:0.1773817539215088 acc:0.6999999284744263\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss:0.15128660202026367 acc:0.6899999380111694\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss:0.19155928492546082 acc:0.7299999594688416\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss:0.11889883130788803 acc:0.7099999785423279\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss:0.12421654164791107 acc:0.6899999380111694\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss:0.17205910384655 acc:0.7299999594688416\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss:0.13432671129703522 acc:0.7099999189376831\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss:0.18536335229873657 acc:0.7499999403953552\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss:0.12102946639060974 acc:0.7199999094009399\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss:0.12548109889030457 acc:0.6699999570846558\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss:0.1541852504014969 acc:0.7299999594688416\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss:0.14369302988052368 acc:0.6799999475479126\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss:0.17887499928474426 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss:0.12514998018741608 acc:0.7099999785423279\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss:0.11432111263275146 acc:0.6999999284744263\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss:0.15298737585544586 acc:0.7199999690055847\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss:0.14270050823688507 acc:0.6899999380111694\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss:0.19161748886108398 acc:0.7399999499320984\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss:0.13569077849388123 acc:0.7099999785423279\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss:0.12043551355600357 acc:0.6799999475479126\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss:0.150882288813591 acc:0.7199999094009399\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss:0.12571415305137634 acc:0.6999999284744263\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss:0.18692681193351746 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss:0.13213451206684113 acc:0.7199999094009399\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss:0.11213064193725586 acc:0.7199999690055847\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss:0.12908302247524261 acc:0.7299999594688416\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss:0.12720102071762085 acc:0.6599999666213989\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss:0.17243707180023193 acc:0.7299999594688416\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss:0.11608169972896576 acc:0.7399999499320984\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss:0.11038123071193695 acc:0.6999999284744263\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss:0.1495172530412674 acc:0.7299999594688416\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss:0.12019667029380798 acc:0.6899999380111694\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss:0.17504830658435822 acc:0.7299999594688416\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss:0.11516910046339035 acc:0.7099999785423279\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss:0.11304061859846115 acc:0.6799999475479126\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss:0.14712542295455933 acc:0.7199999094009399\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss:0.13185231387615204 acc:0.6799999475479126\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss:0.1708233803510666 acc:0.6999999284744263\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss:0.10982832312583923 acc:0.699999988079071\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss:0.10496748983860016 acc:0.6699999570846558\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss:0.1234869435429573 acc:0.7099999189376831\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss:0.11788227409124374 acc:0.6999999284744263\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss:0.16315284371376038 acc:0.7099999189376831\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss:0.10534244030714035 acc:0.6999999284744263\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss:0.10649845749139786 acc:0.6899999380111694\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss:0.1354874074459076 acc:0.7199999094009399\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss:0.10785476863384247 acc:0.7099999189376831\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss:0.17341917753219604 acc:0.7199999690055847\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss:0.10222140699625015 acc:0.7199999690055847\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss:0.11546632647514343 acc:0.699999988079071\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss:0.12765441834926605 acc:0.7099999189376831\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss:0.10793730616569519 acc:0.7099999785423279\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss:0.16140782833099365 acc:0.7199999690055847\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss:0.0971195250749588 acc:0.7399999499320984\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss:0.10154324769973755 acc:0.7099999189376831\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss:0.13232184946537018 acc:0.6999999284744263\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss:0.10632938891649246 acc:0.7099999785423279\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss:0.13215425610542297 acc:0.7299999594688416\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss:0.09596648067235947 acc:0.7099999785423279\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss:0.09970922768115997 acc:0.6999999284744263\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss:0.12613677978515625 acc:0.6899999380111694\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss:0.11212584376335144 acc:0.6699999570846558\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss:0.14549659192562103 acc:0.7099999189376831\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss:0.09747909009456635 acc:0.7099999189376831\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss:0.10950253158807755 acc:0.699999988079071\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss:0.12776398658752441 acc:0.7099999189376831\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss:0.09300655871629715 acc:0.6899999380111694\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss:0.14363247156143188 acc:0.7099999189376831\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss:0.09586764872074127 acc:0.7399999499320984\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss:0.10075154155492783 acc:0.699999988079071\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss:0.11158555746078491 acc:0.7199999094009399\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss:0.0967123955488205 acc:0.7299999594688416\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss:0.15490297973155975 acc:0.7299999594688416\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss:0.09248033910989761 acc:0.7199999690055847\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss:0.09164036810398102 acc:0.7099999785423279\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss:0.11165809631347656 acc:0.7199999690055847\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss:0.0928041934967041 acc:0.6999999284744263\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss:0.1335354596376419 acc:0.7399999499320984\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss:0.08096040040254593 acc:0.6999999284744263\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss:0.09994977712631226 acc:0.6999999284744263\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss:0.12707489728927612 acc:0.7199999690055847\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss:0.09678031504154205 acc:0.6899999380111694\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss:0.13654835522174835 acc:0.6899999380111694\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss:0.08970846235752106 acc:0.7099999785423279\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss:0.09153062105178833 acc:0.7199999094009399\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss:0.11715488135814667 acc:0.7299999594688416\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss:0.09747582674026489 acc:0.7099999189376831\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss:0.13552381098270416 acc:0.7299998998641968\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss:0.09237954765558243 acc:0.7399999499320984\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss:0.08769229054450989 acc:0.7099999785423279\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss:0.10886435210704803 acc:0.7399999499320984\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss:0.09331558644771576 acc:0.6899999380111694\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss:0.1240776777267456 acc:0.7099999785423279\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss:0.08868447691202164 acc:0.7299999594688416\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss:0.07931056618690491 acc:0.7099999785423279\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss:0.10633056610822678 acc:0.7299999594688416\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss:0.09162355959415436 acc:0.7199999690055847\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss:0.13918206095695496 acc:0.7399999499320984\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss:0.08093120902776718 acc:0.75\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss:0.08599402755498886 acc:0.7399999499320984\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss:0.10707128047943115 acc:0.7399999499320984\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss:0.08251983672380447 acc:0.7400000095367432\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss:0.11968925595283508 acc:0.699999988079071\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss:0.07249445468187332 acc:0.7099999785423279\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss:0.08727949857711792 acc:0.7099999785423279\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss:0.10445883870124817 acc:0.7099999785423279\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss:0.08504003286361694 acc:0.699999988079071\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss:0.12643811106681824 acc:0.7199999690055847\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss:0.08668041229248047 acc:0.6899999976158142\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss:0.0752306655049324 acc:0.7299999594688416\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss:0.1208297610282898 acc:0.7099999189376831\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss:0.0839986577630043 acc:0.7099999785423279\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss:0.12236132472753525 acc:0.7199999690055847\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss:0.08186081051826477 acc:0.7199999690055847\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss:0.0776107907295227 acc:0.7099999189376831\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss:0.08441056311130524 acc:0.7199999094009399\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss:0.08334775269031525 acc:0.7199999690055847\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss:0.1109430119395256 acc:0.7299999594688416\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss:0.0720912516117096 acc:0.7299999594688416\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss:0.077720507979393 acc:0.7299999594688416\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss:0.09388653934001923 acc:0.7099999189376831\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss:0.07945211976766586 acc:0.6999999284744263\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss:0.10959815979003906 acc:0.7399999499320984\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss:0.07327383756637573 acc:0.699999988079071\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss:0.075822614133358 acc:0.7499999403953552\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss:0.091422900557518 acc:0.7099999189376831\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss:0.07262344658374786 acc:0.6899999380111694\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss:0.10646927356719971 acc:0.6899999380111694\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss:0.07805818319320679 acc:0.7199999690055847\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss:0.07644148916006088 acc:0.7199999094009399\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss:0.09335426241159439 acc:0.6899999380111694\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss:0.08558324724435806 acc:0.6999999284744263\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss:0.11491508781909943 acc:0.6899999380111694\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss:0.07014938443899155 acc:0.7099999785423279\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss:0.07546168565750122 acc:0.6799999475479126\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss:0.10422725975513458 acc:0.7299999594688416\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss:0.0793844610452652 acc:0.7099999785423279\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss:0.09429161995649338 acc:0.6799999475479126\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss:0.07391919940710068 acc:0.7199999690055847\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss:0.06889630854129791 acc:0.7199999690055847\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss:0.07816057652235031 acc:0.6899999380111694\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss:0.07428406178951263 acc:0.6999999284744263\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss:0.09790636599063873 acc:0.6799999475479126\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss:0.065974161028862 acc:0.7199999690055847\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss:0.08053062111139297 acc:0.7299999594688416\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss:0.08593598008155823 acc:0.7099999189376831\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss:0.07434804737567902 acc:0.7199999094009399\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss:0.11243180185556412 acc:0.7199999690055847\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss:0.06726894527673721 acc:0.7399999499320984\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss:0.0633678287267685 acc:0.6899999380111694\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss:0.08195770531892776 acc:0.7099999785423279\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss:0.06589911133050919 acc:0.7199999690055847\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss:0.09359106421470642 acc:0.6899999380111694\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss:0.060968924313783646 acc:0.7199999094009399\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss:0.06518878042697906 acc:0.7299999594688416\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss:0.0773613303899765 acc:0.7199999690055847\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss:0.06765823066234589 acc:0.6999999284744263\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss:0.10823739320039749 acc:0.6799999475479126\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss:0.06202239543199539 acc:0.699999988079071\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss:0.06355942785739899 acc:0.7199999690055847\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss:0.08355165272951126 acc:0.6999999284744263\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss:0.05952067673206329 acc:0.7099999189376831\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss:0.08862116932868958 acc:0.7099999189376831\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss:0.06657145172357559 acc:0.7399999499320984\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss:0.06042972952127457 acc:0.7399999499320984\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss:0.08531511574983597 acc:0.7199999094009399\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss:0.06474275887012482 acc:0.7099999189376831\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss:0.10228577256202698 acc:0.7099999785423279\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss:0.06267815828323364 acc:0.7599999904632568\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss:0.07233040779829025 acc:0.7099999785423279\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss:0.07802983373403549 acc:0.7299999594688416\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss:0.06328559666872025 acc:0.7199999690055847\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss:0.09211568534374237 acc:0.6899999380111694\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss:0.06118934601545334 acc:0.6899999976158142\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss:0.060848668217659 acc:0.7299999594688416\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss:0.08925914764404297 acc:0.7199999690055847\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss:0.05043501406908035 acc:0.7099999785423279\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss:0.0811193659901619 acc:0.7099999785423279\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss:0.06176243722438812 acc:0.7399999499320984\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss:0.06101206690073013 acc:0.7199999094009399\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss:0.08167561888694763 acc:0.7099999785423279\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss:0.05506521835923195 acc:0.7299999594688416\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss:0.09909841418266296 acc:0.7199999690055847\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss:0.05535759776830673 acc:0.7399999499320984\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss:0.05924433097243309 acc:0.7199999690055847\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss:0.07675322890281677 acc:0.7199999094009399\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss:0.06207672879099846 acc:0.7099999189376831\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss:0.07964877039194107 acc:0.6999999284744263\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss:0.05218441039323807 acc:0.7299999594688416\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss:0.054924383759498596 acc:0.7299999594688416\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss:0.07413125783205032 acc:0.7099999189376831\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss:0.06468544155359268 acc:0.7199999690055847\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss:0.07806672155857086 acc:0.7199999690055847\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss:0.05533495545387268 acc:0.7399999499320984\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss:0.0549800731241703 acc:0.7299999594688416\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss:0.07233606278896332 acc:0.7099999785423279\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss:0.0599580779671669 acc:0.7199999690055847\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss:0.07558733969926834 acc:0.7199999690055847\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss:0.056117769330739975 acc:0.7299999594688416\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss:0.058665160089731216 acc:0.7399999499320984\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss:0.07011722773313522 acc:0.6999999284744263\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss:0.05021645128726959 acc:0.7099999785423279\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss:0.07607113569974899 acc:0.699999988079071\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss:0.05156891793012619 acc:0.7199999690055847\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss:0.060071609914302826 acc:0.7299999594688416\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss:0.06233979016542435 acc:0.7099999785423279\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss:0.048445556312799454 acc:0.7099999785423279\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss:0.07423493266105652 acc:0.7199999690055847\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss:0.052093617618083954 acc:0.7399999499320984\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss:0.05175959691405296 acc:0.699999988079071\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss:0.07033011317253113 acc:0.7099999785423279\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss:0.04783403128385544 acc:0.6899999976158142\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss:0.07702574133872986 acc:0.6999999284744263\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss:0.05534963309764862 acc:0.7199999690055847\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss:0.053662173449993134 acc:0.7299999594688416\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss:0.06378161162137985 acc:0.7199999690055847\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss:0.04743782430887222 acc:0.6699999570846558\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss:0.0678454115986824 acc:0.7099999785423279\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss:0.04838895797729492 acc:0.7399999499320984\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss:0.051063183695077896 acc:0.7299999594688416\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss:0.06908725202083588 acc:0.7199999690055847\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss:0.047366488724946976 acc:0.6799999475479126\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss:0.07404472678899765 acc:0.7299999594688416\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss:0.04948122054338455 acc:0.7399999499320984\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss:0.04804590716958046 acc:0.7099999785423279\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss:0.0656210333108902 acc:0.7099999785423279\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss:0.048577405512332916 acc:0.699999988079071\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss:0.06850054860115051 acc:0.7199999690055847\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss:0.05019213631749153 acc:0.7199999690055847\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss:0.05278347060084343 acc:0.7199999690055847\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss:0.05618341639637947 acc:0.7199999690055847\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss:0.0460272878408432 acc:0.6899999380111694\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss:0.07218959182500839 acc:0.699999988079071\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss:0.05029422789812088 acc:0.7299999594688416\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss:0.04806405305862427 acc:0.7099999785423279\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss:0.05760452523827553 acc:0.7199999690055847\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss:0.05383538827300072 acc:0.7099999785423279\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss:0.057719625532627106 acc:0.7199999690055847\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss:0.04292815551161766 acc:0.7199999690055847\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss:0.04222887009382248 acc:0.7199999690055847\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss:0.05804431438446045 acc:0.7099999785423279\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss:0.05224127322435379 acc:0.699999988079071\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss:0.06219777837395668 acc:0.7099999785423279\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss:0.055587075650691986 acc:0.7099999189376831\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss:0.04647074639797211 acc:0.6899999976158142\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss:0.05955205485224724 acc:0.699999988079071\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss:0.041131578385829926 acc:0.7199999690055847\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss:0.05178333818912506 acc:0.6899999380111694\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss:0.05259748920798302 acc:0.7199999690055847\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss:0.040576934814453125 acc:0.7099999785423279\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss:0.06891968846321106 acc:0.7299999594688416\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss:0.05013604834675789 acc:0.7199999690055847\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss:0.06999284774065018 acc:0.7199999690055847\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss:0.041244376450777054 acc:0.7299999594688416\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss:0.04604903608560562 acc:0.7199999690055847\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss:0.063118577003479 acc:0.7199999690055847\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss:0.04358760267496109 acc:0.6899999976158142\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss:0.05808761715888977 acc:0.7199999690055847\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss:0.04045691713690758 acc:0.7199999690055847\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss:0.04780564457178116 acc:0.7099999189376831\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss:0.05440709367394447 acc:0.7199999690055847\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss:0.03614687919616699 acc:0.6699999570846558\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss:0.04832245409488678 acc:0.7199999690055847\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss:0.036263808608055115 acc:0.7199999690055847\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss:0.04779873788356781 acc:0.699999988079071\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss:0.05580637603998184 acc:0.7299999594688416\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss:0.04532410949468613 acc:0.6899999380111694\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss:0.05269842594861984 acc:0.7299999594688416\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss:0.040812864899635315 acc:0.7699999809265137\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss:0.045362651348114014 acc:0.7199999690055847\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss:0.06307744979858398 acc:0.7599999308586121\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss:0.044340312480926514 acc:0.6899999380111694\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss:0.04957123473286629 acc:0.7199999690055847\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss:0.03837118297815323 acc:0.7300000190734863\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss:0.044502176344394684 acc:0.7199999690055847\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss:0.06127045676112175 acc:0.7299999594688416\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss:0.043552216142416 acc:0.6599999666213989\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss:0.041569989174604416 acc:0.6799999475479126\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss:0.03786102682352066 acc:0.7099999785423279\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss:0.03633912280201912 acc:0.7399999499320984\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss:0.046233899891376495 acc:0.7099999785423279\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss:0.04008945822715759 acc:0.6899999380111694\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss:0.04017219692468643 acc:0.7599999904632568\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss:0.03511505573987961 acc:0.7399999499320984\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss:0.039099130779504776 acc:0.7199999690055847\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss:0.05629897862672806 acc:0.7099999785423279\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss:0.03676604852080345 acc:0.6899999380111694\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss:0.04797796159982681 acc:0.7299999594688416\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss:0.03619981184601784 acc:0.7299999594688416\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss:0.03290863335132599 acc:0.7199999690055847\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss:0.04285207390785217 acc:0.7199999690055847\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss:0.03503154590725899 acc:0.6999999284744263\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss:0.046941548585891724 acc:0.7299999594688416\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss:0.037121329456567764 acc:0.7499999403953552\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss:0.04275498166680336 acc:0.7299999594688416\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss:0.04256422072649002 acc:0.7099999785423279\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss:0.04111774265766144 acc:0.7099999189376831\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss:0.04588521644473076 acc:0.6999999284744263\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss:0.029970262199640274 acc:0.7399999499320984\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss:0.029873058199882507 acc:0.7399999499320984\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss:0.04571814835071564 acc:0.7299999594688416\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss:0.043492741882801056 acc:0.7099999785423279\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss:0.04611515626311302 acc:0.7499999403953552\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss:0.03591568022966385 acc:0.7099999189376831\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss:0.031852103769779205 acc:0.7199999690055847\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss:0.04825688898563385 acc:0.7199999690055847\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss:0.03834811598062515 acc:0.6899999976158142\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss:0.042639996856451035 acc:0.7399999499320984\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss:0.036316514015197754 acc:0.7400000095367432\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss:0.03401798754930496 acc:0.7299999594688416\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss:0.05083145201206207 acc:0.6899999380111694\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss:0.02726237289607525 acc:0.6799999475479126\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss:0.04975086450576782 acc:0.7099999189376831\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss:0.026657860726118088 acc:0.7199999690055847\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss:0.03246472030878067 acc:0.7299999594688416\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss:0.04653521999716759 acc:0.7299999594688416\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss:0.03319859877228737 acc:0.7199999690055847\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss:0.039601318538188934 acc:0.7399999499320984\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss:0.02820124477148056 acc:0.7099999189376831\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss:0.028982792049646378 acc:0.7399999499320984\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss:0.04420259967446327 acc:0.7099999189376831\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss:0.034265242516994476 acc:0.6799999475479126\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss:0.03462887555360794 acc:0.7299999594688416\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss:0.029574621468782425 acc:0.7299998998641968\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss:0.03144257143139839 acc:0.7599999904632568\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss:0.0539851114153862 acc:0.7399999499320984\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss:0.03135146573185921 acc:0.6999999284744263\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss:0.037211958318948746 acc:0.6699999570846558\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss:0.024060308933258057 acc:0.7399999499320984\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss:0.03642832860350609 acc:0.7299999594688416\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss:0.047454364597797394 acc:0.7199999690055847\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss:0.040697455406188965 acc:0.6999999284744263\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss:0.03380503132939339 acc:0.699999988079071\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss:0.024837221950292587 acc:0.6999999284744263\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss:0.027640648186206818 acc:0.7199999690055847\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss:0.04259101301431656 acc:0.7099999189376831\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss:0.03461888059973717 acc:0.6599999666213989\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss:0.034742794930934906 acc:0.7199999690055847\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss:0.022388076409697533 acc:0.7199999690055847\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss:0.02588050439953804 acc:0.7399999499320984\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss:0.04124008119106293 acc:0.7299999594688416\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss:0.03585072606801987 acc:0.6999999284744263\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss:0.03597196564078331 acc:0.7199999690055847\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss:0.024009887129068375 acc:0.75\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss:0.03600108623504639 acc:0.7399999499320984\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss:0.043850190937519073 acc:0.7199999690055847\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss:0.03119702637195587 acc:0.7099999785423279\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss:0.04101129621267319 acc:0.7299999594688416\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss:0.026497995480895042 acc:0.7299999594688416\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss:0.03331911191344261 acc:0.7299999594688416\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss:0.03850467503070831 acc:0.7399999499320984\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss:0.02929721213877201 acc:0.6799999475479126\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss:0.030709823593497276 acc:0.699999988079071\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss:0.02619812823832035 acc:0.7399999499320984\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss:0.03752594068646431 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss:0.0392821729183197 acc:0.7199999690055847\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss:0.029164675623178482 acc:0.6799999475479126\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss:0.034705959260463715 acc:0.7399999499320984\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss:0.030936047434806824 acc:0.7299998998641968\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss:0.03045743703842163 acc:0.7400000095367432\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss:0.03645522892475128 acc:0.7299999594688416\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss:0.025618139654397964 acc:0.6899999380111694\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss:0.03401828184723854 acc:0.7199999690055847\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss:0.023506363853812218 acc:0.7399999499320984\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss:0.025715377181768417 acc:0.7299999594688416\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss:0.033716097474098206 acc:0.7299999594688416\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss:0.029215138405561447 acc:0.6899999976158142\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss:0.02951560541987419 acc:0.7299999594688416\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss:0.02317110449075699 acc:0.7299999594688416\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss:0.026738956570625305 acc:0.7299999594688416\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss:0.040770478546619415 acc:0.7399999499320984\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss:0.02515929937362671 acc:0.6999999284744263\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss:0.03986487537622452 acc:0.7699999809265137\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss:0.025218525901436806 acc:0.7199999690055847\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss:0.022967683151364326 acc:0.7299999594688416\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss:0.037026770412921906 acc:0.7099999785423279\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss:0.027574747800827026 acc:0.6499999761581421\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss:0.03306639567017555 acc:0.7699999809265137\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss:0.023795772343873978 acc:0.7199999690055847\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss:0.026386242359876633 acc:0.7399999499320984\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss:0.04596816375851631 acc:0.7299999594688416\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss:0.025598179548978806 acc:0.6999999284744263\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss:0.03721693903207779 acc:0.7199999094009399\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss:0.02493629790842533 acc:0.7399999499320984\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss:0.028172951191663742 acc:0.7599999308586121\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss:0.03405691683292389 acc:0.7299999594688416\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss:0.027357639744877815 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss:0.027347950264811516 acc:0.699999988079071\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss:0.026075061410665512 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss:0.021637018769979477 acc:0.7199999690055847\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss:0.03590980917215347 acc:0.7299999594688416\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss:0.02507246658205986 acc:0.6799999475479126\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss:0.027580490335822105 acc:0.7399999499320984\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss:0.01927252672612667 acc:0.7499999403953552\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss:0.021093714982271194 acc:0.7299999594688416\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss:0.034739721566438675 acc:0.7299999594688416\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss:0.024804942309856415 acc:0.6799999475479126\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss:0.0329321026802063 acc:0.7499999403953552\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss:0.025043610483407974 acc:0.7199999690055847\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss:0.023791108280420303 acc:0.7399999499320984\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss:0.03779618442058563 acc:0.7299999594688416\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss:0.025676481425762177 acc:0.6899999380111694\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss:0.028526393696665764 acc:0.7399999499320984\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss:0.02572457492351532 acc:0.7199999690055847\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss:0.019733624532818794 acc:0.7399999499320984\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss:0.03875766322016716 acc:0.7299999594688416\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss:0.02905445173382759 acc:0.6999999284744263\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7026273885350318\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxyIiIiIigYJjEREREZHg/wPMsO8ZAsSNSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8f98befef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
